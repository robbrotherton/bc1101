---
format: revealjs
---

# 19|CORRELATION

![](covers/19_correlation.svg){.invertable}


# Overview

- [Correlational research designs]
- [Correlation statistic]
- [Hypothesis test]
- [Correlation & effect size]
- [Learning checks]



# Correlational research designs {.smaller}

- Correlational designs versus experimental designs
  - No independent variable
  - Measure two (or more) variables per participant
  - Examine relationship between variables

- Example research questions
  - Is income related to happiness?
  - Does practice make perfect?
  - Class attendance & final grades

::: {.r-hstack}
![](media/correlation-income-happiness.svg){.invertable style="padding-right: 4em;"}

![](media/correlation-practice-performance.svg){.invertable style="padding-right: 4em;"}

![](media/correlation-absences-final-grade.svg){.invertable}


:::

## Limitations of correlation {.small}

- Correlations are not usually useful for demonstrating [causation]{.emph}
  - Establishing causation requires experiments in which something is [manipulated]{.emph}
  
- E.g. coffee consumption & intelligence
  - Perhaps...
    - A causes B
    - B causes A
    - Something else (C) causes A and B

::: {.reference}
Corley, J., et al. (2010). Caffeine consumption and cognitive function at age 70: The Lothian Birth Cohort 1936 study. *Psychosomatic medicine 72*(2), 206-214. <https://doi.org/10.1097/PSY.0b013e3181c92a9c>
:::

  
# Correlation statistic {.smaller}

- Correlation coefficient
  - A number quantifying the association between two variables
- Three characteristics
  - Strength or consistency (varies from 0 to 1)
  - Direction (negative or positive)
  - Form (e.g. linear)
- Characteristics are independent
  - Can be positive & weak, negative & strong, etc
  
::: {.r-hstack}
![](media/correlation-weak.svg){.invertable}

![](media/correlation-moderate.svg){.invertable}

![](media/correlation-strong.svg){.invertable}

![](media/correlation-perfect.svg){.invertable}
:::


## Logic


```{r ref.label=I('sp-table')}
```

:::: {.columns}

::: {.column width="40%"}
```{r}
#| style: "font-size: 0.7em; margin-top: 3em;"
df <- data.frame(Participant = LETTERS[1:5],
                 Sleep = x,
                 Score = y)

knitr::kable(df)
```
:::

::: {.column width="60%"}
![](media/correlation-example.svg){.invertable width="100%"}
:::

::::




## Pearson's $r$ {.small}

- Most widely used correlation statistic
  - Measures the degree and the direction of the linear relationship between two variables
- Variability & covariability
  - [Variability]{.emph}: How much each variable varies
  - [Covariability]{.emph}: How much $X$ and $Y$ vary in tandem
  - Are changes in $X$ associated with corresponding changes in $Y$?
  - E.g. as $X$ increases, $Y$ tends to increase (positive)
  - Or, as $X$ increases, $Y$ tends to decrease (negative)


## Pearson's $r$

$r = \dfrac{SP}{\sqrt{SS_X SS_Y}} = \dfrac{\textrm{covariability of X and Y}}{\textrm{variability of X and Y separately}}$

:::: {.columns}

::: {.column width="50%"}
- Perfect linear relationship…
  - If every change in $X$ has a corresponding change in $Y$, variability separately = covariability
  - Correlation will be $–1.00$ or $+1.00$
:::

::: {.column width="50%"}
![](media/correlation-perfect.svg){.invertable width="80%"}
:::

::::




# Hypothesis test


## Step 1. State hypotheses {.small}

- Correlation coefficient $r$ is computed for sample data
- Hypotheses concerns relationship in the population
- Greek letter $\rho$ (rho) represents population correlation

- Non-directional:  
    - $H_0: \rho = 0$ 
    - $H_1: \rho \ne 0$
- Directional:  
    - $H_0: \rho \le 0$; $H_1: ρ > 0$ 
    - Or...
    - $H_0: \rho \ge 0$; $H_1: \rho < 0$


## Step 2. Critical region

:::: {.columns}

::: {.column width="30%"}
- Critical value for $r$ is a type of $t$ statistic
  - $df = n - 2$
:::

::: {.column width="70%"}

```{r}
#| style: "font-size: 0.4em;"
bc1101tools::t_table()
```
:::

::::


## Step 3. Calculate statistic {.smaller}

::: {.center-element}
$r = \dfrac{SP}{\sqrt{SS_X SS_Y}}$
:::

:::: {.columns .center-element}

::: {.column width="50%"}
- Numerator: Sum of Products, $SP$
  - Multiply deviations by one another

Definitional formula

::: {style="margin-top: 2.2em;"}
$SP = \Sigma(X - M_X)(Y - M_Y)$
:::

:::

::: {.column width="50%"}
- Denominator: Sum of Squares, $SS$
  - Multiply deviations by themselves

Definitional formula

$$\begin{align}
SS &= \Sigma(X - M_X)^2 \\
   &= \Sigma(X - M_X)(X - M_X)
\end{align}$$

:::

::::


:::: {.columns .center-element}

::: {.column width="50%"}
Computational formula

::: {style="margin-top: 3.65em;"}
$SP = \Sigma XY - \dfrac{\Sigma X \Sigma Y}{n}$
:::
:::

::: {.column width="50%"}
Computational formula

$$\begin{align}
SS &= \Sigma X^2 - \dfrac{(\Sigma X)^2}{n}\\
   &= \Sigma X X - \dfrac{(\Sigma X)(\Sigma X)}{n}
\end{align}$$
:::

::::



## Calculating $r$

```{r}
#| class: "sp-table scores-only"
sp_table
```

::: {.center-element}
$r = \dfrac{SP}{\sqrt{SS_X SS_Y}}$
:::


## Calculating $r$

```{r}
#| class: "sp-table with-devs"
sp_table
```

::: {.center-element}
$r = \dfrac{SP}{\sqrt{SS_X SS_Y}}$
:::

## Calculating $r$

```{r}
#| class: "sp-table"
sp_table
```

::: {.center-element}
$r = \dfrac{SP}{\sqrt{SS_X SS_Y}} = \dfrac{`r sp`}{\sqrt{`r ss_x`*`r ss_y`}} = `r r`$
:::


```{css}
.sp-table {
  font-size: 0.6em;
}
.sp-table.scores-only td:nth-child(n+2):nth-child(-n+3), .sp-table.scores-only th:nth-child(n+2):nth-child(-n+3) {
  visibility: hidden;
}
.sp-table.scores-only td:nth-child(n+5):nth-child(-n+7), .sp-table.scores-only th:nth-child(n+5):nth-child(-n+7) {
  visibility: hidden;
}
.sp-table.with-devs td:nth-child(3), .sp-table.with-devs th:nth-child(3),
.sp-table.with-devs td:nth-child(6), .sp-table.with-devs th:nth-child(6){
  visibility: hidden;
}

```



## Calculating $r$: computation approach

```{r}
#| style: "font-size: 0.6em;"

footer_value <- function(symbol, value) {
  glue::glue("\\({symbol} = {value}\\)")
}

# footer_value("\\Sigma X", 35)

comp <- data.frame(X = x, X2 = x^2, Y = y, Y2 = y^2, XY = x * y)

comp_with_footer <- rbind(comp, c(footer_value("\\Sigma X", sum(x)), 
                                  footer_value("\\Sigma X^2", sum(x^2)), 
                                  footer_value("\\Sigma Y", sum(y)), 
                                  footer_value("\\Sigma Y^2", sum(y^2)), 
                                  footer_value("\\Sigma XY", sum(x*y))))

knitr::kable(comp_with_footer, align = "r",
             col.names = c("\\(X\\)", "\\(X^2\\)", "\\(Y\\)", "\\(Y^2\\)", "\\(XY\\)")) |> 
  kableExtra::row_spec(length(x)+1, extra_css = "border-top: 3px solid var(--text-color);") |> 
  kableExtra::column_spec(1:5, width = "8em")
```

::: {.center-element style="font-size: 0.7em;"}

$SS_X = \Sigma X^2 - \dfrac{(\Sigma X)^2}{n} = `r sum(x^2)` - \dfrac{`r sum(x)`^2}{`r n`} = `r ss_x`$

$SS_Y = \Sigma Y^2 - \dfrac{(\Sigma Y)^2}{n} = `r sum(y^2)` - \dfrac{`r sum(y)`^2}{`r n`} = `r ss_y`$

$SP = \Sigma XY - \dfrac{\Sigma X \Sigma Y}{n} = `r sum(x*y)` - \dfrac{`r sum(x)` \cdot `r sum(y)`}{`r n`} = `r sp`$

$r = \dfrac{SP}{\sqrt{SS_X SS_Y}} = \dfrac{`r sp`}{\sqrt{`r ss_x` \cdot `r ss_y`}} = `r r`$

:::

## Step 4. Make decision

- Hypothesis test for $r$ is a type of $t$ test

::: {.center-element}

$t = \dfrac{\text{sample statistic} - \text{population parameter}}{\text{standard error}}$

:::: {.columns}

::: {.column width="50%"}
$t = \dfrac{r - \rho}{s_r}$
:::

::: {.column width="50%"}
$s_r = \sqrt{\dfrac{1 - r^2}{n-2}}$
:::

$t = \dfrac{`r r`}{\sqrt{\dfrac{1-`r r`^2}{`r n` - 2}}} = `r t`$

::::

:::


## Step 5. Report results

- Value of correlation, $r$
- $df$ (or sample size)
- $p$-value or $\alpha$ level

> “More sleep was associated with higher test scores; however, the correlation did not reach statistical significance; $r (3)  = .73, p > .05$.”


# Correlation & effect size {.small}

- Pearson's $r$ correlation coefficient *is* a standardized measure of effect size
  - Quantifies degree of association on a scale from $0$ to $1$
  - Independent of sample size
  - Related to $r^2$
  <!-- - Related to $t$ -->
  - Even related to Cohen's $d$

::: {.center-element}
$d = \dfrac{2r}{\sqrt{1-r^2}}$
:::

```{r}
r_to_d <- function(r) {
  (2*r)/sqrt(1-r^2)
}
```



## Interpreting effect size {.small}

:::: {.columns}

::: {.column width="30%"}

Cohen (1977)

```{r}
#| style: "font-size: 0.7em;"

cohen <- tibble::tribble(
  ~"\\(d\\)", ~"\\(r\\)", ~"Description",
  "0.3", "0.1", "Small",
  "0.5", "0.3", "Medium",
  "0.8", "0.5", "Large"
)

knitr::kable(cohen, format = "html")
```


:::

::: {.column width="70%"}

Funder & Ozer (2019)

```{r}
#| style: "font-size: 0.7em;"

funder <- tibble::tribble(
  ~"\\(d\\)", ~"\\(r\\)", ~"Description",
  "0.10", "0.05", "Very small for the explanation of single events but potentially consequential in longer run",
  "0.20", "0.10", "Still small at the level of single events but potentially more ultimately consequential",
  "0.40", "0.20", "Medium effect of some explanatory and practical use even in the short run",
  "0.60", "0.30", "Large effect that is potentially powerful in both the short and the long run",
  "0.80", "0.40", "A very large effect in the context of psychological research; likely to be a gross overestimate"
)

# - .05: Very small for the explanation of single events but potentially consequential in longer run
# - .10: Still small at the level of single events but potentially more ultimately consequential
# - .20: Medium effect of some explanatory and practical use even in the short run
# - .30: Large effect that is potentially powerful in both the short and the long run
# - .40: A very large effect in the context of psychological research; likely to be a gross overestimate

knitr::kable(funder, format = "html")
```

:::

::::

::: {.reference}
Funder, D. C., & Ozer, D. J. (2019). Evaluating Effect Size in Psychological Research: Sense and Nonsense. *Advances in Methods and Practices in Psychological Science, 2*(2), 156–168. <https://doi.org/10.1177/2515245919847202>
:::


# See also...

- <https://www.rpsychologist.com/d3/correlation>
  - Interactive correlation visualization
  
  
  
# Learning checks

- True/False
    1. The numerator of Pearson's $r$ cannot be negative
    2. The denominator of Pearson's $r$ cannot be negative
    3. In a non-directional significance test of a correlation, the null hypothesis states that the population correlation is zero






::: {.content-hidden}

# Data and figures

## Cover

```{r cover-ggplot}
#| eval: false

library(ggplot2)
library(dplyr)
set.seed(42)

# data <- faux::rnorm_multi(n = 300, vars = 2, r = .6)

create_correlated_data <- function(n, r, mean_x = 0, sd_x = 1, mean_y = 0, sd_y = 1) {
  # Check if the desired correlation is within the permissible range [-1, 1]
  if (r < -1 || r > 1) {
    stop("r must be between -1 and 1")
  }
  
  # Create x as a normally distributed variable
  x <- rnorm(n, mean = mean_x, sd = sd_x)
  
  # Create an error term that is also normally distributed
  error <- rnorm(n, mean = 0, sd = sqrt(1 - r^2))
  
  # Create y using the desired correlation
  y <- r * (x - mean_x) / sd_x * sd_y + mean_y + error
  
  # Return a dataframe with the variables x and y
  return(data.frame(x, y))
}

data <- create_correlated_data(n = 300, r = .6)

SSx <- sum( (data$X1 - mean(data$X1))^2 )
SSy <- sum( (data$X2 - mean(data$X2))^2 )
SP <- sum( (data$X1 - mean(data$X1)) * (data$X2 - mean(data$X2)))
b <- SP/SSx
a <- mean(data$X2) - b * mean(data$X1)

data |> 
  mutate(yhat = b * X1 + a) |> 
  ggplot(aes(x = X1, y = X2, color = abs(X2 - yhat))) +
  scale_color_gradient(low = "black", high = "grey") +
  geom_point() +
  theme_void() +
  theme(legend.position = "none")

bc1101tools::plot_save("19_correlation.svg", subdir = "covers", width = 10, height = 5)

```


## Example data 

```{r sp-table}
options(digits = 2)
x <-  c(4,5,7,8,11)
y <-  c(5,8,8,10,9)

p <- (x-mean(x)) * (y-mean(y))
sp <- sum(p)
ss_x <- sum((x-mean(x))^2)
ss_y <- sum((y-mean(y))^2)
r <- sp/sqrt(ss_x * ss_y)
n <- length(x)
t <- r / sqrt((1-r^2)/(n-2))

sp_table <- bc1101tools::SP_table(x = c(4,5,7,8,11), y = c(5,8,8,10,9), align = "c")

```

```{r example-plot}
#| eval: false

ggplot(data = NULL, aes(x = x, y = y)) +
  geom_point() +
  scale_x_continuous(limits = c(0, 12), breaks = 0:12) +
  scale_y_continuous(limits = c(0, 10), breaks = 0:10) +
  labs(x = "Sleep (hours)", y = "Test score") +
  bc1101tools::theme_bc1101()

plot_save("correlation-example.svg", width = 3.5, height = 3)

```

## Generic correlation plots

```{r}
#| eval: false

library(bc1101tools)
library(ggplot2)

set.seed(42)

weak <- plot_correlation_generic(df = faux::rnorm_multi(n = 15, vars = 2, r = .2), ellipse = TRUE)
mod <- plot_correlation_generic(df = faux::rnorm_multi(n = 15, vars = 2, r = .5), ellipse = TRUE)
strong <- plot_correlation_generic(df = faux::rnorm_multi(n = 15, vars = 2, r = .8), ellipse = TRUE)
perfect <- plot_correlation_generic(df = data.frame(X = 1:15, Y = 1:15)) + geom_line()

plot_save(plot = weak, "correlation-weak.svg", width = 2, height = 2)
plot_save(plot = mod, "correlation-moderate.svg", width = 2, height = 2)
plot_save(plot = strong, "correlation-strong.svg", width = 2, height = 2)
plot_save(plot = perfect, "correlation-perfect.svg", width = 2, height = 2)

# negative plots
plot_save(plot = weak + scale_x_continuous(trans = "reverse"), "correlation-neg-weak.png", width = 2, height = 2)
plot_save(plot = mod + scale_x_continuous(trans = "reverse"), "correlation-neg-moderate.png", width = 2, height = 2)
plot_save(plot = strong + scale_x_continuous(trans = "reverse"), "correlation-neg-strong.png", width = 2, height = 2)
plot_save(plot = perfect + scale_x_continuous(trans = "reverse"), "correlation-neg-perfect.png", width = 2, height = 2)


## non linear plots
set.seed(1)
d <- faux::rnorm_multi(n = 15, vars = 2, r = .8, varnames = c("x","y"))
p <- ggplot(d, aes(x = x, y = y^2)) +
  geom_point() +
  labs(x = "X", y = "Y") +
  scale_x_continuous(labels = NULL) +
  scale_y_continuous(labels = NULL) +
  bc1101tools::theme_bc1101()

loess <- p + geom_smooth(se=FALSE, color = "black")
linear <- p + geom_smooth(formula = 'y~x', method = "lm", se=FALSE, color = "black")
quad <- p + geom_smooth(formula = 'y~x+I(x^2)', method = "lm", se=FALSE, color = "black")
cube <- p + geom_smooth(formula = 'y~x+I(x^3)', method = "lm", se=FALSE, color = "black")
plot_save(plot = loess, "correlation-nonlinear-loess.png", width = 2, height = 2)
plot_save(plot = linear, "correlation-nonlinear-linear.png", width = 2, height = 2)
plot_save(plot = quad, "correlation-nonlinear-quad.png", width = 2, height = 2)
plot_save(plot = cube, "correlation-nonlinear-cube.png", width = 2, height = 2)

```

## Example correlation plots

```{r}
#| eval: false

library(bc1101tools)
library(ggplot2)

set.seed(1)

plot_correlation_generic(df = faux::rnorm_multi(n = 50, vars = 2, r = .4)) +
  labs(x = "Income", y = "Happiness")
plot_save("correlation-income-happiness.svg", width = 2, height = 2)

plot_correlation_generic(df = faux::rnorm_multi(n = 50, vars = 2, r = .6)) +
  labs(x = "Practice", y = "Performance")
plot_save("correlation-practice-performance.svg", width = 2, height = 2)

plot_correlation_generic(df = faux::rnorm_multi(n = 50, vars = 2, r = -.3)) +
  labs(x = "Absences", y = "Final grade")
plot_save("correlation-absences-final-grade.svg", width = 2, height = 2)

```


:::
