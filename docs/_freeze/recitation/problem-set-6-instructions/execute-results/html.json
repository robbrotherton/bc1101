{
  "hash": "83358d1df2df506ba667423d23be9dee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Problem Set 6\nsubtitle: $t$-test & Confidence Intervals\n\nformat: html\nsidebar: recitation\nnavbar: false\nexecute: \n  echo: true\n---\n\n# Part 1. The $t$ statistic\n\n::: {.content-hidden when-format=\"pdf\"}\n## Instructions\n\nThe general hypothesis testing procedure for the $t$-test (and any kind of test) is the same as the procedure we used in the previous problem set for the $z$-test. The difference is that we're dealing with a different distribution and a different test statistic.\n\nStep 1: State your hypotheses\n\nStep 2: Set decision criteria. As usual, this depends on alpha and whether the test is 1- or 2-tailed. To find the critical regions of the $t$ distribution, we use the `qt()` function. It works just like `qnorm()`, but for the $t$ distribution instead of the normal distribution. The values for the $t$ distribution depend on the degrees of freedom for your test, so we must also specify the degrees of freedom as an argument within the function. E.g.,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqt(c(.025, .975), df = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -2.008559  2.008559\n```\n\n\n:::\n:::\n\n\nStep 3: Calculate your test statistic. Do this by plugging the appropriate values into the $t$ equation\n\nStep 4: Decide. Compare your calculated test statistic with the critical values to determine whether your sample is within the critical region(s), and thus whether to reject the null.\n\nUnlike the $z$-test, however, R does have a built-in function for performing a $t$ test. When you're dealing with real data you can use this rather than following the step-by-step procedure and calculating things manually. For a single-sample $t$-test, the arguments you need to specify are `x` (your data) and `mu` (the population mean), and possibly `alternative`. This last argument allows you to specify the directionality of the alternative hypothesis (i.e. to conduct a one-tailed or two-tailed hypothesis).\n\nThe `t.test()` function doesn't calculate effect size. For that, you can implement the relevant equation. Or alternatively, you could check out the `cohens_d()` function from the `effectsize` package...\n\nNOTE: You can see documentation for any function by typing a question mark followed by the name of the function (with no parentheses after the name) and running the code. Alternatively, you can click over to the Help tab of the pane in the bottom right and search for a function. The documentation isn't always that helpful, but it's worth taking a look when you're unsure about a function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?t.test\n\n?effectsize::cohens_d\n```\n:::\n\n:::\n\n# Part 2. Confidence intervals\n\n::: {.content-hidden when-format=\"pdf\"}\n## Instructions\n\nThe equation for a confidence interval is:\n\n$$\n\\mu = M \\pm t * s_M\n$$\n\nSo this involves computing some critical $t$ values and multiplying by the estimated standard error.\n\nSuppose we're dealing with a sample, $n = 20$, $M = 100$, $SD = 10$ . We want to make a 95% confidence interval for that $M = 100$ point estimate.\n\nStep 1: Find the critical value.\n\nFor 95% confidence, picture the distribution split into 95% in the middle and therefore 2.5% in each tail. Strictly speaking we should have *two* critical values, one for each tail. But since it's symmetrical, we can simplify by just using the upper tail's value for now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncritical_score <- qt(.025, df = 19, lower.tail = FALSE)\n\n# or...\ncritical_score <- qt(.975, df = 19)\n```\n:::\n\n\nStep 2: Find the (estimated) standard error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstd_error <- 10/sqrt(20)\n```\n:::\n\n\nStep 3: Multiply the standard error by critical value. This gives the margin of error; the distance from the center of the distribution (the point estimate) to the boundary of the confidence interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmargin_of_error <- critical_score * std_error\n```\n:::\n\n\nStep 4: Add and subtract the margin of error to the point estimate to describe the limits of the confidence interval.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_upper_limit <- 100 + margin_of_error\nci_lower_limit <- 100 - margin_of_error\n\nc(ci_lower_limit, ci_upper_limit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  95.31986 104.68014\n```\n\n\n:::\n:::\n\n\nThis tells us that the lower boundary of our 95% confidence interval is 95.3 and the upper boundary is 104.7.\n\nNote that if the population standard deviation was known, i.e. we were dealing with the $z$ distribution instead of $t$, the process would be the same, we would just have to use `qnorm()` instead of `qt()` to find the critical score for the appropriate distribution.\n\n### Using actual data\n\nIn the example above, I gave you the values of $n$, $M$, and $SD$. For actual data, i.e. a data.frame with a column of scores, the logic is all the same, you just need to compute those values from the scores in the ways we've practiced.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv(\"data/6_confidence-interval.csv\")\n\nn <- length(df$scores)\nmean <- mean(df$scores)\nsd <- sd(df$scores)\n\ncritical_score <- qt(.975, df = n - 1) # step 1\nstd_error <- sd / sqrt(n) # step 2\nmargin_of_error <- critical_score * std_error # step 3\n\n# step 4:\nci_upper_limit <- mean + margin_of_error\nci_lower_limit <- mean - margin_of_error\n\nc(ci_lower_limit, ci_upper_limit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  95.31001 104.68999\n```\n\n\n:::\n:::\n\n\n### Make your own function\n\nA very useful thing to do would be to take the logic of computing a confidence interval and create your own function to do it. That way if you had to compute more than one confidence interval you can just use your function rather than having to copy and paste a bunch of lines of code and tweak each line.\n\nStep 1-3 can be reduced to a single line of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqt(0.975, df = length(df$scores) - 1) * sd(df$scores) / sqrt(length(df$scores))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.689987\n```\n\n\n:::\n:::\n\n\nSo we can just put line that inside a function, replacing the specific `df$scores` with a generic placeholder `x`, which stands in for whatever set of scores we will eventually feed into the function as its only argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci <- function(x) {\n  qt(0.975, df = length(x) - 1) * sd(x) / sqrt(length(x))\n}\n\nci(df$scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.689987\n```\n\n\n:::\n:::\n\n\nNote that that function returns the *width* of the confidence interval--the margin of error. So you would still need to add & subtract it from the point estimate, per Step 4, to find the actual CI boundaries.\n\n### Visualizing confidence intervals\n\nThe usefulness of creating a function like that is that you can quickly and easily create a summary of some data, including the point estimate (mean) and confidence interval width, and create a graph using these summary values. For a single confidence interval like this, there's not actually any data to map on to the x-axis, so I fudge a little there by telling ggplot `x = \"\"` for the x aesthetic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata_summary <- df |> \n  summarize(mean = mean(scores),\n            ci_width = ci(scores),\n            ci_lower_limit = mean - ci_width,\n            ci_upper_limit = mean + ci_width)\n\ndata_summary |> \n  ggplot(aes(x = \"\", y = mean)) +\n  geom_errorbar(aes(ymax = ci_upper_limit, ymin = ci_lower_limit), width = 0.3) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](problem-set-6-instructions_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n:::\n",
    "supporting": [
      "problem-set-6-instructions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}