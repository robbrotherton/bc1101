[
  {
    "objectID": "recitation/problem-set-4-instructions.html",
    "href": "recitation/problem-set-4-instructions.html",
    "title": "Problem Set 4",
    "section": "",
    "text": "The old-fashioned way to find the probability (the proportion of the distribution) associated with particular \\(z\\)-scores would be to look the score up in a \\(z\\)-score table (Unit Normal Table). R can do this for us much more easily, but it’s a good idea to check your first few answers against a table, e.g. https://www.westga.edu/academics/research/vrc/assets/docs/UnitNormalTable.pdf\nIf you have a \\(z\\)-score in mind and want to know the associated probability, use the pnorm() function. By default, pnorm() assumes we want the proportion of the distribution to the left of the \\(z\\)-score we specify.\n\npnorm(2) \n\n[1] 0.9772499\n\n\nSo .9772499, or ~98% of scores in a normal distribution are less than \\(z = 2\\). Here’s what that looks like:\n\n\n\n\n\n\n\n\n\nTo get the proportion to the right, we set the lower.tail argument to FALSE (i.e. we want the upper tail)\n\npnorm(2, lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nThat tells us that ~2% of scores are greater than \\(z = 2\\). Here’s what that looks like:\n\n\n\n\n\n\n\n\n\nThink about how R’s lower.tail distinction corresponds to the “body” vs. “tail” distinction. It’s not always necessarily the same.\nTo find the proportion of the normal distribution between two \\(z\\)-scores, one way would be to subtract p(lower score) from p(higher score)\n\npnorm(.25) - pnorm(-.25)\n\n[1] 0.1974127\n\n\n\n\n\n\n\n\n\n\n\nJust under .20, or 20% is between \\(z = -.25\\) and \\(z = .25\\). Note that the slightly different code below gives the same answer. Do you understand why? Either solution is perfectly acceptable.\n\npnorm(-.25, lower.tail = F) - pnorm(.25, lower.tail = F)\n\n[1] 0.1974127",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "recitation/problem-set-4-instructions.html#instructions",
    "href": "recitation/problem-set-4-instructions.html#instructions",
    "title": "Problem Set 4",
    "section": "",
    "text": "The old-fashioned way to find the probability (the proportion of the distribution) associated with particular \\(z\\)-scores would be to look the score up in a \\(z\\)-score table (Unit Normal Table). R can do this for us much more easily, but it’s a good idea to check your first few answers against a table, e.g. https://www.westga.edu/academics/research/vrc/assets/docs/UnitNormalTable.pdf\nIf you have a \\(z\\)-score in mind and want to know the associated probability, use the pnorm() function. By default, pnorm() assumes we want the proportion of the distribution to the left of the \\(z\\)-score we specify.\n\npnorm(2) \n\n[1] 0.9772499\n\n\nSo .9772499, or ~98% of scores in a normal distribution are less than \\(z = 2\\).\nTo get the proportion to the right, we set the lower.tail argument to FALSE (i.e. we want the upper tail)\n\npnorm(2, lower.tail = FALSE) # .02275013, or ~2% of scores are greater than z = 2\n\n[1] 0.02275013\n\n\nThink about how R’s lower.tail distinction corresponds to the “body” vs. “tail” distinction. It’s not always necessarily the same.\nTo find the proportion of the normal distribution between two \\(z\\)-scores, one way would be to subtract p(lower score) from p(higher score)\n\npnorm(.25) - pnorm(-.25) # Just under .20, or 20% is between z = -.25 and z = .25\n\n[1] 0.1974127\n\n\nNote that the slightly different code below gives the same answer. Do you understand why? Either solution is perfectly acceptable.\n\npnorm(-.25, lower.tail = F) - pnorm(.25, lower.tail = F)\n\n[1] 0.1974127\n\n\nWhen you have a proportion (quantile) in mind and want to find the associated z-score, qnorm() is the appropriate function\n\nqnorm(.05)\n\n[1] -1.644854\n\n\nThat tells us that the cutoff for the lowest 5% of the distribution corresponds to \\(z = -1.64\\). qnorm() also accepts the lower.tail argument\n\nqnorm(.05, lower.tail = FALSE) # Now we get a positive value\n\n[1] 1.644854\n\n\nThe line below gives the same answer. Do you understand why?\n\nqnorm(.95, lower.tail = TRUE)\n\n[1] 1.644854\n\n\nThe last thing you need to know for the questions below is that both pnorm() and qnorm() have optional arguments for the mean and SD of the distribution. That is, if we have raw scores, we don’t have to convert to z-scores to use these functions, we can specify the appropriate values\n\npnorm(130, mean = 100, sd = 15)\n\n[1] 0.9772499\n\n\n\n\n\n\n\n\nTipSketching distributions\n\n\n\n\n\nWith the kinds of questions that follow, I find it massively useful to sketch a rough normal curve on a piece of paper and mark approximately where the \\(z\\)-scores would go, and shade in the corresponding probability region. My strong recommendation is that you do that on a piece of paper.\nIf are falling in love with R and ggplot in particular, however, you can draw distributions right here.\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(-2, 2), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nThe first stat_function() layer draws the red shaded region, and the second one draws the black line of the full normal curve. I picked c(-4, 4) for the xlims of the full curve because that’s wide enough to show the ends tailing off to 0. For the shaded region, c(-2, 2) was just an arbitrary choice.\nSay you wanted to shade the region below \\(z = 2\\). You could use:\n\nggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(-4, 2), # xlim goes up to 2\n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nThat gives you a good visual sense that most of the distribution is below that point, so when pnorm(2) gives you the answer, it should make sense.\n\npnorm(2)\n\n[1] 0.9772499\n\n\nOr say you want to see what the highest 20% of the distribution looks like.\n\nggplot() +\n  stat_function(fun = dnorm, geom = \"density\", \n                xlim = c(qnorm(0.2, lower.tail = FALSE), 4), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nOr a region of 40% in the middle of the distribution, separating 30% in each tail:\n\nggplot() +\n  stat_function(fun = dnorm, geom = \"density\", \n                xlim = c(qnorm(0.3), qnorm(0.3, lower.tail = FALSE)), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nYou could even turn this into a convenient function…\n\nplot_distribution &lt;- function(lower_z = -4, upper_z = 4) {\n  ggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(lower_z, upper_z), # xlim goes up to 2\n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n}\n\nplot_distribution(-2, 2)\n\n\n\n\n\n\n\n\nI’m not saying this will make your life easier: you still have to know your way around pnorm(), qnorm(), and the idea of chopping the distribution into different regions. But it can be satisfying to produce a visual reference corresponding to your numeric answer.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "recitation/problem-set-1-instructions.html",
    "href": "recitation/problem-set-1-instructions.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "NoteWelcome!\n\n\n\nThese instructions are designed to accompany the file problem-set-1.qmd which students in PSYC BC1101 can find in the posit.cloud course space.\nThat .qmd file is a Quarto document with questions for you to answer. These instructions will help you figure out what you need to do to answer those questions. Read all the instructions carefully and answer the questions to the best of your ability.\nEventually you will “Render” your edited .qmd into a nicely-formatted PDF, weaving together your own text, code, and output. That’s what you will upload to Canvas to complete the assignment.\nTry rendering your document now. First, click on the settings cogwheel next to “Render” in the menu near the top of the editor and select “Preview in Viewer Pane”. Then click the “Render” button itself. You should then see your PDF open in the “viewer” pane, replacing these instructions (run the first code chunk in your .qmd file to get the instructions back). Initially the PDF will just have the questions. As you add answers and code you can click “Render” again at any time to see your work coming together.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "recitation/problem-set-1-instructions.html#welcome",
    "href": "recitation/problem-set-1-instructions.html#welcome",
    "title": "Problem Set 1",
    "section": "",
    "text": "These instructions are designed to accompany the file problem-set-1.qmd which students in PSYC BC1101 can find in the posit.cloud course space.\nThat .qmd file is a Quarto document with questions for you to answer. These instructions will help you figure out what you need to do to answer those questions. Read all the instructions carefully and answer the questions to the best of your ability.\nEventually you will “Render” your edited .qmd into a nicely-formatted PDF, weaving together your own text, code, and output. That’s what you will upload to Canvas to complete the assignment.\nTry rendering your document now. First, click on the settings cogwheel next to “Render” in the menu near the top of the editor and select “Preview in Viewer Pane”. Then click the “Render” button itself. You should then see your PDF open in the “viewer” pane, replacing these instructions (run the first code chunk in your .qmd file to get the instructions back). Initially the PDF will just have the questions. As you add answers and code you can click “Render” again at any time to see your work coming together.\n\n\n\n\n\n\nTipBonus: Personalize your PDF\n\n\n\nLook for the file named _quarto.yml under the Files tab. It contains some instructions that the rendering system uses to make your finished PDF document. The last line is author: \"\". Put your name inside the quotation marks, and every time you render a PDF it will have your name at the top.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "recitation/problem-set-1-instructions.html#instructions",
    "href": "recitation/problem-set-1-instructions.html#instructions",
    "title": "Problem Set 1",
    "section": "Instructions",
    "text": "Instructions\nThis is where we can start using R code to find answers to questions. Using the quarto document format, you include code “chunks” where you will write your code. The code will get executed when you Render the document (or you can execute it interactively to see output right away).\nThe easiest way to insert a code chunk is to put your cursor on a new line and type a forward-slash on your keyboard (/).\nYou should see a pop up showing all the stuff you can put into a document. The first option is an “R code chunk” which is what you need, so you can click it or just press return on the keyboard to insert the chunk. If you prefer, you can also click Insert &gt; Executable Cell &gt; R from the menu at the top of the editor.\nHere’s what a code chunk looks like:\n{r}\n# Code goes here! This is just a comment. Comments are preceded by the # symbol\nYou should thoroughly comment and/or annotate your problem sets with regular text like this, to explain to any reader (including your future self) what your code is doing. (You will forget!)\nFor the following questions, you could easily do these sums by hand - and I recommend checking your answers that way. But it is important to learn how to do these kinds of things in R as well. Usually there’s more than one way to achieve something in R.\nIf you want to add up a set of scores, you can use R just like a calculator:\n\n1 + 2 + 3 + 4\n\n[1] 10\n\n\nWhen you “Render”, the code will be executed and you’ll see the answer right after the code. You can also run the code yourself to see the answer right now. Put your cursor on the line of code and press Ctrl/Command + Enter on your keyboard. You’ll see the answer in the “Console” pane below. Note that you DON’T type 1 + 2 + 3 + 4 = 10. In fact, that’ll confuse R and you’ll get an error. (Try it)\nRather than just typing the sum out like that, a better way is to assign the scores a name. The &lt;- arrow operator is used to assign names. The name can be anything you want except that it cannot include spaces and cannot start with a number.\n\nscores &lt;- c(1, 2, 3, 4)\n\nThe c() function collects items. If you run the previous line of code with Ctrl/Cmd + Enter, you’ll see something called “scores” appear over in the Global Environment pane on the top-right. It consists of the numbers 1, 2, 3, and 4 collected together as a set. You won’t actually see any output though, and nothing will show up in your Rendered document. If you want a named object to be shown, you can type the name as a line of code.\n\nscores\n\n[1] 1 2 3 4\n\n\nTo add the set of scores up, we can use the sum() function.\n\nsum(scores)\n\n[1] 10\n\n\nThis might not seem like a time-saver with just 4 numbers, but imagine if you had a dataset of 1000 numbers…\nYou can do other things, like squaring each score, or squaring the summed scores. Note that R follows the usual order of mathematical operations.\n\nsum(scores^2) \n\n[1] 30\n\nsum(scores)^2 \n\n[1] 100\n\n\nNow you should be ready to answer the questions below by typing your own code.\n\n\n\n\n\n\nTipFormatting equations in-text\n\n\n\n\n\nNotice how mathematical symbols are created in the question text below: Click on the symbols and you’ll see that, behind the scenes, equations are enclosed in dollar signs. Typing a backslash and “Sigma” produces the symbol for summation.\nThere may be times that you’ll want to type symbols or equations like these in your answers to questions, so it’s worth knowing how to do so.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "recitation/problem-set-1-instructions.html#how-to-submit-your-assignment",
    "href": "recitation/problem-set-1-instructions.html#how-to-submit-your-assignment",
    "title": "Problem Set 1",
    "section": "How to submit your assignment",
    "text": "How to submit your assignment\nWhen you’re completely finished, and after clicking “Render” for the final time, you’ll download the pdf version of your finished problem set and submit that via Canvas. The pdf will open either in the Viewer Pane or a new tab, depending on whether you clicked “Preview in Viewer Pane” under the render settings cogwheel. Save it to your hard drive by clicking the download icon, go to the Assignment in Canvas, and upload your completed .pdf there. You’ll do this for each problem set.\n\n\n\n\n\n\nImportantCheck your PDF\n\n\n\nMake sure to carefully look over your pdf when you think you’re done. You want to make sure everything looks the way you expect it to (including showing all your code and comments). That’s what I and your TA will see and provide feedback on, so make sure it is what you intend for us to see.\n\n\n\n\n\n\n\n\nTipCongrats!\n\n\n\nThat’s it! Congrats on completing your first problem set.\nThey get more difficult as the course goes on, and I gradually reduce the amount of guidance I give as you get to grips with the basics. R can be tricky - error messages are common, and not always helpful. The first thing to check is usually for any typing errors. R is always case-sensitive and you must type any names (e.g. of files, folders, column or variable names) exactly right. It is also important to know that when you Render, R ignores everything in your Global Environment pane and tries running all the code in your document from scratch. If it can’t find the right named object in the code it will throw up an error (even if you can see the object in your Global Environment). So make sure it is in your code. Also bear in mind that if you reuse names (e.g. calling two different sets of scores “scores” at different points), R will simply overwrite the object without warning you. This isn’t a problem if you’re aware of it and expecting it, but it can cause confusion if it wasn’t what you meant to do.\nIf you’re ever getting an error message and don’t know what it means or what to do, copy and paste it into Google. Yep, that’s what the pros do. I also encourage you to collaborate with others in the class (sharing ideas/thoughts/struggles is great, just don’t copy and paste one another’s code). And of course you can always get help from me and/or your TA. We can access your work directly through posit.cloud.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#instructions",
    "href": "recitation/problem-set-2-instructions.html#instructions",
    "title": "Problem Set 2",
    "section": "Instructions",
    "text": "Instructions\nAs usual, there are many ways of producing data visualization in R, but the most widely used and flexible is probably the ggplot2 package. The “gg” in “ggplot” refers to the “grammar of graphics”. This package isn’t built-in to R; someone else created it and made it freely available as an add-on. For packages like that, we have to tell R we want to use them using the library() function.\n\nlibrary(ggplot2)\n\nFor these instructions I use the same file, “2_example.csv” in the “data” folder as I did for the previous instructions. Since I already read the data in a previous code chunk, don’t necessarily need to do it again; however, maybe in your answers to the previous part you also used the name my_data, which would overwrite my my_data! I’ll read in the data again to be safe, but if you’re being careful about the names you give things you don’t need to. (but it doesn’t hurt).\n\nmy_data &lt;- read.csv(\"data/2_example.csv\")\n\nThe following two lines create a simple bar graph. ggplot works by layering, using the + symbol. The first line specifies the name of the data.frame and the ‘aesthetics’: we want the scores (which are in a column conveniently named scores in the data.frame) on the x-axis. Then we add geom_bar() as the geometry layer. geom_bar is designed to take a set of scores and calculate the frequencies, which become the height of the bars on the \\(y\\)-axis (which is why we don’t need to explicitly specific a \\(y\\) aesthetic in the aes() function.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThat’s all there is to it: we now have a perfectly serviceable bar graph showing the frequency distribution.\nIf we instead wanted a histogram, there are a couple of ways to do it. Remember, the only visual difference between a bar graph and histogram is that a histogram has bars that are touching, while the bar graph has a bit of space between them. So we can repeat the previous code, but specify that the width of the bars should be 1.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar(width = 1)\n\n\n\n\n\n\n\n\nAnd that basically produces what we want. But there is also a dedicated histogram geom we can use:\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhen you run those lines of code you’ll notice a warning message in the console advising you to pick a new binwidth. That’s because the default is to have 30 bins, which often won’t be appropriate for your data. Here, because sleep duration was recorded ‘to the nearest whole hour’, i.e. the data is integer, we probably want a binwidth of 1 unit, i.e. we want a bar for each hour.\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nNow this is identical to the geom_bar(width = 1) version from before. Note that bar’s width and the binwidth are different things, though. The width is literally the width of the bars; binwidth is the width of the intervals into which scores are grouped. The usefulness of geom_histogram()is that it allows for more flexibility with the binwidth, which can be useful for continuous scores and datasets which cover a width range of scores, for which we might want a grouped frequency histogram with a wider binwidth.\nLastly, we make a frequency polygon in much the same way as a histogram, remembering to specify the appropriate binwidth.\n\nggplot(my_data, aes(x = scores)) + \n  geom_freqpoly(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipBonus: Making things look nice\n\n\n\n\n\nThe default options for the look of the plot are still pretty ugly though. If you’re so inclined, you can customize just about everything about the look and layout of a graph using ggplot. Some aspects are fairly straight forward, like changing the outline and fill of the bars within the geom_histogram() function. We add another layer, labs() to set the axis titles. Stating breaks within scale_x_continuous() determines the labels of the x axis. And then there are all the theme() elements we can change or remove.\n\nggplot(data = my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1, fill = \"grey\", color = \"black\") +\n  labs(x = \"Scores\", y = \"Frequency\") +\n  scale_x_continuous(breaks = 3:9) +\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_line(),\n        axis.line.y = element_line(),\n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nI don’t suggest you get into the weeds on this unless you find that kind of thing satisfying (like I do). But for psych majors it can be a useful exercise figuring out how to produce a graph formatted according to, say, APA conventions, by tweaking the theme options.\nThere’s a learning curve to ggplot, but once you get to grips with it, it is an incredibly powerful and flexible way of visualizing data.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#instructions-1",
    "href": "recitation/problem-set-2-instructions.html#instructions-1",
    "title": "Problem Set 2",
    "section": "Instructions",
    "text": "Instructions\nAs usual, there are many ways of producing data visualization in R, but the most widely used and flexible is probably the ggplot2 package. The “gg” in “ggplot” refers to the “grammar of graphics”. This package isn’t built-in to R; someone else created it and made it freely available as an add-on. For packages like that, we have to tell R we want to use them using the library() function.\n\nlibrary(ggplot2)\n\nFor these instructions I use the same file, “2_example.csv” in the “data” folder as I did for the previous instructions. Since I already read the data in a previous code chunk, don’t necessarily need to do it again; however, maybe in your answers to the previous part you also used the name my_data, which would overwrite my my_data! I’ll read in the data again to be safe, but if you’re being careful about the names you give things you don’t need to. (but it doesn’t hurt).\n\nmy_data &lt;- read.csv(\"data/2_example.csv\")\n\nThe following two lines create a simple bar graph. ggplot works by layering, using the + symbol. The first line specifies the name of the data.frame and the ‘aesthetics’: we want the scores (which are in a column conveniently named scores in the data.frame) on the x-axis. Then we add geom_bar() as the geometry layer. geom_bar is designed to take a set of scores and calculate the frequencies, which become the height of the bars on the \\(y\\)-axis (which is why we don’t need to explicitly specific a \\(y\\) aesthetic in the aes() function.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThat’s all there is to it: we now have a perfectly serviceable bar graph showing the frequency distribution.\nIf we instead wanted a histogram, there are a couple of ways to do it. Remember, the only visual difference between a bar graph and histogram is that a histogram has bars that are touching, while the bar graph has a bit of space between them. So we can repeat the previous code, but specify that the width of the bars should be 1.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar(width = 1)\n\n\n\n\n\n\n\n\nAnd that basically produces what we want. But there is also a dedicated histogram geom we can use:\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhen you run those lines of code you’ll notice a warning message in the console advising you to pick a new binwidth. That’s because the default is to have 30 bins, which often won’t be appropriate for your data. Here, because sleep duration was recorded ‘to the nearest whole hour’, i.e. the data is integer, we probably want a binwidth of 1 unit, i.e. we want a bar for each hour.\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nNow this is identical to the geom_bar(width = 1) version from before. Note that bar’s width and the binwidth are different things, though. The width is literally the width of the bars; binwidth is the width of the intervals into which scores are grouped. The usefulness of geom_histogram()is that it allows for more flexibility with the binwidth, which can be useful for continuous scores and datasets which cover a width range of scores, for which we might want a grouped frequency histogram with a wider binwidth.\nLastly, we make a frequency polygon in much the same way as a histogram, remembering to specify the appropriate binwidth.\n\nggplot(my_data, aes(x = scores)) + \n  geom_freqpoly(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipBonus: Making things look nice\n\n\n\n\n\nThe default options for the look of the plot are still pretty ugly though. If you’re so inclined, you can customize just about everything about the look and layout of a graph using ggplot. Some aspects are fairly straight forward, like changing the outline and fill of the bars within the geom_histogram() function. We add another layer, labs() to set the axis titles. Stating breaks within scale_x_continuous() determines the labels of the x axis. And then there are all the theme() elements we can change or remove.\n\nggplot(data = my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1, fill = \"grey\", color = \"black\") +\n  labs(x = \"Scores\", y = \"Frequency\") +\n  scale_x_continuous(breaks = 3:9) +\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_line(),\n        axis.line.y = element_line(),\n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nI don’t suggest you get into the weeds on this unless you find that kind of thing satisfying (like I do). But for psych majors it can be a useful exercise figuring out how to produce a graph formatted according to, say, APA conventions, by tweaking the theme options.\nThere’s a learning curve to ggplot, but once you get to grips with it, it is an incredibly powerful and flexible way of visualizing data.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-6-instructions.html",
    "href": "recitation/problem-set-6-instructions.html",
    "title": "Problem Set 6",
    "section": "",
    "text": "The general hypothesis testing procedure for the \\(t\\)-test (and any kind of test) is the same as the procedure we used in the previous problem set for the \\(z\\)-test. The difference is that we’re dealing with a different distribution and a different test statistic.\nStep 1: State your hypotheses\nStep 2: Set decision criteria. As usual, this depends on alpha and whether the test is 1- or 2-tailed. To find the critical regions of the \\(t\\) distribution, we use the qt() function. It works just like qnorm(), but for the \\(t\\) distribution instead of the normal distribution. The values for the \\(t\\) distribution depend on the degrees of freedom for your test, so we must also specify the degrees of freedom as an argument within the function. E.g.,\n\nqt(c(.025, .975), df = 50)\n\n[1] -2.008559  2.008559\n\n\nStep 3: Calculate your test statistic. Do this by plugging the appropriate values into the \\(t\\) equation\nStep 4: Decide. Compare your calculated test statistic with the critical values to determine whether your sample is within the critical region(s), and thus whether to reject the null.\nUnlike the \\(z\\)-test, however, R does have a built-in function for performing a \\(t\\) test. When you’re dealing with real data you can use this rather than following the step-by-step procedure and calculating things manually. For a single-sample \\(t\\)-test, the arguments you need to specify are x (your data) and mu (the population mean), and possibly alternative. This last argument allows you to specify the directionality of the alternative hypothesis (i.e. to conduct a one-tailed or two-tailed hypothesis).\nThe t.test() function doesn’t calculate effect size. For that, you can implement the relevant equation. Or alternatively, you could check out the cohens_d() function from the effectsize package…\nNOTE: You can see documentation for any function by typing a question mark followed by the name of the function (with no parentheses after the name) and running the code. Alternatively, you can click over to the Help tab of the pane in the bottom right and search for a function. The documentation isn’t always that helpful, but it’s worth taking a look when you’re unsure about a function.\n\n?t.test\n\n?effectsize::cohens_d",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "recitation/problem-set-6-instructions.html#instructions",
    "href": "recitation/problem-set-6-instructions.html#instructions",
    "title": "Problem Set 6",
    "section": "",
    "text": "The general hypothesis testing procedure for the \\(t\\)-test (and any kind of test) is the same as the procedure we used in the previous problem set for the \\(z\\)-test. The difference is that we’re dealing with a different distribution and a different test statistic.\nStep 1: State your hypotheses\nStep 2: Set decision criteria. As usual, this depends on alpha and whether the test is 1- or 2-tailed. To find the critical regions of the \\(t\\) distribution, we use the qt() function. It works just like qnorm(), but for the \\(t\\) distribution instead of the normal distribution. The values for the \\(t\\) distribution depend on the degrees of freedom for your test, so we must also specify the degrees of freedom as an argument within the function. E.g.,\n\nqt(c(.025, .975), df = 50)\n\n[1] -2.008559  2.008559\n\n\nStep 3: Calculate your test statistic. Do this by plugging the appropriate values into the \\(t\\) equation\nStep 4: Decide. Compare your calculated test statistic with the critical values to determine whether your sample is within the critical region(s), and thus whether to reject the null.\nUnlike the \\(z\\)-test, however, R does have a built-in function for performing a \\(t\\) test. When you’re dealing with real data you can use this rather than following the step-by-step procedure and calculating things manually. For a single-sample \\(t\\)-test, the arguments you need to specify are x (your data) and mu (the population mean), and possibly alternative. This last argument allows you to specify the directionality of the alternative hypothesis (i.e. to conduct a one-tailed or two-tailed hypothesis).\nThe t.test() function doesn’t calculate effect size. For that, you can implement the relevant equation. Or alternatively, you could check out the cohens_d() function from the effectsize package…\nNOTE: You can see documentation for any function by typing a question mark followed by the name of the function (with no parentheses after the name) and running the code. Alternatively, you can click over to the Help tab of the pane in the bottom right and search for a function. The documentation isn’t always that helpful, but it’s worth taking a look when you’re unsure about a function.\n\n?t.test\n\n?effectsize::cohens_d",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "recitation/problem-set-6-instructions.html#instructions-1",
    "href": "recitation/problem-set-6-instructions.html#instructions-1",
    "title": "Problem Set 6",
    "section": "Instructions",
    "text": "Instructions\nThe equation for a confidence interval is:\n\\[\n\\mu = M \\pm t * s_M\n\\]\nSo this involves computing some critical \\(t\\) values and multiplying by the estimated standard error.\nSuppose we’re dealing with a sample, \\(n = 20\\), \\(M = 100\\), \\(SD = 10\\) . We want to make a 95% confidence interval for that \\(M = 100\\) point estimate.\nStep 1: Find the critical value.\nFor 95% confidence, picture the distribution split into 95% in the middle and therefore 2.5% in each tail. Strictly speaking we should have two critical values, one for each tail. But since it’s symmetrical, we can simplify by just using the upper tail’s value for now.\n\ncritical_score &lt;- qt(.025, df = 19, lower.tail = FALSE)\n\n# or...\ncritical_score &lt;- qt(.975, df = 19)\n\nStep 2: Find the (estimated) standard error\n\nstd_error &lt;- 10/sqrt(20)\n\nStep 3: Multiply the standard error by critical value. This gives the margin of error; the distance from the center of the distribution (the point estimate) to the boundary of the confidence interval.\n\nmargin_of_error &lt;- critical_score * std_error\n\nStep 4: Add and subtract the margin of error to the point estimate to describe the limits of the confidence interval.\n\nci_upper_limit &lt;- 100 + margin_of_error\nci_lower_limit &lt;- 100 - margin_of_error\n\nc(ci_lower_limit, ci_upper_limit)\n\n[1]  95.31986 104.68014\n\n\nThis tells us that the lower boundary of our 95% confidence interval is 95.3 and the upper boundary is 104.7.\nNote that if the population standard deviation was known, i.e. we were dealing with the \\(z\\) distribution instead of \\(t\\), the process would be the same, we would just have to use qnorm() instead of qt() to find the critical score for the appropriate distribution.\n\nUsing actual data\nIn the example above, I gave you the values of \\(n\\), \\(M\\), and \\(SD\\). For actual data, i.e. a data.frame with a column of scores, the logic is all the same, you just need to compute those values from the scores in the ways we’ve practiced.\n\ndf &lt;- read.csv(\"data/6_confidence-interval.csv\")\n\nn &lt;- length(df$scores)\nmean &lt;- mean(df$scores)\nsd &lt;- sd(df$scores)\n\ncritical_score &lt;- qt(.975, df = n - 1) # step 1\nstd_error &lt;- sd / sqrt(n) # step 2\nmargin_of_error &lt;- critical_score * std_error # step 3\n\n# step 4:\nci_upper_limit &lt;- mean + margin_of_error\nci_lower_limit &lt;- mean - margin_of_error\n\nc(ci_lower_limit, ci_upper_limit)\n\n[1]  95.31001 104.68999\n\n\n\n\nMake your own function\nA very useful thing to do would be to take the logic of computing a confidence interval and create your own function to do it. That way if you had to compute more than one confidence interval you can just use your function rather than having to copy and paste a bunch of lines of code and tweak each line.\nStep 1-3 can be reduced to a single line of code:\n\nqt(0.975, df = length(df$scores) - 1) * sd(df$scores) / sqrt(length(df$scores))\n\n[1] 4.689987\n\n\nSo we can just put line that inside a function, replacing the specific df$scores with a generic placeholder x, which stands in for whatever set of scores we will eventually feed into the function as its only argument.\n\nci &lt;- function(x) {\n  qt(0.975, df = length(x) - 1) * sd(x) / sqrt(length(x))\n}\n\nci(df$scores)\n\n[1] 4.689987\n\n\nNote that that function returns the width of the confidence interval–the margin of error. So you would still need to add & subtract it from the point estimate, per Step 4, to find the actual CI boundaries.\n\n\nVisualizing confidence intervals\nThe usefulness of creating a function like that is that you can quickly and easily create a summary of some data, including the point estimate (mean) and confidence interval width, and create a graph using these summary values. For a single confidence interval like this, there’s not actually any data to map on to the x-axis, so I fudge a little there by telling ggplot x = \"\" for the x aesthetic.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ndata_summary &lt;- df |&gt; \n  summarize(mean = mean(scores),\n            ci_width = ci(scores),\n            ci_lower_limit = mean - ci_width,\n            ci_upper_limit = mean + ci_width)\n\ndata_summary |&gt; \n  ggplot(aes(x = \"\", y = mean)) +\n  geom_errorbar(aes(ymax = ci_upper_limit, ymin = ci_lower_limit), width = 0.3) +\n  geom_point()",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 6"
    ]
  },
  {
    "objectID": "recitation/problem-set-8-instructions.html",
    "href": "recitation/problem-set-8-instructions.html",
    "title": "Problem Set 8",
    "section": "",
    "text": "The aov() function can be used to perform ANOVA on data in a data.frame. The arguments supplied to aov() are formula and data (the name of the data.frame containing the relevant data). The formula specifies the DV and grouping variable, in the form DV ~ IV, meaning we want to compare scores on the DV broken into groups according to the IV.\n\naov(formula = DV ~ IV, data = my_data)\n\nFor data from a within-subjects design, you need to include a participant error term in the formula. It will look something like this, where participant is the name of a column identifying each participant in your data (note the capital E in Error!)\n\naov(formula = DV ~ IV + Error(participant/IV), data = my_data)\n\nSince the aov() function requires a formula, it is important to have the data you are using in the correct format. For question 1, the data you are given is in what’s called ‘wide’ format, meaning there are separate columns for each condition of the IV. For the formula argument to work, that data must be in ‘long’ format, meaning just one column for the IV and one for DV scores. Remember to switch the data from one format to the other using the pivot_longer() function from the tidyr package (like we have practiced in some previous problem sets).\n\nlibrary(tidyr)\n\ndata_long &lt;- my_data |&gt; \n  pivot_longer(everything())\n\nIf there is a column identifying participants, as is typical for a within-subjects design, you don’t want that to be pivoted; rather it should get duplicated as another column identifying which participant each score came from. Accordingly, rather than pivoting everything(), you can exclude the id_column (whatever it is named) using the syntax -id_column.\n\ndata_long_within &lt;- my_data |&gt; \n  pivot_longer(-id_column)\n\nMake sure to look at your newly reshaped dataframes and understand how the format has been changed without altering the actual recorded data. Also, make note of the column names (or change them), which you will need to specify in following analyses.\nThe output that aov() produces isn’t very useful by itself. E.g. it doesn’t give us an \\(F\\) value or significance level. To get those, we need to use the summary() function in combination with aov().\n\nmodel_between &lt;- aov(formula = DV ~ IV, data = data_long)\nsummary(model_between)\n\n\n\nFor ANOVA, the most commonly-reported measure of effect size is eta-squared, \\(\\eta^2\\), or partial eta-squared, \\(\\eta^2_p\\). The effectsize package has an eta_squared function which accepts a model produced by aov() and returns that measure of effect size. It works for between- or within-participants models; for the latter you can specify partial = TRUE as an argument within the function.\n\neffectsize::eta_squared(model_between)\neffectsize::eta_squared(model_within, partial = TRUE)\n\n\n\n\nThere is a function that makes performing a Tukey test easy: R’s built-in TukeyHSD() function.\n\nTukeyHSD(model_between)\n\nHowever, TukeyHSD() also only works with between-participants models; for a within-participants model you’ll need to use the mathematical approach from the lectures, using \\(MS_{error}\\) and \\(df_{error}\\). You will find these values in the output of the summary() function.\nYou’ll also need the \\(q\\) statistic, which you can find using qtukey().  This works just like our old friends qnorm() and qt()–you specify a proportion (e.g. .05) and it tells you the corresponding \\(q\\) statistic. Some other arguments are required as well though:\n\nq &lt;- qtukey(p = 0.05, nmeans = 3, df = 10, lower.tail = FALSE)\n\n…where p = alpha, nmeans = \\(k\\), and df = \\(df_{within}\\). The resulting \\(q\\) value should be used in the calculation \n\\[q * \\sqrt{\\dfrac{MSwithin}{n}}\\]\nThis tells you the size of difference between means required to count as statistically significant.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 8"
    ]
  },
  {
    "objectID": "recitation/problem-set-8-instructions.html#instructions",
    "href": "recitation/problem-set-8-instructions.html#instructions",
    "title": "Problem Set 8",
    "section": "",
    "text": "The aov() function can be used to perform ANOVA on data in a data.frame. The arguments supplied to aov() are formula and data (the name of the data.frame containing the relevant data). The formula specifies the DV and grouping variable, in the form DV ~ IV, meaning we want to compare scores on the DV broken into groups according to the IV.\n\naov(formula = DV ~ IV, data = my_data)\n\nFor data from a within-subjects design, you need to include a participant error term in the formula. It will look something like this, where participant is the name of a column identifying each participant in your data (note the capital E in Error!)\n\naov(formula = DV ~ IV + Error(participant/IV), data = my_data)\n\nSince the aov() function requires a formula, it is important to have the data you are using in the correct format. For question 1, the data you are given is in what’s called ‘wide’ format, meaning there are separate columns for each condition of the IV. For the formula argument to work, that data must be in ‘long’ format, meaning just one column for the IV and one for DV scores. Remember to switch the data from one format to the other using the pivot_longer() function from the tidyr package (like we have practiced in some previous problem sets).\n\nlibrary(tidyr)\n\ndata_long &lt;- my_data |&gt; \n  pivot_longer(everything())\n\nIf there is a column identifying participants, as is typical for a within-subjects design, you don’t want that to be pivoted; rather it should get duplicated as another column identifying which participant each score came from. Accordingly, rather than pivoting everything(), you can exclude the id_column (whatever it is named) using the syntax -id_column.\n\ndata_long_within &lt;- my_data |&gt; \n  pivot_longer(-id_column)\n\nMake sure to look at your newly reshaped dataframes and understand how the format has been changed without altering the actual recorded data. Also, make note of the column names (or change them), which you will need to specify in following analyses.\nThe output that aov() produces isn’t very useful by itself. E.g. it doesn’t give us an \\(F\\) value or significance level. To get those, we need to use the summary() function in combination with aov().\n\nmodel_between &lt;- aov(formula = DV ~ IV, data = data_long)\nsummary(model_between)\n\n\n\nFor ANOVA, the most commonly-reported measure of effect size is eta-squared, \\(\\eta^2\\), or partial eta-squared, \\(\\eta^2_p\\). The effectsize package has an eta_squared function which accepts a model produced by aov() and returns that measure of effect size. It works for between- or within-participants models; for the latter you can specify partial = TRUE as an argument within the function.\n\neffectsize::eta_squared(model_between)\neffectsize::eta_squared(model_within, partial = TRUE)\n\n\n\n\nThere is a function that makes performing a Tukey test easy: R’s built-in TukeyHSD() function.\n\nTukeyHSD(model_between)\n\nHowever, TukeyHSD() also only works with between-participants models; for a within-participants model you’ll need to use the mathematical approach from the lectures, using \\(MS_{error}\\) and \\(df_{error}\\). You will find these values in the output of the summary() function.\nYou’ll also need the \\(q\\) statistic, which you can find using qtukey().  This works just like our old friends qnorm() and qt()–you specify a proportion (e.g. .05) and it tells you the corresponding \\(q\\) statistic. Some other arguments are required as well though:\n\nq &lt;- qtukey(p = 0.05, nmeans = 3, df = 10, lower.tail = FALSE)\n\n…where p = alpha, nmeans = \\(k\\), and df = \\(df_{within}\\). The resulting \\(q\\) value should be used in the calculation \n\\[q * \\sqrt{\\dfrac{MSwithin}{n}}\\]\nThis tells you the size of difference between means required to count as statistically significant.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 8"
    ]
  },
  {
    "objectID": "slides/04_central-tendency.html#finding-the-center",
    "href": "slides/04_central-tendency.html#finding-the-center",
    "title": "PSYC BC1101",
    "section": "Finding the center",
    "text": "Finding the center\nWhere is the center of the distribution?"
  },
  {
    "objectID": "slides/04_central-tendency.html#finding-the-center-1",
    "href": "slides/04_central-tendency.html#finding-the-center-1",
    "title": "PSYC BC1101",
    "section": "Finding the center",
    "text": "Finding the center\nWhere is the center of the distribution?"
  },
  {
    "objectID": "slides/04_central-tendency.html#finding-the-center-2",
    "href": "slides/04_central-tendency.html#finding-the-center-2",
    "title": "PSYC BC1101",
    "section": "Finding the center",
    "text": "Finding the center\nWhere is the center of the distribution?"
  },
  {
    "objectID": "slides/04_central-tendency.html#measures-of-central-tendency",
    "href": "slides/04_central-tendency.html#measures-of-central-tendency",
    "title": "PSYC BC1101",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\nImagine you get the following grades:\n\n90, 0, 80, 85, 90\n\nHow do you fairly describe all these scores with a single number?\nThree ways:\n\nMode: grade you get most often\nMedian = grade that divides lowest 50% of scores from highest 50% of scores\nMean = sum of grades / # of grades = \\(\\dfrac{\\Sigma X} N\\)"
  },
  {
    "objectID": "slides/04_central-tendency.html#measures-of-central-tendency-1",
    "href": "slides/04_central-tendency.html#measures-of-central-tendency-1",
    "title": "PSYC BC1101",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\nImagine you get the following grades:\n\n90, 0, 80, 85, 90\n\nThree ways:\n\nMode = 90, 0, 80, 85, 90\nMedian = 0, 80, 85, 90, 90\nMean = (90 + 0 + 80 + 85 + 90) / 5 = 69"
  },
  {
    "objectID": "slides/04_central-tendency.html#mode-1",
    "href": "slides/04_central-tendency.html#mode-1",
    "title": "PSYC BC1101",
    "section": "Mode",
    "text": "Mode\n\nThe score/category with the greatest frequency\n\nWhat occurs most often?\n\n\n\n\n\n\n\n\n\n\\(X\\)\n\\(f\\)\n\n\n\n\n5\n1\n\n\n4\n2\n\n\n3\n4\n\n\n2\n5\n\n\n1\n3\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\n\\(f\\)\n\n\n\n\n90-99\n7\n\n\n80-89\n4\n\n\n70-79\n5\n\n\n60-60\n3\n\n\n50-59\n0\n\n\n40-49\n1"
  },
  {
    "objectID": "slides/04_central-tendency.html#mode-generic-examples",
    "href": "slides/04_central-tendency.html#mode-generic-examples",
    "title": "PSYC BC1101",
    "section": "Mode: generic examples",
    "text": "Mode: generic examples"
  },
  {
    "objectID": "slides/04_central-tendency.html#mode-realistic-example",
    "href": "slides/04_central-tendency.html#mode-realistic-example",
    "title": "PSYC BC1101",
    "section": "Mode: realistic example",
    "text": "Mode: realistic example"
  },
  {
    "objectID": "slides/04_central-tendency.html#median-1",
    "href": "slides/04_central-tendency.html#median-1",
    "title": "PSYC BC1101",
    "section": "Median",
    "text": "Median\n\nDefinition:\n\nThe midpoint of the scores in a distribution when they are listed in order from smallest to largest\nDivides the scores into two groups of equal size\nEqual number of data points either side of the median\n50% below, 50% above"
  },
  {
    "objectID": "slides/04_central-tendency.html#locating-the-median",
    "href": "slides/04_central-tendency.html#locating-the-median",
    "title": "PSYC BC1101",
    "section": "Locating the median",
    "text": "Locating the median\n\nPut scores in order\nFind the number that gives and equal number of scores on either side\nOdd number of scores\n\nMedian is the center score\n\n\n\n1 2 3 4 5"
  },
  {
    "objectID": "slides/04_central-tendency.html#locating-the-median-1",
    "href": "slides/04_central-tendency.html#locating-the-median-1",
    "title": "PSYC BC1101",
    "section": "Locating the median",
    "text": "Locating the median\n\nPut scores in order\nFind the number that gives and equal number of scores on either side\nEven number of scores:\n\nAverage the 2 numbers either side of center\n\n\n\n1 2 3 | 4 5 6\n(3 + 4) / 2 = 3.5\n65 70 70 80 90 90\n65 70 80 80 80 90 92 95"
  },
  {
    "objectID": "slides/04_central-tendency.html#mean-1",
    "href": "slides/04_central-tendency.html#mean-1",
    "title": "PSYC BC1101",
    "section": "Mean",
    "text": "Mean\n\n\n\nWhat is the “average”?\n\nTake a set of scores\nAdd them up\nDivide by how many there are\n\nDeveloped in the 16th century\n\nMainly used by astronomers\n\nAdolphe Quetelet (1796-1874)\n\nApplied the concept to people\nSize measurements (BMI), divorce, crime, suicide\nSee The Atlantic: How the Idea of a ‘Normal’ Person Got Invented"
  },
  {
    "objectID": "slides/04_central-tendency.html#history",
    "href": "slides/04_central-tendency.html#history",
    "title": "PSYC BC1101",
    "section": "History",
    "text": "History\n\n\n\nAmerican Civil War\n\nMass production of uniforms\nSmall, Medium, Large\nAlso food rations, weapon design, etc\n\n1926: Plane cockpits\n\nBased on average measurements\nBy WW2 worked terribly\nDidn’t fit most pilots\nNobody is average on all dimensions"
  },
  {
    "objectID": "slides/04_central-tendency.html#calculating-the-mean",
    "href": "slides/04_central-tendency.html#calculating-the-mean",
    "title": "PSYC BC1101",
    "section": "Calculating the mean",
    "text": "Calculating the mean\n\nSum of scores divided by number of scores\nRepresented by a symbol (unlike mode & median)\n\n\n\nSample: \\(M = \\dfrac{\\Sigma X} n\\)\n(sometimes \\(\\overline{X}\\))\n\nPopulation: \\(\\mu = \\dfrac{\\Sigma X} N\\)"
  },
  {
    "objectID": "slides/04_central-tendency.html#visualizing-the-mean",
    "href": "slides/04_central-tendency.html#visualizing-the-mean",
    "title": "PSYC BC1101",
    "section": "Visualizing the mean",
    "text": "Visualizing the mean\n\nAnother way of thinking about the mean\n\nThe balance point for the distribution"
  },
  {
    "objectID": "slides/04_central-tendency.html#distributions-income",
    "href": "slides/04_central-tendency.html#distributions-income",
    "title": "PSYC BC1101",
    "section": "Distributions: income",
    "text": "Distributions: income\n\nSensitivity to outliers\n\nExtreme values; observations far from the center\nMean is more influenced by outliers than median"
  },
  {
    "objectID": "slides/04_central-tendency.html#distributions-income-1",
    "href": "slides/04_central-tendency.html#distributions-income-1",
    "title": "PSYC BC1101",
    "section": "Distributions: income",
    "text": "Distributions: income"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#independent-samples",
    "href": "slides/17_related-samples-ANOVA.html#independent-samples",
    "title": "PSYC BC1101",
    "section": "Independent-samples",
    "text": "Independent-samples\n\n\n\nIndependent-measures ANOVA\n\n“Between-participants”\nDifferent participants in each treatment\nBetween-groups variability could be due to treatment effect and/or individual differences"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#related-samples",
    "href": "slides/17_related-samples-ANOVA.html#related-samples",
    "title": "PSYC BC1101",
    "section": "Related samples",
    "text": "Related samples\n\n\n\nRepeated-measures (within-participants)\n\nSame participants in each treatment\n\nOr matched-samples\n\nMatched participants in each treatment"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#related-samples-1",
    "href": "slides/17_related-samples-ANOVA.html#related-samples-1",
    "title": "PSYC BC1101",
    "section": "Related samples",
    "text": "Related samples\n\n\n\nAdvantages\n\nEliminates individual differences as a source of variability between treatments\nSmaller number of participants needed\n\nDisadvantages\n\nSomething other than the treatment may cause participant’s scores to change\nE.g. practice, world events, natural improvement"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#partitioning-variance",
    "href": "slides/17_related-samples-ANOVA.html#partitioning-variance",
    "title": "PSYC BC1101",
    "section": "Partitioning variance",
    "text": "Partitioning variance\n\nIndependent samples\n\n\n\nTotal variance\n\n\nVariance between treatments\n\\(MS_{between}\\)\n\n\n\nTreatment effect\nSampling error\nIndividual differences\n\n\n\nVariance within groups\n\\(MS_{within}\\)\n\n\n\nSampling error\nIndividual differences\n\n\n\n\n\\(F = \\dfrac{{MS_{between}}}{{MS_{within}}} = \\dfrac{treatment \\cdot error \\cdot ind. \\ diff.}{error \\cdot ind. \\ diff.}\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#partitioning-variance-1",
    "href": "slides/17_related-samples-ANOVA.html#partitioning-variance-1",
    "title": "PSYC BC1101",
    "section": "Partitioning variance",
    "text": "Partitioning variance\n\nRelated samples: problem\n\n\n\nTotal variance\n\n\nVariance between treatments\n\\(MS_{between \\ treatments}\\)\n\n\n\nTreatment effect\nSampling error\n\n\n\nVariance within groups\n\\(MS_{within}\\)\n\n\n\nSampling error\nIndividual differences\n\n\n\n\n\\(F = \\dfrac{{MS_{between \\ treatments}}}{{MS_{within}}} = \\dfrac{treatment \\cdot error}{error \\cdot ind. \\ diff.}\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#partitioning-variance-2",
    "href": "slides/17_related-samples-ANOVA.html#partitioning-variance-2",
    "title": "PSYC BC1101",
    "section": "Partitioning variance",
    "text": "Partitioning variance\n\nRelated samples: solution\n\n\n\nTotal variance\n\n\nVariance between treatments\n\\(MS_{between \\ treatments}\\)\n\n\n\nTreatment effect\nSampling error\n\n\n\nVariance within groups\n\\(MS_{within}\\)\n\n\nError\n\\(MS_{error}\\)\n\n\nBetween subjects\n\\(MS_{between \\ S's}\\)\n\n\n\nSampling error\n\n\n\n\nIndividual differences\n\n\n\n\n\\(F = \\dfrac{{MS_{between \\ treatments}}}{{MS_{error}}} = \\dfrac{treatment \\cdot error}{error}\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#calculations-ss-and-df",
    "href": "slides/17_related-samples-ANOVA.html#calculations-ss-and-df",
    "title": "PSYC BC1101",
    "section": "Calculations: \\(SS\\) and \\(df\\)",
    "text": "Calculations: \\(SS\\) and \\(df\\)\n\n\n\n\\(SS_{total}\\)\n\\(SS_{btwn \\ T's}\\) \\(SS_{within}\\)\n\\(SS_{error}\\) \\(SS_{btwn \\ S's}\\)\n\n\\(df_{total}\\)\n\\(df_{btwn \\ T's}\\) \\(df_{within}\\)\n\\(df_{error}\\) \\(df_{btwn \\ S's}\\)\n\n\n\n\\(SS_{total} = \\Sigma X^2 - \\dfrac{G^2}{N}\\)\n\\(SS_{within} = \\Sigma SS_{within \\ each \\ treatment}\\)\n\\(SS_{between \\ treatments} = \\Sigma \\dfrac{T^2}{n} - \\dfrac{G^2}{N}\\)\n\\(SS_{between \\ subjects} = \\Sigma \\dfrac{P^2}{k} - \\dfrac{G^2}{N}\\)\n\\(SS_{error} = SS_{within}-SS_{between \\ subjects}\\)\n\n\\(df_{total} = N-1\\)\n\\(df_{within} = N-k\\)\n\\(df_{between \\ treatments} = k-1\\)\n\\(df_{between \\ subjects} = n-1\\)\n\\(df_{error} = df_{within}-df_{between \\ subjects}\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#calculations-ms-and-f",
    "href": "slides/17_related-samples-ANOVA.html#calculations-ms-and-f",
    "title": "PSYC BC1101",
    "section": "Calculations: \\(MS\\) and \\(F\\)",
    "text": "Calculations: \\(MS\\) and \\(F\\)\n\nStep 2. \\(MS\\) values\n\n\\[\\begin{align}\nMS_{between \\ treatments} &= \\dfrac{SS_{between \\ treatments}}{df_{between \\ treatments}} \\\\\n\\ \\\\\nMS_{error} &= \\dfrac{SS_{error}}{df_{error}}\n\\end{align}\\]\n\nStep 3. \\(F\\)-ratio\n\n\n\\(F = \\dfrac{MS_{between \\ treatments}}{MS_{error}}\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#summary-table",
    "href": "slides/17_related-samples-ANOVA.html#summary-table",
    "title": "PSYC BC1101",
    "section": "Summary table",
    "text": "Summary table\n\n\n\n\n\nSource\n\\(SS\\)\n\\(df\\)\n\\(MS\\)\n\\(F\\)\n\n\n\n\nBetween treatments\n\n\n\n\n\n\nWithin treatments\n\n\n\n\n\n\n   Between subjects\n\n\n\n\n\n\n   Error\n\n\n\n\n\n\nTotal"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#step-1.-state-hypotheses",
    "href": "slides/17_related-samples-ANOVA.html#step-1.-state-hypotheses",
    "title": "PSYC BC1101",
    "section": "Step 1. State hypotheses",
    "text": "Step 1. State hypotheses\n\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n\nPerson\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\n\n\n\nA\n9\n3\n5\n\n\nB\n11\n5\n6\n\n\nC\n13\n4\n7\n\n\n\n\n\n\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\)\n\\(H_1\\) : At least one population mean differs from another"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#step-2.-critical-region",
    "href": "slides/17_related-samples-ANOVA.html#step-2.-critical-region",
    "title": "PSYC BC1101",
    "section": "Step 2. Critical region",
    "text": "Step 2. Critical region\n\nNumerator: \\(df_{between \\ treatments} = k-1\\)\nDenominator: \\(df_{error} = df_{within}-df_{between \\ S's} = (N-k)-(n-1)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha = .05\\)\n\n\n\\(df_{numerator}\\)\n\n\n\n\\(df_{denominator}\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n161.45\n199.50\n215.71\n224.58\n230.16\n233.99\n236.77\n238.88\n240.54\n241.88\n\n\n2\n18.51\n19.00\n19.16\n19.25\n19.30\n19.33\n19.35\n19.37\n19.39\n19.40\n\n\n3\n10.13\n9.55\n9.28\n9.12\n9.01\n8.94\n8.89\n8.85\n8.81\n8.79\n\n\n4\n7.71\n6.94\n6.59\n6.39\n6.26\n6.16\n6.09\n6.04\n6.00\n5.96\n\n\n5\n6.61\n5.79\n5.41\n5.19\n5.05\n4.95\n4.88\n4.82\n4.77\n4.74\n\n\n6\n5.99\n5.14\n4.76\n4.53\n4.39\n4.28\n4.21\n4.15\n4.10\n4.06\n\n\n7\n5.59\n4.74\n4.35\n4.12\n3.97\n3.87\n3.79\n3.73\n3.68\n3.64\n\n\n8\n5.32\n4.46\n4.07\n3.84\n3.69\n3.58\n3.50\n3.44\n3.39\n3.35\n\n\n9\n5.12\n4.26\n3.86\n3.63\n3.48\n3.37\n3.29\n3.23\n3.18\n3.14\n\n\n10\n4.96\n4.10\n3.71\n3.48\n3.33\n3.22\n3.13\n3.07\n3.02\n2.98"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#calculate-f-ratio",
    "href": "slides/17_related-samples-ANOVA.html#calculate-f-ratio",
    "title": "PSYC BC1101",
    "section": "3. Calculate \\(F\\)-ratio",
    "text": "3. Calculate \\(F\\)-ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n\n\nPerson\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\nP\n\n\n\n\nA\n9\n3\n5\n17\n\n\nB\n11\n5\n6\n22\n\n\nC\n13\n4\n7\n24\n\n\n\\(M\\)\n11\n4\n6\n\n\n\n\\(T\\)\n33\n12\n18\n\n\n\n\\(SS\\)\n8\n2\n2\n\n\n\n\n\n\n\n\\(n = 3\\)\n\\(k = 3\\)\n\\(N = 9\\)\n\\(G = 63\\)\n\\(\\Sigma X^2 = 531\\)\n\n\n\\[\\begin{align}\ndf_{total} &= N-1 = 8 \\\\\ndf_{within} &= N-k  = 6 \\\\\ndf_{between \\ treatments} &= k-1  = 2 \\\\\ndf_{between \\ subjects} &= n-1  = 2\\\\\ndf_{error} &= df_{within}-df_{between \\ subjects}  = 4\n\\end{align}\\]"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#calculate-f-ratio-1",
    "href": "slides/17_related-samples-ANOVA.html#calculate-f-ratio-1",
    "title": "PSYC BC1101",
    "section": "3. Calculate \\(F\\)-ratio",
    "text": "3. Calculate \\(F\\)-ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n\n\nPerson\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\nP\n\n\n\n\nA\n9\n3\n5\n17\n\n\nB\n11\n5\n6\n22\n\n\nC\n13\n4\n7\n24\n\n\n\\(M\\)\n11\n4\n6\n\n\n\n\\(T\\)\n33\n12\n18\n\n\n\n\\(SS\\)\n8\n2\n2\n\n\n\n\n\n\n\n\\(n = 3\\)\n\\(k = 3\\)\n\\(N = 9\\)\n\\(G = 63\\)\n\\(\\Sigma X^2 = 531\\)\n\n\n\n\n\\(SS_{total} = \\Sigma X^2 - \\dfrac{G^2}{N} = 90\\)\n\\(SS_{within} = \\Sigma SS_{within \\ each \\ treatment} = 12\\)\n\\(SS_{between \\ treatments} = \\Sigma \\dfrac{T^2}{n} - \\dfrac{G^2}{N} = 78\\)\n\n\\(SS_{between \\ subjects} = \\Sigma \\dfrac{P^2}{k} - \\dfrac{G^2}{N} = 8.67\\)\n\\(SS_{error} = \\Sigma SS_{within}-SS_{between \\ subjects} = 3.33\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#calculate-f-ratio-2",
    "href": "slides/17_related-samples-ANOVA.html#calculate-f-ratio-2",
    "title": "PSYC BC1101",
    "section": "3. Calculate \\(F\\)-ratio",
    "text": "3. Calculate \\(F\\)-ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n\n\nPerson\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\nP\n\n\n\n\nA\n9\n3\n5\n17\n\n\nB\n11\n5\n6\n22\n\n\nC\n13\n4\n7\n24\n\n\n\\(M\\)\n11\n4\n6\n\n\n\n\\(T\\)\n33\n12\n18\n\n\n\n\\(SS\\)\n8\n2\n2\n\n\n\n\n\n\n\n\\(n = 3\\)\n\\(k = 3\\)\n\\(N = 9\\)\n\\(G = 63\\)\n\\(\\Sigma X^2 = 531\\)\n\n\n\\(MS_{between \\ treatments} = \\dfrac{SS_{between \\ treatments}}{df_{between \\ treatments}} = 39\\)\n\\(MS_{error} = \\dfrac{SS_{error}}{df_{error}} = 0.83\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#calculate-f-ratio-3",
    "href": "slides/17_related-samples-ANOVA.html#calculate-f-ratio-3",
    "title": "PSYC BC1101",
    "section": "3. Calculate \\(F\\)-ratio",
    "text": "3. Calculate \\(F\\)-ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n\n\nPerson\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\nP\n\n\n\n\nA\n9\n3\n5\n17\n\n\nB\n11\n5\n6\n22\n\n\nC\n13\n4\n7\n24\n\n\n\\(M\\)\n11\n4\n6\n\n\n\n\\(T\\)\n33\n12\n18\n\n\n\n\\(SS\\)\n8\n2\n2\n\n\n\n\n\n\n\n\\(n = 3\\)\n\\(k = 3\\)\n\\(N = 9\\)\n\\(G = 63\\)\n\\(\\Sigma X^2 = 531\\)\n\n\n\\(F = \\dfrac{MS_{between \\ treatments}}{MS_{error}} = 46.8\\)"
  },
  {
    "objectID": "slides/17_related-samples-ANOVA.html#report-results",
    "href": "slides/17_related-samples-ANOVA.html#report-results",
    "title": "PSYC BC1101",
    "section": "Report results",
    "text": "Report results\n\nA single-factor, related-samples ANOVA revealed a significant difference in people’s test scores when the test was preceded by consumption of a banana (\\(M = 11.00\\); \\(SD = 2.00\\)), a candy bar (\\(M = 4.00\\); \\(SD = 1.00\\)), and no snack (\\(M = 6.00\\); \\(SD = 1.00\\)); \\(F(2,4) = 46.8\\), \\(p &lt; .05\\), \\(\\eta^2 = 0.96\\). Post-hoc tests using Tukey’s HSD revealed that test scores were significantly better following banana consumption than following no snack or candy consumption; the candy did not differ significantly from the control condition."
  },
  {
    "objectID": "slides/11_the-t-test.html#z-test",
    "href": "slides/11_the-t-test.html#z-test",
    "title": "PSYC BC1101",
    "section": "\\(z\\)-test",
    "text": "\\(z\\)-test\n\nUseful if we know everything about original population\n\n\n\n\n\n\\(z = \\dfrac{M - \\mu}{\\sigma_M}\\)"
  },
  {
    "objectID": "slides/11_the-t-test.html#problem",
    "href": "slides/11_the-t-test.html#problem",
    "title": "PSYC BC1101",
    "section": "Problem",
    "text": "Problem\n\nOften don’t know everything about original population\n\n\n\n\n\n\\(\\renewcommand{\\CancelColor}{\\red}z = \\dfrac{M - \\mu}{\\require{enclose}\\enclose{horizontalstrike}{\\sigma_M}}\\)"
  },
  {
    "objectID": "slides/11_the-t-test.html#t-test-solution",
    "href": "slides/11_the-t-test.html#t-test-solution",
    "title": "PSYC BC1101",
    "section": "\\(t\\)-test solution",
    "text": "\\(t\\)-test solution\n\nEstimate population variability using sample\n\n\n\n\n\n\\(t = \\dfrac{M-\\mu}{s_M}\\)"
  },
  {
    "objectID": "slides/11_the-t-test.html#the-t-statistic",
    "href": "slides/11_the-t-test.html#the-t-statistic",
    "title": "PSYC BC1101",
    "section": "The \\(t\\) statistic",
    "text": "The \\(t\\) statistic\n\nEstimated standard error \\(s_M\\) used in place of (unknown) population standard error \\(\\sigma_M\\)\n\n\n\\[z = \\dfrac{M - \\mu}{\\require{enclose}\\enclose{horizontalstrike}{\\sigma_M}}\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\nt = \\dfrac{M - \\mu}{s_M}\\]\n\\[\n\\begin{align}\n\\text{Standard error} = \\sigma_M = \\dfrac{\\sigma}{\\sqrt{n}}\n\\ \\text{or...} \\\n\\dfrac{\\sqrt{\\sigma^2}}{\\sqrt{n}} \\\n\\text{or...} \\\n\\sqrt{\\dfrac{\\sigma^2}{n}}\n\\\\ \\\\\n\\text{Estimated standard error} = s_M = \\dfrac{s}{\\sqrt{n}}\n\\ \\text{or...} \\\n\\dfrac{\\sqrt{s^2}}{\\sqrt{n}} \\\n\\text{or...} \\\n\\sqrt{\\dfrac{s^2}{n}}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/11_the-t-test.html#degrees-of-freedom",
    "href": "slides/11_the-t-test.html#degrees-of-freedom",
    "title": "PSYC BC1101",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\n\n\\(df\\) depends on kind of \\(t\\)-test you’re doing\nSingle sample \\(t\\)-test: \\(df = n - 1\\)\n\n\\(\\text{Population variance} = \\sigma^2 = \\dfrac{SS}{N}\\)\n\\(\\text{Sample variance} = s^2 = \\dfrac{SS}{df} = \\dfrac{SS}{n-1}\\)\n\\(\\text{Sample standard deviation} = s = \\sqrt{\\dfrac{SS}{df}} = \\sqrt{\\dfrac{SS}{n-1}}\\)"
  },
  {
    "objectID": "slides/11_the-t-test.html#the-t-distribution-1",
    "href": "slides/11_the-t-test.html#the-t-distribution-1",
    "title": "PSYC BC1101",
    "section": "The \\(t\\) distribution",
    "text": "The \\(t\\) distribution\n\n\n\nviewof df = html`&lt;input type=range min=1 max=50 step=1 value=1 style=\"width: 50%;\"&gt;`\n\n\n\n\n\n\n\n\njStat = require(\"https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js\")\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\n\n\nheight = 400\nwidth = 800\n\n\n&lt;!-- comment --&gt;\ntex`df = ${df.toLocaleString(\"en\")}`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata = {\n  var values = jStat(-4, 4, 210)[0],\n      &lt;!-- df = df, --&gt;\n      arr = [];\n  for (var i in values) {\n    arr.push(\n      {\n        value: values[i], \n        density: jStat.studentt.pdf(values[i], df)\n      }\n    )\n  }\n  return arr;\n}\n\nnorm_data = {\n  var values = jStat(-4, 4, 210)[0],\n      &lt;!-- df = df, --&gt;\n      arr = [];\n  for (var i in values) {\n    arr.push(\n      {\n        value: values[i],\n        density: jStat.normal.pdf(values[i], 0, 1)\n      }\n    )\n  }\n  return arr;\n}\n\n&lt;!-- norm_data --&gt;\n\n\nchart = {\n  const svg = d3.select(DOM.svg(width, height));\n\n  &lt;!-- svg.append(\"g\") --&gt;\n  &lt;!--     .call(xAxis); --&gt;\n\n  &lt;!-- svg.append(\"g\") --&gt;\n  &lt;!--     .call(yAxis); --&gt;\n  \n  svg.append(\"path\")\n      .datum(data)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"red\")\n      .attr(\"stroke-width\", 4)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"d\", line);\n      \n  svg.append(\"path\")\n      .datum(norm_arr)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"black\")\n      .attr(\"stroke-width\", 2)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"stroke-dasharray\", \"5, 5\")\n      .attr(\"d\", line)\n      .attr(\"class\", \"invertable\");\n  \n  return svg.node();\n}\n\n&lt;!-- margin = ({top: 20, right: 0, bottom: 30, left: 40}) --&gt;\nmargin = ({top: 20, right: 0, bottom: 0, left: 0})\n\nline = d3.line()\n    .x(d =&gt; x(d.value))\n    .y(d =&gt; y(d.density))\n\nx = d3.scaleLinear()\n  .domain([d3.min(data, d =&gt; d.value * 0.9), d3.max(data, d =&gt; d.value * 0.9)]).nice()\n  .range([margin.left, width - margin.right])\n\n&lt;!-- y = d3.scaleLinear() --&gt;\n&lt;!--   .domain([d3.min(data, d =&gt; d.density * 0.9), d3.max(data, d =&gt; d.density / 0.9)]) --&gt;\n&lt;!--   .range([height - margin.bottom, margin.top]) --&gt;\n  \ny = d3.scaleLinear()\n  .domain([0, 0.4])\n  .range([height - margin.bottom, margin.top])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal distribution \\(t\\) distribution"
  },
  {
    "objectID": "slides/11_the-t-test.html#t-table",
    "href": "slides/11_the-t-test.html#t-table",
    "title": "PSYC BC1101",
    "section": "\\(t\\) table",
    "text": "\\(t\\) table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProportion\nin 1 tail\n\n\n\n0.1\n\n\n\n0.05\n\n\n\n0.025\n\n\n\n0.01\n\n\n\n0.005\n\n\n\nProportion\nin 2 tails\n0.2\n0.1\n0.05\n0.02\n0.01\n\n\n\n\n1\n3.078\n6.314\n12.706\n31.821\n63.657\n\n\n2\n1.886\n2.920\n4.303\n6.965\n9.925\n\n\n3\n1.638\n2.353\n3.182\n4.541\n5.841\n\n\n4\n1.533\n2.132\n2.776\n3.747\n4.604\n\n\n5\n1.476\n2.015\n2.571\n3.365\n4.032\n\n\n6\n1.440\n1.943\n2.447\n3.143\n3.707\n\n\n7\n1.415\n1.895\n2.365\n2.998\n3.499\n\n\n\\(df\\)       8\n1.397\n1.860\n2.306\n2.896\n3.355\n\n\n9\n1.383\n1.833\n2.262\n2.821\n3.250\n\n\n10\n1.372\n1.812\n2.228\n2.764\n3.169\n\n\n11\n1.363\n1.796\n2.201\n2.718\n3.106\n\n\n12\n1.356\n1.782\n2.179\n2.681\n3.055\n\n\n13\n1.350\n1.771\n2.160\n2.650\n3.012\n\n\n14\n1.345\n1.761\n2.145\n2.624\n2.977\n\n\n15\n1.341\n1.753\n2.131\n2.602\n2.947\n\n\n...\n...\n...\n...\n...\n..."
  },
  {
    "objectID": "slides/11_the-t-test.html#t-table-r",
    "href": "slides/11_the-t-test.html#t-table-r",
    "title": "PSYC BC1101",
    "section": "\\(t\\) table & R",
    "text": "\\(t\\) table & R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProportion\nin 1 tail\n\n\n\n0.1\n\n\n\n0.05\n\n\n\n0.025\n\n\n\n0.01\n\n\n\n0.005\n\n\n\nProportion\nin 2 tails\n0.2\n0.1\n0.05\n0.02\n0.01\n\n\n\n\n1\n3.078\n6.314\n12.706\n31.821\n63.657\n\n\n2\n1.886\n2.920\n4.303\n6.965\n9.925\n\n\n3\n1.638\n2.353\n3.182\n4.541\n5.841\n\n\n4\n1.533\n2.132\n2.776\n3.747\n4.604\n\n\n5\n1.476\n2.015\n2.571\n3.365\n4.032\n\n\n6\n1.440\n1.943\n2.447\n3.143\n3.707\n\n\n7\n1.415\n1.895\n2.365\n2.998\n3.499\n\n\n\\(df\\)       8\n1.397\n1.860\n2.306\n2.896\n3.355\n\n\n9\n1.383\n1.833\n2.262\n2.821\n3.250\n\n\n10\n1.372\n1.812\n2.228\n2.764\n3.169\n\n\n11\n1.363\n1.796\n2.201\n2.718\n3.106\n\n\n12\n1.356\n1.782\n2.179\n2.681\n3.055\n\n\n13\n1.350\n1.771\n2.160\n2.650\n3.012\n\n\n14\n1.345\n1.761\n2.145\n2.624\n2.977\n\n\n15\n1.341\n1.753\n2.131\n2.602\n2.947\n\n\n...\n...\n...\n...\n...\n...\n\n\n\n\n\n\n\nUsing R\npt() and qt() instead of pnorm() and qnorm()\n\n\nqnorm(.05)\n\n[1] -1.644854\n\nqt(.05)\n\nError in qt(0.05): argument \"df\" is missing, with no default\n\nqt(.05, df = 5)\n\n[1] -2.015048\n\nqt(.05, df = 10)\n\n[1] -1.812461"
  },
  {
    "objectID": "slides/11_the-t-test.html#class-reaction-times",
    "href": "slides/11_the-t-test.html#class-reaction-times",
    "title": "PSYC BC1101",
    "section": "Class reaction times",
    "text": "Class reaction times\n\n\n [1] 327.0 335.0 359.0 430.0 275.4 272.0 350.0 343.2 278.0 354.0 303.0 328.0\n[13] 371.0 312.0 346.0 359.0    NA 259.0 313.6 258.0 244.0 374.4    NA 338.0\n[25] 290.0\n\n\n\n\n\n\n\n\n\nRT\n\\(f\\)\n\n\n\n\n240-259\n3\n\n\n260-279\n3\n\n\n280-299\n1\n\n\n300-319\n3\n\n\n320-339\n4\n\n\n340-359\n6\n\n\n360-379\n2\n\n\n380-399\n0\n\n\n400-419\n0\n\n\n420-439\n1"
  },
  {
    "objectID": "slides/11_the-t-test.html#hypothesis-test",
    "href": "slides/11_the-t-test.html#hypothesis-test",
    "title": "PSYC BC1101",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\nFour steps:\n\n1: State the null and alternative hypotheses\n2: Locate the critical region using the \\(t\\) distribution probabilities, \\(df\\), and \\(\\alpha\\)\n3: Calculate the \\(t\\) test statistic\n4: Make a decision regarding \\(H_0\\) (null hypothesis)"
  },
  {
    "objectID": "slides/11_the-t-test.html#state-hypotheses",
    "href": "slides/11_the-t-test.html#state-hypotheses",
    "title": "PSYC BC1101",
    "section": "1. State hypotheses",
    "text": "1. State hypotheses\n\nStep 1: State hypotheses\n\n\\(H_0\\): Stats students have the same average reaction times as the general population \\(\\mu = 284\\)\n\\(H_1\\): Stats students have different average reaction times to the general population"
  },
  {
    "objectID": "slides/11_the-t-test.html#decision-criterion",
    "href": "slides/11_the-t-test.html#decision-criterion",
    "title": "PSYC BC1101",
    "section": "2. Decision criterion",
    "text": "2. Decision criterion\n\n\n\nSpecify \\(\\alpha\\), identify critical region(s)\nFor \\(t\\), depends on \\(df\\) and thus \\(n\\)\nFor single-sample \\(t\\)-test, \\(df = n – 1\\)\n\n\n\n\n\n\n\n\n\\(df\\)\n\\(\\alpha = .05\\)\n\n\n\n\n1\n12.706\n\n\n2\n4.303\n\n\n3\n3.182\n\n\n4\n2.776\n\n\n5\n2.571\n\n\n...\n...\n\n\n20\n2.086\n\n\n21\n2.080\n\n\n22\n2.074\n\n\n23\n2.069\n\n\n24\n2.064\n\n\n25\n2.060\n\n\n26\n2.056\n\n\n27\n2.052\n\n\n28\n2.048\n\n\n29\n2.045\n\n\n30\n2.042\n\n\n...\n..."
  },
  {
    "objectID": "slides/11_the-t-test.html#calculate-statistic",
    "href": "slides/11_the-t-test.html#calculate-statistic",
    "title": "PSYC BC1101",
    "section": "3. Calculate statistic",
    "text": "3. Calculate statistic\n\nCalculate \\(t\\)-statistic for the sample mean\nQuantifies the difference between the observed sample mean and the hypothesized population mean divided by the estimated standard error\n\n\n\n\\(\\mu = 284 \\\\ M = 322.59 \\\\ SD = 45.31 \\\\ n = 23\\)\n\n\\[\\begin{align}\nt = \\dfrac{M - \\mu}{s_M} &= \\dfrac{322.59 - 284}{45.31/\\sqrt{23}} \\\\\n&= \\dfrac{38.59}{9.45} \\\\\n&= 4.08\n\\end{align}\\]"
  },
  {
    "objectID": "slides/11_the-t-test.html#make-decision",
    "href": "slides/11_the-t-test.html#make-decision",
    "title": "PSYC BC1101",
    "section": "4. Make decision",
    "text": "4. Make decision\n\nStep 4a: Make a decision about \\(H_0\\)\n\n\\(t = 4.08\\) exceeds critical values \\([-2.07, 2.07]\\)\n\\(p &lt; \\alpha\\)\n“Statistically significant” difference"
  },
  {
    "objectID": "slides/11_the-t-test.html#conclusion",
    "href": "slides/11_the-t-test.html#conclusion",
    "title": "PSYC BC1101",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#ss-and-df",
    "href": "slides/16_ANOVA-pt-2.html#ss-and-df",
    "title": "PSYC BC1101",
    "section": "\\(SS\\) and \\(df\\)",
    "text": "\\(SS\\) and \\(df\\)\n\n\n\\(SS_{total} = \\Sigma X^2 - \\dfrac{G^2}{N}\\) \\(df_{total} = N-1\\)\n\\(SS_{within} = \\Sigma SS_{each \\ treamtent}\\) \\(df_{within} = N-k\\)\n\\(SS_{between} = \\Sigma \\dfrac{T^2}{n} - \\dfrac{G^2}{N}\\) \\(df_{between} = k-1\\)\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(k\\)\nNumber of treatment conditions\n\n\n\\(n_1, n_2...\\)\nNumber of scores in each treatment\n\n\n\\(N\\)\nTotal number of scores\n\n\n\\(T_1, T_2...\\)\nSum of scores \\((\\Sigma X)\\) for each treatment\n\n\n\\(G\\)\nGrand total of all scores in the study"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#summary-table",
    "href": "slides/16_ANOVA-pt-2.html#summary-table",
    "title": "PSYC BC1101",
    "section": "Summary table",
    "text": "Summary table\n\n\n\n\n\nSource\n\\(SS\\)\n\\(df\\)\n\\(MS\\)\n\\(F\\)\n\n\n\n\nBetween treatments\n\n\n\n\n\n\nWithin treatments\n\n\n\n\n\n\nTotal"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#step-1.-state-hypotheses",
    "href": "slides/16_ANOVA-pt-2.html#step-1.-state-hypotheses",
    "title": "PSYC BC1101",
    "section": "Step 1. State hypotheses",
    "text": "Step 1. State hypotheses\n\n\\(H_0: \\mu_1 = \\mu_2 = \\mu_3\\)\n\nNo treatment effect\nNumerator & denominator should be about the same\n\\(F\\) should be near \\(1.00\\)\n\n\\(H_1\\) : At least one population mean differs from another\n\nThere is some treatment effect\nNumerator bigger than denominator\n\\(F\\) should be noticeably larger than \\(1.00\\)"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#step-2.-critical-region",
    "href": "slides/16_ANOVA-pt-2.html#step-2.-critical-region",
    "title": "PSYC BC1101",
    "section": "Step 2. Critical region",
    "text": "Step 2. Critical region\n\nLike \\(t\\) distributions, there is a different \\(F\\) distribution for each value of \\(df\\)\n\nNow we have two different \\(df\\) values\n\\(df\\) numerator \\((df_{between})\\)\n\\(df\\) denominator \\((df_{within})\\)\nNote, distribution isn’t symmetrical\n\\(F\\) values are always positive"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#f-table",
    "href": "slides/16_ANOVA-pt-2.html#f-table",
    "title": "PSYC BC1101",
    "section": "\\(F\\) table",
    "text": "\\(F\\) table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha = .05\\)\n\n\n\\(df_{numerator}\\)\n\n\n\n\\(df_{denominator}\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n161.45\n199.50\n215.71\n224.58\n230.16\n233.99\n236.77\n238.88\n240.54\n241.88\n\n\n2\n18.51\n19.00\n19.16\n19.25\n19.30\n19.33\n19.35\n19.37\n19.39\n19.40\n\n\n3\n10.13\n9.55\n9.28\n9.12\n9.01\n8.94\n8.89\n8.85\n8.81\n8.79\n\n\n4\n7.71\n6.94\n6.59\n6.39\n6.26\n6.16\n6.09\n6.04\n6.00\n5.96\n\n\n5\n6.61\n5.79\n5.41\n5.19\n5.05\n4.95\n4.88\n4.82\n4.77\n4.74\n\n\n6\n5.99\n5.14\n4.76\n4.53\n4.39\n4.28\n4.21\n4.15\n4.10\n4.06\n\n\n7\n5.59\n4.74\n4.35\n4.12\n3.97\n3.87\n3.79\n3.73\n3.68\n3.64\n\n\n8\n5.32\n4.46\n4.07\n3.84\n3.69\n3.58\n3.50\n3.44\n3.39\n3.35\n\n\n9\n5.12\n4.26\n3.86\n3.63\n3.48\n3.37\n3.29\n3.23\n3.18\n3.14\n\n\n10\n4.96\n4.10\n3.71\n3.48\n3.33\n3.22\n3.13\n3.07\n3.02\n2.98\n\n\n\n\n\n\n\njStat = require(\"../js/jstat.js\")\n\naov_table = {\n\n  var df1, df2, critical\n  \n  function getDf2() {\n    try {\n      df2 = d3.select(this).select(\"td\")._groups[0][0].innerHTML;\n    } catch {} finally {update();}\n  }\n  \n    function getDf1() {\n    try {\n      df1 = d3.select(this)._groups[0][0].cellIndex\n      if (df1 &lt; 1) {df1 = 1}\n    } catch {} finally {update();}\n  }\n  \n  d3.select(\"#tbl\").selectAll(\"tr\").on(\"click\", getDf2)\n  d3.select(\"#tbl\").selectAll(\"td\").on(\"click\", getDf1)\n\n\n  const w = 1050\n  const h = 200\n  const margin = {right: 100, left: 100, bottom: 20}\n  \n  const svg = d3.select(\"#tbl\").append(\"svg\")\n    .attr(\"width\", w).attr(\"height\", h)\n    \n\n  const x = d3.scaleLinear()\n    .domain([0,4])\n    .range([margin.left, w - margin.right])\n  const y = d3.scaleLinear()\n    .domain([0,1])\n    .range([h - margin.bottom, 0])\n  const xAxis = d3.axisBottom(x)\n  const line = d3.line()\n    .x(d =&gt; x(d.value))\n    .y(d =&gt; y(d.density))\n  \n  function makeCurve(df1, df2) {\n    var arr = []\n    var values = jStat(0.01, 4, 210)[0]\n    for (var i = 0; i &lt; values.length; i++) {\n      arr.push({value: values[i],\n                density: jStat.centralF.pdf(values[i], Number(df1), Number(df2))})\n    }\n    return arr\n  }\n  \n  svg.append(\"g\").attr(\"transform\", `translate(0, ${h - margin.bottom})`)\n    .call(xAxis)\n    \n  const curve = svg.append(\"path\")\n    .attr(\"d\", line(makeCurve(10, 10)))\n    .style(\"fill\", \"none\")\n    .style(\"stroke\", \"black\")\n  \n  const df1Input = document.getElementById('df1-input')\n  df1Input.oninput = function() {\n    df1 = df1Input.value\n    update();\n  }\n  \n  const df2Input = document.getElementById('df2-input')\n  df2Input.oninput = function() {\n    df2 = df2Input.value\n    update();\n  }\n  \n  function update() {\n    d3.select(\"#df1-value\").text(df1)\n    d3.select(\"#df2-value\").text(df2)\n    critical = jStat.centralF.inv(0.95, Number(df1), Number(df2))\n    curve.attr(\"d\", line(makeCurve(Number(df1), Number(df2))))\n  }\n  \n\n\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(df_{numerator} = \\)1\n\\(df_{denominator} = \\)1"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#step-3.-calculate-test-statistic",
    "href": "slides/16_ANOVA-pt-2.html#step-3.-calculate-test-statistic",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculate test statistic",
    "text": "Step 3. Calculate test statistic\n\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\n\n\n\n9\n3\n5\n\n\n11\n5\n6\n\n\n13\n4\n7\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\\(M = 6\\)\n\n\n\n\n\n\n\\(N = 9\\)\n\\(n = 3\\)\n\\(k = 3\\)\n\\(G = 63\\)\n\\(\\Sigma X^2 = 531\\)"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#step-3.-ss-and-dfs",
    "href": "slides/16_ANOVA-pt-2.html#step-3.-ss-and-dfs",
    "title": "PSYC BC1101",
    "section": "Step 3. \\(SS\\) and \\(df\\)s",
    "text": "Step 3. \\(SS\\) and \\(df\\)s\n\\(SS_{total} = \\Sigma X^2 - \\dfrac{G^2}{N} = 531 - \\dfrac{63^2}{9} = 90\\)\n\\(SS_{within} = \\Sigma SS_{each \\ treamtent} = 8+2+2 = 12\\)\n\\(SS_{between} = \\Sigma \\dfrac{T^2}{n} - \\dfrac{G^2}{N} = \\dfrac{33^2}{3}+\\dfrac{12^2}{3}+\\dfrac{18^2}{3} - \\dfrac{63^2}{9}  = 78\\)\n\\(df_{total} = N-1 = 8\\)\n\\(df_{within} = N-k = 6\\)\n\\(df_{between} = k-1 = 2\\)"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#step-3.-ms-and-f",
    "href": "slides/16_ANOVA-pt-2.html#step-3.-ms-and-f",
    "title": "PSYC BC1101",
    "section": "Step 3. \\(MS\\) and \\(F\\)",
    "text": "Step 3. \\(MS\\) and \\(F\\)\n\\(MS_{between} = \\dfrac{SS_{between}}{df_{between}} = 39\\)\n\\(MS_{within} = \\dfrac{SS_{within}}{df_{within}} = 2\\)\n\\(F = \\dfrac{MS_{between}}{MS_{within}} = \\dfrac{39}{2} = 19.5\\)"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#step-4.-make-decision",
    "href": "slides/16_ANOVA-pt-2.html#step-4.-make-decision",
    "title": "PSYC BC1101",
    "section": "Step 4. Make decision",
    "text": "Step 4. Make decision\n\n\\(F &gt; F_{critical}\\)?\n\nReject or fail to reject \\(H_0\\)\n\nStep 4b. Effect size\n\nCompute percentage of variance accounted for by treatment\n\\(r^2\\) concept (proportion of variance explained)\nFor ANOVA called \\(\\eta^2\\) (“eta squared”)\n\n\n\n\\(\\eta^2 = \\dfrac{SS_{between}}{SS_{total}} = 0.87\\)"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#report-results",
    "href": "slides/16_ANOVA-pt-2.html#report-results",
    "title": "PSYC BC1101",
    "section": "Report results",
    "text": "Report results\n\nDescriptives\n\nTreatment means and standard deviations are presented in text, table and/or graph\n\nHypothesis test outcome\n\nResults of ANOVA are summarized, including\n\\(F\\) and \\(df\\) values, \\(p\\), \\(\\eta^2\\) (if significant)\n\n\n\nA single-factor, independent-samples ANOVA revealed a significant difference between people who consumed a banana (\\(M = 11\\); \\(SD = 2\\)), a candy bar (\\(M = 4\\); \\(SD = 1\\)), and the control condition \\((M = 6\\); \\(SD = 1)\\); \\(F(2,6) = 19.5\\), \\(p &lt; .05\\), \\(\\eta^2 = 0.87\\)."
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#tukeys-hsd",
    "href": "slides/16_ANOVA-pt-2.html#tukeys-hsd",
    "title": "PSYC BC1101",
    "section": "Tukey’s \\(HSD\\)",
    "text": "Tukey’s \\(HSD\\)\n\n\nTukey’s Honestly Significant Difference\n\nMinimum difference between pairs of treatment means so that \\(p &lt; \\alpha_{experimentwise}\\)\n\\(q\\) is the Studentized Range statistic\nDepends on \\(\\alpha\\), \\(k\\), and \\(df\\) for denominator\nFind \\(q\\) in table or R\n\n\n\n\n\n\\[\\begin{align}\nHSD &= q \\sqrt{\\dfrac{MS_{within}}{n}} \\\\\n    &= 4.34 \\sqrt{\\dfrac{2}{3}} \\\\\n    &= 3.54\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(df\\)\n\n\nNumber of Conditions\n\n\n\n\n2\n3\n4\n5\n6\n\n\n\n\n5\n3.64\n4.60\n5.22\n5.67\n6.03\n\n\n6\n3.46\n4.34\n4.90\n5.30\n5.63\n\n\n7\n3.34\n4.16\n4.68\n5.06\n5.36\n\n\n8\n3.26\n4.04\n4.53\n4.89\n5.17\n\n\n9\n3.20\n3.95\n4.41\n4.76\n5.02\n\n\n10\n3.15\n3.88\n4.33\n4.65\n4.91"
  },
  {
    "objectID": "slides/16_ANOVA-pt-2.html#tukeys-hsd-1",
    "href": "slides/16_ANOVA-pt-2.html#tukeys-hsd-1",
    "title": "PSYC BC1101",
    "section": "Tukey’s \\(HSD\\)",
    "text": "Tukey’s \\(HSD\\)\n\n\nTukey’s Honestly Significant Difference\n\nMinimum difference between pairs of treatment means so that \\(p &lt; \\alpha_{experimentwise}\\)\n\\(q\\) is the Studentized Range statistic\nDepends on \\(\\alpha\\), \\(k\\), and \\(df\\) for denominator\nFind \\(q\\) in table or R\n\n\n\n\n\n\\[\\begin{align}\nHSD &= q \\sqrt{\\dfrac{MS_{within}}{n}} \\\\\n    &= 4.34 \\sqrt{\\dfrac{2}{3}} \\\\\n    &= 3.54\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\n\n\n\n9\n3\n5\n\n\n11\n5\n6\n\n\n13\n4\n7\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\\(M = 6\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#single-sample-t-test-design",
    "href": "slides/13_independent-samples-t-test.html#single-sample-t-test-design",
    "title": "PSYC BC1101",
    "section": "Single-sample \\(t\\)-test design",
    "text": "Single-sample \\(t\\)-test design\n\nCompare sample against expected population mean based on logic/theory/scale design\nE.g. give everyone $10 💵\n\n\n\n\n\n\nWhat is your current level of happiness?\n  \n    \n    1. A lot less than usual\n    \n    \n    2. A little less than usual\n    \n    \n    3. About average\n    \n    \n    4. A little more than usual\n    \n    \n    5. A lot more than usual\n  \n\n\n\n\n\\(\\mu = 3\\)\n\n\n\nimport {likert} from \"../ojs/utils.qmd\";"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#single-sample-t-test-logic",
    "href": "slides/13_independent-samples-t-test.html#single-sample-t-test-logic",
    "title": "PSYC BC1101",
    "section": "Single-sample \\(t\\)-test logic",
    "text": "Single-sample \\(t\\)-test logic\n\n\n\n\n\n\n\n\n\nPartially known\noriginal population\n\\(\\mu\\)\n\n\nUnknown\ntreated\npopulation\n\n\nSample\n\n\nTreated sample \\(n, M, SD\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#independent-samples-design",
    "href": "slides/13_independent-samples-t-test.html#independent-samples-design",
    "title": "PSYC BC1101",
    "section": "Independent-samples design",
    "text": "Independent-samples design\nWhat if… give everyone $10\n\n\nGroup A:\nSpend this on yourself 💵\n\n\n\nWhat is your current level of happiness?\n  \n    \n    1. A lot less than usual\n    \n    \n    2. A little less than usual\n    \n    \n    3. About average\n    \n    \n    4. A little more than usual\n    \n    \n    5. A lot more than usual\n  \n\n\n\n\nGroup B:\nSpend this on someone else 💵\n\n\n\nWhat is your current level of happiness?\n  \n    \n    1. A lot less than usual\n    \n    \n    2. A little less than usual\n    \n    \n    3. About average\n    \n    \n    4. A little more than usual\n    \n    \n    5. A lot more than usual\n  \n\n\n\n\n\nDunn, E. W., Aknin, L. B., & Norton, M. I. (2014). Prosocial spending and happiness: Using money to benefit others pays off. Current Directions in Psychological Science, 23(1), 41-47. https://doi.org/10.1177/0963721413512503"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#independent-samples-t-test-logic",
    "href": "slides/13_independent-samples-t-test.html#independent-samples-t-test-logic",
    "title": "PSYC BC1101",
    "section": "Independent-samples \\(t\\)-test logic",
    "text": "Independent-samples \\(t\\)-test logic\n\n\n\n\n\n\n\n\n\nUnknown\ntreated population\nA\n\n\nUnknown\ntreated population\nB\n\n\nSample A\n\\(n, M, SD\\)\n\n\nSample B\n\\(n, M, SD\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#equations",
    "href": "slides/13_independent-samples-t-test.html#equations",
    "title": "PSYC BC1101",
    "section": "Equations",
    "text": "Equations\n\n\n\nDenominator: \\(s_{M_1-M_2}\\)\n\nEstimated standard error of the mean difference\n\n\n\n\\(s_{M_1-M_2} = \\sqrt{\\dfrac{s_p^2}{n_1}+\\dfrac{s_p^2}{n_2}}\\)\n\n\n\n\n\\(s_p^2\\): Pooled variance\n\nWeighted average of two sample variances\n\n\n\n\\(\\begin{align} s_p^2 = &\\dfrac{SS_1+SS_2}{df_1+df_2} \\\\ \\\\ \\text{or... } &\\dfrac{df_1*s_1^2 + df_2*s_2^2}{df_1+df_2} \\\\ \\text{because... } s^2 = &\\dfrac{SS}{df} \\text{ so... } SS = df*s^2 \\end{align}\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#calculating-independent-samples-t",
    "href": "slides/13_independent-samples-t-test.html#calculating-independent-samples-t",
    "title": "PSYC BC1101",
    "section": "Calculating independent samples \\(t\\)",
    "text": "Calculating independent samples \\(t\\)\n\nPooled variance: \\(s_p^2 = \\dfrac{SS_1+SS_2}{df_1+df_2}\\)\nEstimated standard error of mean difference: \\(s_{M_1-M_2} = \\sqrt{\\dfrac{s_p^2}{n_1}+\\dfrac{s_p^2}{n_2}}\\)\n\\(t\\) statistic: \\(t = \\dfrac{(M_1-M_2)-(\\mu_1-\\mu_2)}{s_{M_1-M_2}}\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#step-1-state-hypotheses",
    "href": "slides/13_independent-samples-t-test.html#step-1-state-hypotheses",
    "title": "PSYC BC1101",
    "section": "Step 1: State hypotheses",
    "text": "Step 1: State hypotheses\n\nNull: There is no difference between groups\n\nThe treatment has no effect\n\\(μ_1 – μ_2 = 0\\)\n\nAlternative: There is a difference\n\nThe treatment has an effect\nDirectional: \\(μ_1 – μ_2 &lt; 0\\) or \\(μ_1 – μ_2 &gt; 0\\)\nNondirectional: \\(μ_1 – μ_2 \\ne 0\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#step-2-define-critical-region",
    "href": "slides/13_independent-samples-t-test.html#step-2-define-critical-region",
    "title": "PSYC BC1101",
    "section": "Step 2: Define critical region",
    "text": "Step 2: Define critical region\n\n\n\nDepends on \\(\\alpha\\) and \\(df\\)\n\n\\[\n\\begin{align}\ndf &= df_1 + df_2 \\\\\n&= (n_1 – 1) + (n_2 – 1) \\\\\n&= N - 2\n\\end{align}\n\\] \n\n\n\n\n\n\n\n\n\n\n\nProportion\nin 1 tail\n\n\n\n0.025\n\n\n\nProportion\nin 2 tails\n0.05\n\n\n\n\n1\n12.706\n\n\n2\n4.303\n\n\n3\n3.182\n\n\n4\n2.776\n\n\n5\n2.571\n\n\n6\n2.447\n\n\n7\n2.365\n\n\n\\(df\\)       8\n2.306\n\n\n9\n2.262\n\n\n10\n2.228\n\n\n11\n2.201\n\n\n12\n2.179\n\n\n13\n2.160\n\n\n14\n2.145\n\n\n15\n2.131\n\n\n...\n..."
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#step-3-calculate-t-statistic",
    "href": "slides/13_independent-samples-t-test.html#step-3-calculate-t-statistic",
    "title": "PSYC BC1101",
    "section": "Step 3: Calculate \\(t\\) statistic",
    "text": "Step 3: Calculate \\(t\\) statistic\n\n\nSpend $10 on self\n\n\n\n\n\n\\(X\\)\n\\(X-M\\)\n\\((X-M)^2\\)\n\n\n\n\n1\n-2\n4\n\n\n5\n2\n4\n\n\n2\n-1\n1\n\n\n4\n1\n1\n\n\n3\n0\n0\n\n\n\\(M = 3.00\\)\n\n\\(SS = 10.00\\)\n\n\n\n\n\\(s^2 = 2.50\\)\n\n\n\n\n\\(s = 1.58\\)\n\n\n\n\n\n\nSpend $10 on other\n\n\n\n\n\n\\(X\\)\n\\(X-M\\)\n\\((X-M)^2\\)\n\n\n\n\n5\n1\n1\n\n\n5\n1\n1\n\n\n2\n-2\n4\n\n\n5\n1\n1\n\n\n3\n-1\n1\n\n\n\\(M = 4.00\\)\n\n\\(SS = 8.00\\)\n\n\n\n\n\\(s^2 = 2.00\\)\n\n\n\n\n\\(s = 1.41\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#step-3-calculate-t-statistic-1",
    "href": "slides/13_independent-samples-t-test.html#step-3-calculate-t-statistic-1",
    "title": "PSYC BC1101",
    "section": "Step 3 : Calculate \\(t\\) statistic",
    "text": "Step 3 : Calculate \\(t\\) statistic\n\n\\(s_p^2 = \\dfrac{SS_1+SS_2}{df_1+df_2} = \\dfrac{10 + 8}{4 + 4} = 2.25\\)\n\n\\(s_{M_1-M_2} = \\sqrt{\\dfrac{s_p^2}{n_1}+\\dfrac{s_p^2}{n_2}} = \\sqrt{\\dfrac{2.25}{5}+\\dfrac{2.25}{5}} = 0.95\\)\n\n\\(t = \\dfrac{(M_1-M_2)-(\\mu_1-\\mu_2)}{s_{M_1-M_2}} = \\dfrac{3 - 4}{0.95} = -1.05\\)"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#step-4-make-decision",
    "href": "slides/13_independent-samples-t-test.html#step-4-make-decision",
    "title": "PSYC BC1101",
    "section": "Step 4: Make decision",
    "text": "Step 4: Make decision"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#step-4b-effect-size",
    "href": "slides/13_independent-samples-t-test.html#step-4b-effect-size",
    "title": "PSYC BC1101",
    "section": "Step 4b: Effect size",
    "text": "Step 4b: Effect size\n\nCohen’s \\(d\\) for independent samples\n\n\\[\\begin{align}\nd &= \\dfrac{\\text{difference between means}}{\\text{pooled standard deviation}} \\\\\n  &= \\dfrac{(M_1 - M_2) - (\\mu_1 - \\mu_2)}{\\sqrt{s^2_p}} \\\\\n  &= \\dfrac{3 - 4}{\\sqrt{2.25}} \\\\\n  &= -0.67\n\\end{align}\\]\n\nNote: not required for nonsignificant differences"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#step-5-report-results",
    "href": "slides/13_independent-samples-t-test.html#step-5-report-results",
    "title": "PSYC BC1101",
    "section": "Step 5: Report results",
    "text": "Step 5: Report results\n\nA two-tailed independent-samples \\(t\\) test suggested that the difference in average happiness between people in the “Spend on self group” \\((M = 3\\); \\(SD = 1.58)\\) and the “Spend on other” group \\((M = 4\\); \\(SD = 1.41)\\) was nonsignificant; \\(t(8) =\\) \\(-1.05\\), \\(p &gt; .05\\)."
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#homogeneity-of-variance",
    "href": "slides/13_independent-samples-t-test.html#homogeneity-of-variance",
    "title": "PSYC BC1101",
    "section": "Homogeneity of variance",
    "text": "Homogeneity of variance\n\nTesting the homogeneity of variance assumption\n\nHartley’s F-max test\n\n\n\n\\(F_{max} = \\dfrac{s^2_{largest}}{s^2_{smallest}}\\)\n\n\nSmall value (near 1) indicates similar sample variances, larger values indicate larger difference\nLook up associated critical value for \\(F\\)-max test\nIf value exceeds critical value, indicates homogeneity assumption has been violated"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#homogeneity-of-variance-correction",
    "href": "slides/13_independent-samples-t-test.html#homogeneity-of-variance-correction",
    "title": "PSYC BC1101",
    "section": "Homogeneity of variance correction",
    "text": "Homogeneity of variance correction\n\nIf homogeneity of variance assumption is violated…\n\nCalculate standard error without pooled variance\nAdjust \\(df\\) using equation:\n\n\n\n\\[df = \\dfrac{(\\dfrac{s_1^2}{n_1}+\\dfrac{s_2^2}{n_2})}\n{\\dfrac{(\\dfrac{s_1^2}{n_1})^2}{n_1-1} + \\dfrac{(\\dfrac{s_2^2}{n_2})^2}{n_2-1}\n}\\]"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#homogeneity-of-variance-correction-1",
    "href": "slides/13_independent-samples-t-test.html#homogeneity-of-variance-correction-1",
    "title": "PSYC BC1101",
    "section": "Homogeneity of variance correction",
    "text": "Homogeneity of variance correction\n\n…or let R do the work for you\n\nt.test() function automatically applies correction\nSpecify var.equal = TRUE to override\n\n\n\n\n\nconditionA &lt;- c(1, 5, 2, 4, 3)\nconditionB &lt;- c(5, 5, 2, 5, 3)\n\nt.test(x = conditionA, y = conditionB)\n\n\n    Welch Two Sample t-test\n\ndata:  conditionA and conditionB\nt = -1.0541, df = 7.9024, p-value = 0.323\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.192378  1.192378\nsample estimates:\nmean of x mean of y \n        3         4 \n\n\n\n\nconditionA &lt;- c(1, 5, 2, 4, 3)\nconditionB &lt;- c(5, 5, 2, 5, 3)\n\nt.test(x = conditionA, y = conditionB, \n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  conditionA and conditionB\nt = -1.0541, df = 8, p-value = 0.3226\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.187668  1.187668\nsample estimates:\nmean of x mean of y \n        3         4"
  },
  {
    "objectID": "slides/13_independent-samples-t-test.html#confidence-interval-1",
    "href": "slides/13_independent-samples-t-test.html#confidence-interval-1",
    "title": "PSYC BC1101",
    "section": "Confidence interval",
    "text": "Confidence interval\n\n\n\n\\[\\begin{align}\n(\\mu_1 - \\mu_2) &= (M_1 - M_2) \\pm t * s_{M_1 - M_2} \\\\\n                &= -1 \\pm 2.31 * 0.95 \\\\\n                &= -3.19, 1.19\n\\end{align}\\]\n\nimport { addCIPlot } from \"../ojs/confidence-interval.qmd\"\n\nchart = {\n  d3.select(\"#ci-independent\")\n    .call(addCIPlot, {test_type: \"independent\",\n                      point_estimate: -1,\n                      standard_deviation: 1.5,\n                      n: 10,\n                      ci: 95,\n                      disable_controls: true})\n}"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#research-designs-1",
    "href": "slides/12_the-t-test-pt-2.html#research-designs-1",
    "title": "PSYC BC1101",
    "section": "Research designs",
    "text": "Research designs\n\nE.g. measure of happiness\n\n\n\n\n\n\nWhat is your current level of happiness?\n  \n    \n    1. A lot less than usual\n    \n    \n    2. A little less than usual\n    \n    \n    3. About average\n    \n    \n    4. A little more than usual\n    \n    \n    5. A lot more than usual\n  \n\n\n\n\n\\(\\mu = 3\\)"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#effect-size-r2",
    "href": "slides/12_the-t-test-pt-2.html#effect-size-r2",
    "title": "PSYC BC1101",
    "section": "Effect size: \\(r^2\\)",
    "text": "Effect size: \\(r^2\\)\n\nProportion of all variability in the data attributable to treatment effect\nSimplifying assumption: Treatment adds or subtracts a constant to each score\nE.g. 1 point on a scale of 1 to 5\n\\(r^2\\) separates that variability due to treatment from natural variability between scores\n\n\n\\(r^2 = \\dfrac{SS_{treatment}}{SS_{total}}\\)"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#r2",
    "href": "slides/12_the-t-test-pt-2.html#r2",
    "title": "PSYC BC1101",
    "section": "\\(r^2\\)",
    "text": "\\(r^2\\)\n\n\n\nCalculate sum of squared deviations from sample \\(M\\)\n\nVariability excluding treatment effect\n\\(SS_{without \\ treatment}\\)\n\nCalculate \\(SS\\) from \\(H_0\\) \\(\\mu\\)\n\nThis is total variability\n\\(SS_{total}\\)\n\nSubstract \\(SS_{without \\ treatment}\\) from \\(SS_{total}\\) to find \\(SS_{treatment}\\)\n\nVariability attributable to treatment effect\n\n\n\n\n\n\n\n\\[\\begin{align}\nr^2 = \\dfrac{SS_{treatment}}{SS_{total}} &= \\dfrac{SS_{total} - SS_{without \\ treatment}}{SS_{total}} \\\\\n&= \\dfrac{10-6}{10} = 0.4\n\\end{align}\\]"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#r2-1",
    "href": "slides/12_the-t-test-pt-2.html#r2-1",
    "title": "PSYC BC1101",
    "section": "\\(r^2\\)",
    "text": "\\(r^2\\)\n\nIf we already calculated \\(t\\)…\n\n\n\\(r^2 = \\dfrac{t^2}{t^2 + df}\\)\n\n\nWorks for any kind of \\(t\\)-test\n\nSingle / related / independent-samples\n\nInterpreting \\(r^2\\)\n\n\\(r^2 = 0.01\\): small effect\n\\(r^2 = 0.09\\): medium effect\n\\(r^2 = 0.25\\): large effect"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#calculating-ci-boundaries",
    "href": "slides/12_the-t-test-pt-2.html#calculating-ci-boundaries",
    "title": "PSYC BC1101",
    "section": "Calculating CI boundaries",
    "text": "Calculating CI boundaries\n\n\n\nSo far, we have been specifying \\(\\mu\\), calculating \\(M\\) and \\(s_M\\), solving for \\(t\\)\nFor CI, rearrange to solve for \\(\\mu\\)\n\nCalculate \\(M\\) and \\(s_M\\), specify \\(t\\) (based on desired width of CI —99%, 95%, 90%, 80% etc), solve for \\(\\mu\\)\n\n\n\n\n\\(t = \\dfrac{M - \\mu}{s_M}\\)\n\\(\\mu = M \\pm t * s_M\\)"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#confidence-interval-interpretation",
    "href": "slides/12_the-t-test-pt-2.html#confidence-interval-interpretation",
    "title": "PSYC BC1101",
    "section": "Confidence interval interpretation",
    "text": "Confidence interval interpretation\n\nWhat does a confidence interval tell us?\n\nIndicates precision of parameter estimate\nQuantifies variability around a single point estimate\nNOT “we are 95% sure the true population mean is within this range”\nNOT “sample means from this population will fall within this range 95% of the time”\n\n\n\n“The parameter is an unknown constant and no probability statement concerning its value may be made.”1\n\nJerzy Neyman, original developer of confidence intervals"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#factors-that-affect-ci-width",
    "href": "slides/12_the-t-test-pt-2.html#factors-that-affect-ci-width",
    "title": "PSYC BC1101",
    "section": "Factors that affect CI width",
    "text": "Factors that affect CI width\n\n\n\n\nPoint estimate: \n\n\n\n\n\nVariability: \n\n\n\n\nn = \n30\n\n\n\n\nCI: \n95"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#ci-nhst",
    "href": "slides/12_the-t-test-pt-2.html#ci-nhst",
    "title": "PSYC BC1101",
    "section": "CI & NHST",
    "text": "CI & NHST\n\n\\(p\\) value and CI always agree about statistical significance if CI is \\(1 – alpha\\)\n\nE.g. \\(\\alpha = .05\\) and 95% confidence interval\n\nIf the \\(p &lt; \\alpha\\), the confidence interval will not contain the null hypothesis value\nIf the confidence interval does not contain the null hypothesis value, the results are statistically significant\nBoth significance level and confidence level define a distance from a mean to a limit\n\nThe distances in both cases are exactly the same"
  },
  {
    "objectID": "slides/12_the-t-test-pt-2.html#ci-nhst-demonstration",
    "href": "slides/12_the-t-test-pt-2.html#ci-nhst-demonstration",
    "title": "PSYC BC1101",
    "section": "CI & NHST demonstration",
    "text": "CI & NHST demonstration\n\n\n\nn = \n15\n\n\n\n\nd = \n0.7\n\n\n\n\n\nShow: \\(H_0\\) \\(H_1\\) Both\n\n\n\n\n\n\np"
  },
  {
    "objectID": "slides/01_course-info.html#panopto",
    "href": "slides/01_course-info.html#panopto",
    "title": "PSYC BC1101",
    "section": "Panopto",
    "text": "Panopto\n\nIn-lecture quizzes\nNavigation\nSubtitles\nAlso: Class notes Google Doc"
  },
  {
    "objectID": "slides/01_course-info.html#topics",
    "href": "slides/01_course-info.html#topics",
    "title": "PSYC BC1101",
    "section": "Topics",
    "text": "Topics\n\nBasic issues:\n\nTerminology, variables & measurement\n\nDescriptive statistics:\n\nFrequency, central tendency, variability, z-scores\n\nInferential statistics:\n\nProbability, sampling; hypothesis testing\n\\(t\\)-tests; ANOVA; Correlation & regression\n\nLogical progression"
  },
  {
    "objectID": "slides/01_course-info.html#exams",
    "href": "slides/01_course-info.html#exams",
    "title": "PSYC BC1101",
    "section": "Exams",
    "text": "Exams\n\n3 multiple choice exams\n\nMultiple choice\n60 minutes\nOnly lecture material, not R\nNon-cumulative\nExcept inasmuch as later concepts rely on ones introduced earlier\n\nSome questions will involve sums, but only simple ones you can do on paper"
  },
  {
    "objectID": "slides/01_course-info.html#r-problem-sets",
    "href": "slides/01_course-info.html#r-problem-sets",
    "title": "PSYC BC1101",
    "section": "R problem sets",
    "text": "R problem sets\n\nPractical application of stats to data\n\nUsing RStudio Cloud\nShow code; how you worked out answers\nWork on .qmd in RStudio Cloud\nUpload rendered .pdf to Canvas"
  },
  {
    "objectID": "slides/01_course-info.html#grading",
    "href": "slides/01_course-info.html#grading",
    "title": "PSYC BC1101",
    "section": "Grading",
    "text": "Grading\n\nWrong answers ≠ lower grade\n\n0, 1, or 2 points\n0 = No submission, 1 = Incomplete, 2 = Valid attempt\nDeadline: End of recitation\n\n\n\n# comment your R code to show thought process\n\n# e.g. here I'm adding 2 and 2\n2 + 2 \n\n[1] 4"
  },
  {
    "objectID": "slides/01_course-info.html#math",
    "href": "slides/01_course-info.html#math",
    "title": "PSYC BC1101",
    "section": "Math",
    "text": "Math\n\nStatistics requires basic math skills\nE.g. order of operations\n\nParentheses\nExponents (like squaring/square root)\nMultiplication & division\nSummation\nAddition & subtraction"
  },
  {
    "objectID": "slides/01_course-info.html#math-1",
    "href": "slides/01_course-info.html#math-1",
    "title": "PSYC BC1101",
    "section": "Math",
    "text": "Math\n\nSummation\n\nSymbol \\(\\Sigma\\) (Greek letter Sigma) means add up\nSummation is done after operations in parentheses, squaring, and multiplication or division, but before other addition or subtraction\nE.g… \\(X = [2, 4, 7]\\)\n\n\n\n\\(\\Sigma X = ?\\)\n\\(\\Sigma X + 1 = ?\\)\n\\(\\Sigma(X + 1) = ?\\)"
  },
  {
    "objectID": "slides/01_course-info.html#math-2",
    "href": "slides/01_course-info.html#math-2",
    "title": "PSYC BC1101",
    "section": "Math",
    "text": "Math\n\nAlgebra\n\nRearranging equations\nE.g.\n\n\n\\[\\begin{align}12 &= 7 + X \\\\ X &= ? \\end{align}\\]"
  },
  {
    "objectID": "slides/01_course-info.html#r",
    "href": "slides/01_course-info.html#r",
    "title": "PSYC BC1101",
    "section": "",
    "text": "Disadvantages\n\nA little tricky to begin with\n\nAdvantages\n\nFree\nCan do stuff other stats software can’t\nReproducible analyses\nPretty graphs\nFeel like a super cool hacker\nPirate jokes\nGood for your career"
  },
  {
    "objectID": "slides/01_course-info.html#lastly",
    "href": "slides/01_course-info.html#lastly",
    "title": "PSYC BC1101",
    "section": "Lastly",
    "text": "Lastly\n\nCheck Canvas & email regularly\nLet me know about problems\n\n\nw = 1050\nh = w/2\n\ncover = {\n\n  const svg = d3.select(\"#cover-image\")\n    .append(\"svg\")\n    .attr(\"width\", w)\n    .attr(\"height\", h)\n    .style(\"transform\", \"scaleY(-1)\")\n    \n  const g = svg.append(\"g\")\n  \n  g\n    .selectAll(\"rect\")\n    .data(data)\n    .enter()\n      .append(\"rect\")\n      .attr(\"fill\", \"black\")\n        .attr(\"x\", function(d, i){return 5 + i*(w/10)})\n        .attr(\"y\", 0)\n        .attr(\"height\", 0)\n        .attr(\"width\", w/10 - 10)\n        .attr(\"fill\", d =&gt; d.color)\n          .transition()\n          .duration(d =&gt; d.duration)\n          .delay(d =&gt; d.delay)\n          .attr(\"height\", d =&gt; d.value * 26)\n  \n\n \n return svg.node()\n}"
  },
  {
    "objectID": "slides/07_probability.html#probability-and-games",
    "href": "slides/07_probability.html#probability-and-games",
    "title": "PSYC BC1101",
    "section": "Probability and games",
    "text": "Probability and games\n\n\n\nSettlers of Catan\n\nBoard of hex tiles, each with a number\nPlace “settlements” at intersection of tiles\nEach turn, roll 2 dice\nYou get resources if your settlement is touching the rolled total\nWhere do you put your first settlement?"
  },
  {
    "objectID": "slides/07_probability.html#definition-notation",
    "href": "slides/07_probability.html#definition-notation",
    "title": "PSYC BC1101",
    "section": "Definition & notation",
    "text": "Definition & notation\n\nSeveral different outcomes are possible\n\nThe probability of any specific outcome is a fraction of all possible outcomes\n\\(p\\) is the symbol for “probability”\nProbability of some specific outcome is specified by \\(p(event)\\)\n\n\n\\(p(A) = \\dfrac{number \\ of \\ outcomes \\ classified \\ as \\ A}{total \\ number \\ of \\ possible \\ outcomes}\\)"
  },
  {
    "objectID": "slides/07_probability.html#example-coin-flip",
    "href": "slides/07_probability.html#example-coin-flip",
    "title": "PSYC BC1101",
    "section": "Example: coin flip",
    "text": "Example: coin flip\n\nE.g. Flipping a coin\n\nNumerator: number of those outcomes\nDenominator: all possible outcomes\n\n\n\n\n\n\\(p(heads) = 1/2 = .5\\)\n\n\n\\(p(tails) = 1/2 = .5%\\)"
  },
  {
    "objectID": "slides/07_probability.html#example-rolling-dice",
    "href": "slides/07_probability.html#example-rolling-dice",
    "title": "PSYC BC1101",
    "section": "Example: rolling dice",
    "text": "Example: rolling dice\n\nAll possible outcomes:\n1, 2, 3, 4, 5, 6\n\n\n\n\\(p(6) = 1/6 = 0.17\\)\n\\(p(1) = 1/6 = 0.17\\)\n\\(p(odd) = 3/6 = 0.5\\)"
  },
  {
    "objectID": "slides/07_probability.html#example-rolling-2-dice",
    "href": "slides/07_probability.html#example-rolling-2-dice",
    "title": "PSYC BC1101",
    "section": "Example: rolling 2 dice",
    "text": "Example: rolling 2 dice\n\n\n\n\n\\(p(2) = 1/36 = .03\\)\n\\(p(12) = 1/36 = .03\\)\n\\(p(7) = 6/36 = .17\\)\n\n\n\n\n\n\nRoll\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n\n\n2\n3\n4\n5\n6\n7\n8\n\n\n3\n4\n5\n6\n7\n8\n9\n\n\n4\n5\n6\n7\n8\n9\n10\n\n\n5\n6\n7\n8\n9\n10\n11\n\n\n6\n7\n8\n9\n10\n11\n12"
  },
  {
    "objectID": "slides/07_probability.html#sampling-marbles",
    "href": "slides/07_probability.html#sampling-marbles",
    "title": "PSYC BC1101",
    "section": "Sampling marbles",
    "text": "Sampling marbles\n\n\n\n\n\nJar of marbles\n\nContains 25 white & 25 blue marbles\nWhat is the probability of randomly drawing a white marble?\nNumber of those outcomes (25)\nDivided by total number of outcomes (50)\n\n\n\\(p(white) = 25/50 = .5\\)"
  },
  {
    "objectID": "slides/07_probability.html#more-marbles",
    "href": "slides/07_probability.html#more-marbles",
    "title": "PSYC BC1101",
    "section": "More marbles",
    "text": "More marbles\n\n\n\n\n\nDifferent jar\n\n40 blue & 10 white marbles\nWhat is the probability of randomly drawing a white marble?\n\n\n\\(p(white) = 10/50 = .2\\)"
  },
  {
    "objectID": "slides/07_probability.html#repeated-sampling",
    "href": "slides/07_probability.html#repeated-sampling",
    "title": "PSYC BC1101",
    "section": "Repeated sampling",
    "text": "Repeated sampling\n\n\n\n\n\nRepeated sampling\n\n40 blue, 10 white\nWhat is the probability of randomly drawing one white marble and then drawing a second white marble?\n\n\n\\(p(first \\ white) = 10/50 = .2\\)\n\\(p(second \\ white)\\) depends on whether we put the first one back or not"
  },
  {
    "objectID": "slides/07_probability.html#repeated-sampling-1",
    "href": "slides/07_probability.html#repeated-sampling-1",
    "title": "PSYC BC1101",
    "section": "Repeated sampling",
    "text": "Repeated sampling\n\n\n\n\n\nWithout replacement\n\n\\[\\begin{align} p(white) & = 10/50 = .2 \\\\\np(second \\ white) & = 9/49 \\approx .18 \\\\\np(both \\ white) & = .2 * .18  \\approx .037 \\end{align}\\]\n\nWith replacement\n\n\\(\\begin{align} p(white) &= 10/50 = .2 \\\\\np(second \\ white) &= 10/50 = .2 \\\\\np(both \\ white) &= .2 * .2 = .04\\end{align}\\)"
  },
  {
    "objectID": "slides/07_probability.html#random-sampling",
    "href": "slides/07_probability.html#random-sampling",
    "title": "PSYC BC1101",
    "section": "Random sampling",
    "text": "Random sampling\n\n“Random sample” definition\n\nA sample produced by a process that assures:\n\nEach individual in the population has an equal chance of being selected\nProbability of being selected stays constant from one selection to the next when more than one individual is selected\n\n“Independent random sampling”\n\nRequires sampling with replacement"
  },
  {
    "objectID": "slides/07_probability.html#unit-normal-table",
    "href": "slides/07_probability.html#unit-normal-table",
    "title": "PSYC BC1101",
    "section": "Unit Normal Table",
    "text": "Unit Normal Table\n\n\n\n\n\n\n\n\\(z\\)\nProportion in body\nProportion in tail\nProportion between \\(M\\) and \\(z\\)\n\n\n\n\n0.0\n0.5000\n0.5000\n0.0000\n\n\n0.1\n0.5398\n0.4602\n0.0398\n\n\n0.2\n0.5793\n0.4207\n0.0793\n\n\n0.3\n0.6179\n0.3821\n0.1179\n\n\n0.4\n0.6554\n0.3446\n0.1554\n\n\n0.5\n0.6915\n0.3085\n0.1915\n\n\n0.6\n0.7257\n0.2743\n0.2257\n\n\n0.7\n0.7580\n0.2420\n0.2580\n\n\n0.8\n0.7881\n0.2119\n0.2881\n\n\n0.9\n0.8159\n0.1841\n0.3159\n\n\n1.0\n0.8413\n0.1587\n0.3413\n\n\n1.1\n0.8643\n0.1357\n0.3643\n\n\n1.2\n0.8849\n0.1151\n0.3849\n\n\n1.3\n0.9032\n0.0968\n0.4032\n\n\n1.4\n0.9192\n0.0808\n0.4192\n\n\n1.5\n0.9332\n0.0668\n0.4332\n\n\n1.6\n0.9452\n0.0548\n0.4452\n\n\n1.7\n0.9554\n0.0446\n0.4554\n\n\n1.8\n0.9641\n0.0359\n0.4641\n\n\n1.9\n0.9713\n0.0287\n0.4713\n\n\n2.0\n0.9772\n0.0228\n0.4772"
  },
  {
    "objectID": "slides/07_probability.html#unit-normal-table-1",
    "href": "slides/07_probability.html#unit-normal-table-1",
    "title": "PSYC BC1101",
    "section": "Unit Normal Table",
    "text": "Unit Normal Table\n\n\n\n\n\n\n\n\\(z\\)\nProportion in body\nProportion in tail\nProportion between \\(M\\) and \\(z\\)\n\n\n\n\n0.0\n0.5000\n0.5000\n0.0000\n\n\n0.1\n0.5398\n0.4602\n0.0398\n\n\n0.2\n0.5793\n0.4207\n0.0793\n\n\n0.3\n0.6179\n0.3821\n0.1179\n\n\n0.4\n0.6554\n0.3446\n0.1554\n\n\n0.5\n0.6915\n0.3085\n0.1915\n\n\n0.6\n0.7257\n0.2743\n0.2257\n\n\n0.7\n0.7580\n0.2420\n0.2580\n\n\n0.8\n0.7881\n0.2119\n0.2881\n\n\n0.9\n0.8159\n0.1841\n0.3159\n\n\n1.0\n0.8413\n0.1587\n0.3413\n\n\n1.1\n0.8643\n0.1357\n0.3643\n\n\n1.2\n0.8849\n0.1151\n0.3849\n\n\n1.3\n0.9032\n0.0968\n0.4032\n\n\n1.4\n0.9192\n0.0808\n0.4192\n\n\n1.5\n0.9332\n0.0668\n0.4332\n\n\n1.6\n0.9452\n0.0548\n0.4452\n\n\n1.7\n0.9554\n0.0446\n0.4554\n\n\n1.8\n0.9641\n0.0359\n0.4641\n\n\n1.9\n0.9713\n0.0287\n0.4713\n\n\n2.0\n0.9772\n0.0228\n0.4772"
  },
  {
    "objectID": "slides/07_probability.html#using-r",
    "href": "slides/07_probability.html#using-r",
    "title": "PSYC BC1101",
    "section": "Using R",
    "text": "Using R\n\npnorm(0.2)  # area to the left of z = 0.2\n\n[1] 0.5792597\n\npnorm(0.2, lower.tail=FALSE) # area to the right of z = 0.2\n\n[1] 0.4207403\n\n# can specify different mean & SD\npnorm(700, mean=500, sd=100, lower.tail=FALSE) \n\n[1] 0.02275013\n\n# can specify proportion & find corresponding score\nqnorm(.0228, mean=500, sd=100, lower.tail=FALSE) \n\n[1] 699.9077"
  },
  {
    "objectID": "slides/07_probability.html#spiderman",
    "href": "slides/07_probability.html#spiderman",
    "title": "PSYC BC1101",
    "section": "Spiderman",
    "text": "Spiderman\n\nAre Peter Parker’s RTs “noticeably different?”\n\n\\(z = -2.5\\)\nCan state precise probability of observing a \\(z\\)-score that (or more) extreme\n\n\n\n\n\n\n\n\npnorm(-2.5)\n\n[1] 0.006209665\n\npnorm(159, mean = 284, sd = 50)\n\n[1] 0.006209665"
  },
  {
    "objectID": "slides/07_probability.html#warning",
    "href": "slides/07_probability.html#warning",
    "title": "PSYC BC1101",
    "section": "Warning",
    "text": "Warning\n\nProbabilities given in the Unit Normal Table will be accurate only for normally distributed scores\n\nShape of the distribution must be verified\nImportant assumption of Central Limit Theorem"
  },
  {
    "objectID": "slides/20_regression.html#correlation",
    "href": "slides/20_regression.html#correlation",
    "title": "PSYC BC1101",
    "section": "Correlation",
    "text": "Correlation\n\nPearson’s \\(r\\), bivariate correlation coefficient\nQuantifies the strength of linear relationship between two variables"
  },
  {
    "objectID": "slides/20_regression.html#correlation-example",
    "href": "slides/20_regression.html#correlation-example",
    "title": "PSYC BC1101",
    "section": "Correlation example",
    "text": "Correlation example\n\n\n\n\n\n\n\nParticipant\nSleep duration\nTest score\n\n\n\n\nA\n4\n5\n\n\nB\n5\n8\n\n\nC\n7\n8\n\n\nD\n8\n10\n\n\nE\n11\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\\(r = \\dfrac{SP}{\\sqrt{SS_X SS_Y}} = \\dfrac{15}{\\sqrt{30*14}} = 0.73\\)"
  },
  {
    "objectID": "slides/20_regression.html#regression-1",
    "href": "slides/20_regression.html#regression-1",
    "title": "PSYC BC1101",
    "section": "Regression",
    "text": "Regression\n\n\n\nRegression defines the line of best fit\n\nMakes relationship easier to see\nShows “central tendency” of the relationship\nEmphasizes prediction"
  },
  {
    "objectID": "slides/20_regression.html#regression-2",
    "href": "slides/20_regression.html#regression-2",
    "title": "PSYC BC1101",
    "section": "Regression",
    "text": "Regression\n\n\n\nRegression defines the line of best fit\n\nMakes relationship easier to see\nShows “central tendency” of the relationship\nEmphasizes prediction\nLine of best fit is the line that minimizes prediction error"
  },
  {
    "objectID": "slides/20_regression.html#straight-line-equation",
    "href": "slides/20_regression.html#straight-line-equation",
    "title": "PSYC BC1101",
    "section": "Straight line equation",
    "text": "Straight line equation\n\n\\(Y = bX + a\\)\n\n\\(X\\) and \\(Y\\) are variables\n\\(a\\) (the intercept) and \\(b\\) (the slope) are constants\n\n\n\n\n\n\n\n\n\nCelcius\nFahrenheit\n\n\n\n\n0\n32\n\n\n10\n50\n\n\n20\n68\n\n\n30\n86\n\n\n40\n104\n\n\n50\n122\n\n\n\n\n\n\n\\(Y = 1.8 X + 32\\)"
  },
  {
    "objectID": "slides/20_regression.html#regression-3",
    "href": "slides/20_regression.html#regression-3",
    "title": "PSYC BC1101",
    "section": "Regression",
    "text": "Regression\n\n\n\nRegression line equation\n\n\\(\\hat{Y} = bX + a\\)\n\\(\\hat{Y}\\): value of \\(Y\\) predicted by the regression equation for each value of \\(X\\)\n\\((Y - \\hat{Y})\\): residual (deviation of each data point from the regression line)\nRegression defines line that minimizes the sum of squared residuals\n\\(SS_{residual} = \\Sigma(Y - \\hat{Y})^2\\)\n“Least-squared-error solution”"
  },
  {
    "objectID": "slides/20_regression.html#regression-solving-b",
    "href": "slides/20_regression.html#regression-solving-b",
    "title": "PSYC BC1101",
    "section": "Regression: solving \\(b\\)",
    "text": "Regression: solving \\(b\\)\n\n\n\nRegression line equation: \\(\\hat{Y} = bX + a\\)\n\nThe slope of the line, \\(b\\):\n\n\n\n\\[\\begin{align}\nb &= \\dfrac{SP}{SS_X} \\\\\n  &= \\dfrac{15}{30} \\\\\n  &= 0.5\n\\end{align}\\]"
  },
  {
    "objectID": "slides/20_regression.html#regression-solving-a",
    "href": "slides/20_regression.html#regression-solving-a",
    "title": "PSYC BC1101",
    "section": "Regression: solving \\(a\\)",
    "text": "Regression: solving \\(a\\)\n\n\n\nThe intercept of the line, \\(a\\)\n\nThe value of \\(Y\\) when \\(X = 0\\)\nThe line goes through \\((M_X, M_Y)\\) therefore:\n\n\n\n\\[\\begin{align}\na &= M_Y - b * M_X \\\\\n  &= 8 - 0.5 * 7 \\\\\n  &= 4.5\n\\end{align}\\]"
  },
  {
    "objectID": "slides/20_regression.html#ss_residual",
    "href": "slides/20_regression.html#ss_residual",
    "title": "PSYC BC1101",
    "section": "\\(SS_{residual}\\)",
    "text": "\\(SS_{residual}\\)\n\n\n\n\n\n\n\nSleep\nTest score\n\\(\\hat{Y}\\)\n\\(Y - \\hat{Y}\\)\n\\((Y - \\hat{Y})^2\\)\n\n\n\n\n4\n5\n6.5\n-1.5\n2.25\n\n\n5\n8\n7.0\n1.0\n1.00\n\n\n7\n8\n8.0\n0.0\n0.00\n\n\n8\n10\n8.5\n1.5\n2.25\n\n\n11\n9\n10.0\n-1.0\n1.00\n\n\n\n\n\n\n\\[\\begin{align}\nSS_{residual} &= \\Sigma(Y - \\hat{Y})^2 \\\\\n&= \\Sigma(2.25, 1, 0, 2.25, 1) \\\\\n&= 6.5\n\\end{align}\\]"
  },
  {
    "objectID": "slides/20_regression.html#standard-error-of-the-estimate",
    "href": "slides/20_regression.html#standard-error-of-the-estimate",
    "title": "PSYC BC1101",
    "section": "Standard error of the estimate",
    "text": "Standard error of the estimate\n\n\n\n\\(s_{error}\\)\n\nQuantifies precision of regression estimate\nAverage distance of points from the regression line\nRemember… \\(s = \\sqrt{\\dfrac{SS}{df}}\\)\n\n\n\n\n\n\n\\(s_{error} = \\sqrt{\\dfrac{SS_{residual}}{df}}=\\sqrt{\\dfrac{6.5}{5-2}} = 1.47\\)"
  },
  {
    "objectID": "slides/20_regression.html#analysis-of-regression",
    "href": "slides/20_regression.html#analysis-of-regression",
    "title": "PSYC BC1101",
    "section": "Analysis of regression",
    "text": "Analysis of regression\n\nPartitioning variance (like ANOVA)\n\n\n\n\n\\(SS_{Y}\\)\n\n\n\\(SS_{regression}\\)\n\n\n\\(SS_{residual}\\)\n\n\n\n\\(df_{Y}\\)\n\n\n\\(df_{regression}\\)\n\n\n\\(df_{residual}\\)\n\n\n\n\n\\(SS_Y = \\Sigma(Y - M_Y)^2\\)\n\\(SS_{residual} = \\Sigma(Y - \\hat{Y})^2\\)\n\\(SS_{regression} = SS_Y - SS_{residual}\\)\n\n\\(df_Y = n - 1\\)\n\\(df_{residual} = n - 2\\)\n\\(df_{regression} = 1\\)"
  },
  {
    "objectID": "slides/20_regression.html#analysis-of-regression-1",
    "href": "slides/20_regression.html#analysis-of-regression-1",
    "title": "PSYC BC1101",
    "section": "Analysis of regression",
    "text": "Analysis of regression\n\nPartitioning variance (like ANOVA)\n\n\n\\(MS_{regression}=\\dfrac{SS_{regression}}{df_{regression}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ MS_{residual}=\\dfrac{SS_{residual}}{df_{residual}}\\)\n\\(F = \\dfrac{MS_{regression}}{MS_{residual}}\\)"
  },
  {
    "objectID": "slides/20_regression.html#step-1-hypotheses",
    "href": "slides/20_regression.html#step-1-hypotheses",
    "title": "PSYC BC1101",
    "section": "Step 1: Hypotheses",
    "text": "Step 1: Hypotheses\n\n\\(H_0\\): the slope of the regression line \\(\\beta = 0\\)\n\ni.e., there is no association between variables\nKnowing \\(X\\) does not help to predict \\(Y\\)\n\n\\(H_1\\): \\(\\beta \\ne 0\\)"
  },
  {
    "objectID": "slides/20_regression.html#step-2.-critical-region",
    "href": "slides/20_regression.html#step-2.-critical-region",
    "title": "PSYC BC1101",
    "section": "Step 2. Critical region",
    "text": "Step 2. Critical region\n\nNumerator: \\(df_{regression} = 1\\)\nDenominator: \\(df_{residual} = n-2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha = .05\\)\n\n\n\\(df_{numerator}\\)\n\n\n\n\\(df_{denominator}\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1\n161.45\n199.50\n215.71\n224.58\n230.16\n233.99\n236.77\n238.88\n240.54\n241.88\n\n\n2\n18.51\n19.00\n19.16\n19.25\n19.30\n19.33\n19.35\n19.37\n19.39\n19.40\n\n\n3\n10.13\n9.55\n9.28\n9.12\n9.01\n8.94\n8.89\n8.85\n8.81\n8.79\n\n\n4\n7.71\n6.94\n6.59\n6.39\n6.26\n6.16\n6.09\n6.04\n6.00\n5.96\n\n\n5\n6.61\n5.79\n5.41\n5.19\n5.05\n4.95\n4.88\n4.82\n4.77\n4.74\n\n\n6\n5.99\n5.14\n4.76\n4.53\n4.39\n4.28\n4.21\n4.15\n4.10\n4.06\n\n\n7\n5.59\n4.74\n4.35\n4.12\n3.97\n3.87\n3.79\n3.73\n3.68\n3.64\n\n\n8\n5.32\n4.46\n4.07\n3.84\n3.69\n3.58\n3.50\n3.44\n3.39\n3.35\n\n\n9\n5.12\n4.26\n3.86\n3.63\n3.48\n3.37\n3.29\n3.23\n3.18\n3.14\n\n\n10\n4.96\n4.10\n3.71\n3.48\n3.33\n3.22\n3.13\n3.07\n3.02\n2.98"
  },
  {
    "objectID": "slides/20_regression.html#step-3.-calculate",
    "href": "slides/20_regression.html#step-3.-calculate",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculate",
    "text": "Step 3. Calculate\n\n\n\n\n\n\n\n\n\n\nParticipant\nAmount of sleep\n(\\(X\\))\nTest score\n(\\(Y\\))\n\n\n\n\nA\n4\n5\n\n\nB\n5\n8\n\n\nC\n7\n8\n\n\nD\n8\n10\n\n\nE\n11\n9\n\n\n\n\n\n\n\n\\(SS_{Y} = 14\\)\n\\(SS_{residual} = 6.5\\)\n\\(SS_{regression} = 7.5\\)\n\n\\(df_Y = 4\\)\n\\(df_{residual} = 3\\)\n\\(df_{regression} = 1\\)"
  },
  {
    "objectID": "slides/20_regression.html#step-3.-calculate-1",
    "href": "slides/20_regression.html#step-3.-calculate-1",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculate",
    "text": "Step 3. Calculate\n\n\n\n\n\n\n\n\n\n\nParticipant\nAmount of sleep\n(\\(X\\))\nTest score\n(\\(Y\\))\n\n\n\n\nA\n4\n5\n\n\nB\n5\n8\n\n\nC\n7\n8\n\n\nD\n8\n10\n\n\nE\n11\n9\n\n\n\n\n\n\n\\(MS_{regression} = \\dfrac{SS_{regression}}{df_{regression}} = \\dfrac{7.5}{1} = 7.5\\)\n\\(MS_{residual} = \\dfrac{SS_{residual}}{df_{residual}} = \\dfrac{6.5}{3} = 2.17\\)"
  },
  {
    "objectID": "slides/20_regression.html#step-3.-calculate-2",
    "href": "slides/20_regression.html#step-3.-calculate-2",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculate",
    "text": "Step 3. Calculate\n\n\n\n\n\n\n\n\n\n\nParticipant\nAmount of sleep\n(\\(X\\))\nTest score\n(\\(Y\\))\n\n\n\n\nA\n4\n5\n\n\nB\n5\n8\n\n\nC\n7\n8\n\n\nD\n8\n10\n\n\nE\n11\n9\n\n\n\n\n\n\n\\(F = \\dfrac{MS_{regression}}{MS_{residual}} = \\dfrac{7.5}{2.17} = 3.46\\)"
  },
  {
    "objectID": "slides/20_regression.html#step-4.-make-decision",
    "href": "slides/20_regression.html#step-4.-make-decision",
    "title": "PSYC BC1101",
    "section": "Step 4. Make decision",
    "text": "Step 4. Make decision\n\n\\(F &gt; F_{critical}\\)?\n\nReject or fail to reject \\(H_0\\), no relationship in population\n\nStep 4b: Effect size\n\n\\(r^2\\): Coefficient of determination\nProportion of variance explained by the regression\n\n\n\n\\(r^2 = \\dfrac{SS_{regression}}{SS_Y} = \\dfrac{7.5}{14} = 0.54\\)"
  },
  {
    "objectID": "slides/20_regression.html#step-5.-report",
    "href": "slides/20_regression.html#step-5.-report",
    "title": "PSYC BC1101",
    "section": "Step 5. Report",
    "text": "Step 5. Report\n\nLonger sleep duration was associated with an increase in test performance, \\(b = 0.5\\). However, the association was nonsignificant; \\(F(1, 3) = 3.46\\), \\(p &gt; .05\\)."
  },
  {
    "objectID": "slides/20_regression.html#ojs-minimizing-error",
    "href": "slides/20_regression.html#ojs-minimizing-error",
    "title": "PSYC BC1101",
    "section": "ojs minimizing error",
    "text": "ojs minimizing error\nhi hi\n\n\n\n\nchangable_chart = {\n\n  const w = 900;\n  const h = 500;\n  \n  const margin = {top: 20, right: 20, left: 110, bottom: 75}\n\n  const x_values = [4,5,7,8,11]\n  const y_values = [5,8,8,10,9]\n  const yhat_values = [0,0,0,0,0]\n  \n  const data = [{xVal: 4, yVal: 5},\n                {xVal: 5, yVal: 8},\n                {xVal: 7, yVal: 8},\n                {xVal: 8, yVal: 10},\n                {xVal: 11, yVal: 9}]\n                \n  const aInput = d3.select(\"#a-control\")\n  const bInput = d3.select(\"#b-control\")\n  \n  aInput.on(\"input\", drawFitLine);\n  bInput.on(\"input\", drawFitLine);\n  \n  const x = d3.scaleLinear()\n    .range([margin.left, w - margin.right - 300])\n    .domain([0, 12])\n  const y = d3.scaleLinear()\n    .range([h - margin.bottom, margin.top])\n    .domain([0, 10])\n  \n  const xAxis = d3.axisBottom(x);\n  const yAxis = d3.axisLeft(y);\n  \n  \n  const svg = d3.select(\"#error-plot-container\").append(\"svg\")\n    .attr(\"width\", w)\n    .attr(\"height\", h)\n    \n  const axes = svg.append(\"g\")\n  const axisFontSize = \"0.7em\"\n  axes.append(\"g\").attr(\"transform\", `translate(0, ${h - margin.bottom})`).call(xAxis).style(\"font-size\", axisFontSize).attr(\"font-family\", \"Times New Roman\");\n  axes.append(\"g\").attr(\"transform\", `translate(${margin.left}, 0)`).call(yAxis).style(\"font-size\", axisFontSize).attr(\"font-family\", \"Times New Roman\");\n  \n  const sumSquaresPlot = svg.append(\"g\")\n  sumSquaresPlot.attr(\"transform\", `translate(500, 0)`)\n  \n  const axisTitles = svg.append(\"g\")\n    .style(\"font-size\", \"0.8em\")\n    .style(\"font-family\", \"Times New Roman\")\n    .style(\"fill\", \"var(--text-color)\")\n  \n  axisTitles.append(\"text\")\n    .attr(\"transform\", `translate(${margin.left + (w - margin.left - margin.right) / 2}, ${h - 10})`).text(\"Sleep duration\")\n    .attr(\"text-anchor\", \"middle\");\n  \n  axisTitles.append(\"text\")\n    .attr(\"transform\", `translate(0, ${margin.top + (h - margin.top - margin.bottom) / 2})`)\n    .text(\"Score\")\n    \n  \n  const residuals = svg.append(\"g\")\n  residuals.selectAll(\"line\").data(data).enter()\n      .append(\"line\")\n        .attr(\"x1\", d =&gt; x(d.xVal))\n        .attr(\"x2\", d =&gt; x(d.xVal))\n        .attr(\"y1\", d =&gt; y(d.yVal))\n        &lt;!-- .attr(\"y2\", d =&gt; y(d.yVal)) --&gt;\n        .style(\"stroke\", \"red\")\n        .style(\"stroke-width\", 2)\n        .attr(\"stroke-dasharray\", [5, 3])\n        \n  const fitLine = svg.append(\"line\")\n    .style(\"stroke\", \"dodgerblue\")\n    .style(\"stroke-width\", 3)\n    \n  const dots = svg.append(\"g\")\n  const squares = svg.append(\"g\")\n  \n  const ssResidualSquare = sumSquaresPlot.append(\"g\").append(\"polygon\")\n    .style(\"fill\", \"plum\")\n    .style(\"opacity\", 0.8)\n    \n  dots.selectAll(\"circle\").data(data).enter().append(\"circle\")\n    .attr(\"cx\", d =&gt; x(d.xVal))\n    .attr(\"cy\", d =&gt; y(d.yVal))\n    .attr(\"r\", 5)\n    .style(\"fill\", \"var(--text-color)\")\n    \n  squares.selectAll(\"polygon\").data(data).enter()\n    .append(\"polygon\")\n    .style(\"fill\", \"plum\")\n    .style(\"opacity\", 0.5)\n    \n  function drawFitLine() {\n    var a = Number(aInput.property(\"value\"));\n    d3.select(\"#a-output\").text(a);\n    var b = Number(bInput.property(\"value\"));\n    d3.select(\"#b-output\").text(b);\n    \n    var yhat1 = a\n    var yhat2 = b * 12 + a\n    \n    fitLine\n      .attr(\"x1\", x(0))\n      .attr(\"x2\", x(12))\n      .attr(\"y1\", y(yhat1))\n      .attr(\"y2\", y(yhat2))\n      \n    var ssResidual = 0;\n    \n    for(var i = 0; i &lt; data.length; i++) {\n      var yHat = b * data[i].xVal + a;\n      var residual = data[i].yVal - yHat\n      data[i].yHat = yHat;\n      data[i].res = residual;\n      \n      ssResidual += Math.pow(residual, 2);\n    }\n    \n    residuals.selectAll(\"line\")\n        .attr(\"y2\", d =&gt; y(d.yHat))\n        \n    squares.selectAll(\"polygon\")\n      .attr(\"points\", d =&gt; [[x(d.xVal), y(d.yVal)],\n                            [x(d.xVal), y(d.yHat)],\n                            [x(d.xVal + Math.abs(d.res)), y(d.yHat)],\n                            [x(d.xVal + Math.abs(d.res)), y(d.yVal)]])\n                            \n    ssResidualSquare\n    .attr(\"points\", [[x(0), y(10)],\n                     [x(0), y(10 - Math.sqrt(ssResidual))],\n                     [x(0 + Math.sqrt(ssResidual)), y(10 - Math.sqrt(ssResidual))],\n                     [x(0 + Math.sqrt(ssResidual)), y(10)]])\n  }\n  \n  drawFitLine();\n  \n  // this is the dashed outline of the minimum SSresidual\n  sumSquaresPlot.append(\"polygon\")\n    .attr(\"points\", [[x(0), y(10)],\n                     [x(0), y(10 - Math.sqrt(6.5))],\n                     [x(Math.sqrt(6.5)), y(10 - Math.sqrt(6.5))],\n                     [x(Math.sqrt(6.5)), y(10)]])\n    .style(\"fill\", \"none\")\n    .style(\"stroke\", \"black\")\n    .style(\"stroke-dasharray\", [5, 3])\n    \n    sumSquaresPlot.append(\"text\")\n      .text(\"LSE solution\")\n      .attr(\"x\", x(0))\n      .attr(\"y\", y(10.05))\n      .style(\"font-size\", \"0.4em\")\n      \n    sumSquaresPlot.append(\"text\")\n      .text(\"SSresidual\")\n      .attr(\"x\", x(0))\n      .attr(\"y\", y(9.6))\n      .style(\"font-size\", \"0.5em\")\n      .style(\"fill\", \"white\")\n  \n}"
  },
  {
    "objectID": "slides/15_ANOVA.html#comparing-groups-t-statistic",
    "href": "slides/15_ANOVA.html#comparing-groups-t-statistic",
    "title": "PSYC BC1101",
    "section": "Comparing groups: \\(t\\) statistic",
    "text": "Comparing groups: \\(t\\) statistic\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\n\\(t = \\dfrac{ \\textrm{difference between groups}} {\\textrm{difference expected due to chance}}\\)\n\\[\\begin{align}\nt &= \\dfrac{(M_1-M_2)-(\\mu_1-\\mu_2)}{s_{(M_1-M_2)}} \\\\\n&= \\dfrac{11 - 4}{1.29} \\\\\n&= 5.42\n\\end{align}\\]"
  },
  {
    "objectID": "slides/15_ANOVA.html#comparing-groups-t-statistic-1",
    "href": "slides/15_ANOVA.html#comparing-groups-t-statistic-1",
    "title": "PSYC BC1101",
    "section": "Comparing groups: \\(t\\) statistic",
    "text": "Comparing groups: \\(t\\) statistic\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\\(t = 5.42\\)\n\n\n\\(t = \\dfrac{\\text{difference between groups}}{\\text{difference expected due to chance}}\\)\nis analogous to…\n\\(\\dfrac{treatment \\cdot chance}{chance}\\)"
  },
  {
    "objectID": "slides/15_ANOVA.html#comparing-groups-variances",
    "href": "slides/15_ANOVA.html#comparing-groups-variances",
    "title": "PSYC BC1101",
    "section": "Comparing groups: Variances",
    "text": "Comparing groups: Variances\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\\(t = 5.42\\)\n\n\n\nTotal\nvariability in data\n\n\nVariability\nbetween groups\n\n\n\nChance\nTreatment effect\n\n\n\nVariability\nwithin groups\n\n\n\nChance\n\n\n\n\n\\(\\dfrac{\\text{variability between groups}}{\\text{variability within groups}}\\)\nis analogous to…\n\\(\\dfrac{treatment \\cdot chance}{chance}\\)"
  },
  {
    "objectID": "slides/15_ANOVA.html#total-variance",
    "href": "slides/15_ANOVA.html#total-variance",
    "title": "PSYC BC1101",
    "section": "Total variance",
    "text": "Total variance\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\\(t = 5.42\\)\n\n\nTotal variance\n\n\\(SS_{total}\\)\nFind sum of squared deviations of all scores (ignoring different groups) from grand mean (mean of all scores)\n\\(df_{total} = N - 1\\)\n\n\n\\[\\begin{align}\n\\text{Variance}_{total} &= \\dfrac{SS_{total}}{df_{total}} \\\\\n&= \\dfrac{83.5}{5} \\\\\n&= 16.7\n\\end{align}\\]"
  },
  {
    "objectID": "slides/15_ANOVA.html#within-groups-variance",
    "href": "slides/15_ANOVA.html#within-groups-variance",
    "title": "PSYC BC1101",
    "section": "Within groups variance",
    "text": "Within groups variance\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\\(t = 5.42\\)\n\\(\\text{Variance}_{total} = 16.7\\)\n\n\nVariance within groups\n\n\\(SS_{within} = \\Sigma SS_{each \\ treatment}\\)\nFind \\(SS\\) for each individual group from respective group mean\nThen add \\(SS\\) values\n\\(df_{within} = \\Sigma df_{each \\ treatment}\\)\n\n\n\\[\\begin{align}\n\\text{Variance}_{within} &= \\dfrac{SS_{within}}{df_{within}} \\\\\n&= \\dfrac{8 + 2}{2 + 2} \\\\\n&= 2.5\n\\end{align}\\]"
  },
  {
    "objectID": "slides/15_ANOVA.html#between-groups-variance",
    "href": "slides/15_ANOVA.html#between-groups-variance",
    "title": "PSYC BC1101",
    "section": "Between groups variance",
    "text": "Between groups variance\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\\(t = 5.42\\)\n\\(\\text{Variance}_{total} = 16.7\\)\n\\(\\text{Variance}_{within} = 2.5\\)\n\n\nTotal variability in data = Var between groups + Var within groups\nTherefore…\n\n\n\\(SS_{between} = SS_{total} – SS_{within}\\)\n\\(df_{between} = df_{total} – df_{within}\\)\n\\[\\begin{align}\n\\text{Variance}_{between} &= \\dfrac{SS_{between}}{df_{between}} \\\\\n&= \\dfrac{83.5 - 10}{5-4} \\\\\n&= 73.5\n\\end{align}\\]"
  },
  {
    "objectID": "slides/15_ANOVA.html#ratio-of-variances",
    "href": "slides/15_ANOVA.html#ratio-of-variances",
    "title": "PSYC BC1101",
    "section": "Ratio of variances",
    "text": "Ratio of variances\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\\(t = 5.42\\)\n\\(\\text{Variance}_{total} = 16.7\\)\n\\(\\text{Variance}_{within} = 2.5\\) \\(\\text{Variance}_{between} = 73.5\\)\n\n\n\nTotal\nvariability in data\n\n\nVariability\nbetween groups\n\n\n\nChance\nTreatment effect\n\n\n\nVariability\nwithin groups\n\n\n\nChance\n\n\n\n\\(\\dfrac{\\text{variance between groups}}{\\text{variance within groups}}\\)\n\\(\\dfrac{73.5}{2.5} = 29.4\\)"
  },
  {
    "objectID": "slides/15_ANOVA.html#the-f-ratio",
    "href": "slides/15_ANOVA.html#the-f-ratio",
    "title": "PSYC BC1101",
    "section": "The \\(F\\) ratio",
    "text": "The \\(F\\) ratio\n\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n\n\n\n\n9\n3\n\n\n11\n5\n\n\n13\n4\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\n\n\n\n\n\\(t = 5.42\\)\n\\(F = 29.4\\)\n\n\n\nTotal\nvariability in data\n\n\nVariability\nbetween groups\n\n\n\nChance\nTreatment effect\n\n\n\nVariability\nwithin groups\n\n\n\nChance\n\n\n\n\n\\(\\dfrac{\\text{variance between groups}}{\\text{variance within groups}}\\)\nis analogous to…\n\\(\\dfrac{treatment \\cdot chance}{chance}\\)"
  },
  {
    "objectID": "slides/15_ANOVA.html#more-complicated-design",
    "href": "slides/15_ANOVA.html#more-complicated-design",
    "title": "PSYC BC1101",
    "section": "More complicated design",
    "text": "More complicated design\n\n\n\n\n\n\n\n\n\n\n\nManipulation\n\n\n\n🍌\nBanana\n🍬\nCandy\n😐\nControl\n\n\n\n\n9\n3\n5\n\n\n11\n5\n6\n\n\n13\n4\n7\n\n\n\n\n\n\n\n\n\\(M = 11\\)\n\\(M = 4\\)\n\\(M = 6\\)\n\n\n\n\n\n\nDifferences among 3 means\n\nDid banana improve scores? Candy bar harm scores? Both? Neither?"
  },
  {
    "objectID": "slides/15_ANOVA.html#limitation-of-t-test",
    "href": "slides/15_ANOVA.html#limitation-of-t-test",
    "title": "PSYC BC1101",
    "section": "Limitation of \\(t\\) test",
    "text": "Limitation of \\(t\\) test\n\nCan only compare two populations\n\n\\(H_0\\): \\(\\mu_1 - \\mu_2 = 0\\)"
  },
  {
    "objectID": "slides/15_ANOVA.html#advantage-of-anova",
    "href": "slides/15_ANOVA.html#advantage-of-anova",
    "title": "PSYC BC1101",
    "section": "Advantage of ANOVA",
    "text": "Advantage of ANOVA\n\nANOVA is a tool for the general case\n\nComparing any number of populations\n\\(H_0\\): \\(\\mu_1 = \\mu_2 = \\mu_3 = \\dots = \\mu_n\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC BC1101 STATISTICS",
    "section": "",
    "text": "Professor: Dr. Rob Brotherton (rbrother@barnard.edu)\nOffice Hours: Thursday, 9-10AM, 415M Milbank\nLecture time: Tu/Th 10:10-11:25AM\nLecture venue: LL002 MILSTEIN\nSection 001 recitation: Wed 10:10-12:00PM, 516 Milstein\nSection 002 recitation: Wed 12:10-2:00PM, 222 Milbank Hall\nSection 003 recitation: Wed 2:10-4:00PM, 111 Milstein"
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "PSYC BC1101 STATISTICS",
    "section": "Course Overview",
    "text": "Course Overview\nThis course, required for the psychology major, provides an introduction to statistical methods commonly used in psychological research. Topics include measures of central tendency and variability, probability and sampling, confidence intervals and hypothesis testing, t-test and analysis of variance, correlation and regression. In addition to learning the conceptual and mathematical underpinnings of these techniques, you will learn to calculate and interpret statistics with reference to real-world contexts and research questions typical in psychological research. The course includes recitation meetings that provide instruction in the analysis of psychological data using the R statistical coding language commonly used by behavioral science researchers.\nThe course is designed to fulfill these goals:\n\nUnderstand what statistics are, and why and when they are needed\nTranslate observations about the world into statistical statements and questions\nLearn how statistical methods are used to test psychological hypotheses\nUse statistical software (R) to describe, visualize, analyze, and report data"
  },
  {
    "objectID": "index.html#class-schedule",
    "href": "index.html#class-schedule",
    "title": "PSYC BC1101 STATISTICS",
    "section": "Class Schedule",
    "text": "Class Schedule\n\n\n\n\n\nDate\nLecture / Recitation Topic\n\n\n\n\n1/20/26\n1: Course overview\n\n\n1/21/26\n(no recitation first week)\n\n\n1/22/26\n2: Variables & measurement\n\n\n1/27/26\n3: Frequency\n\n\n1/28/26\nProblem Set 1\n\n\n1/29/26\n4: Central tendency\n\n\n2/3/26\n5: Variability\n\n\n2/4/26\nProblem Set 2\n\n\n2/5/26\n6: z-scores\n\n\n2/10/26\n7: Probability\n\n\n2/11/26\nProblem Set 3\n\n\n2/12/26\n8: Sampling\n\n\n2/17/26\nReview\n\n\n2/18/26\n-\n\n\n2/19/26\nEXAM 1\n\n\n2/24/26\n9: Hypothesis testing\n\n\n2/25/26\nProblem Set 4\n\n\n2/26/26\n10: Hypothesis testing continued\n\n\n3/3/26\n11: the t-test\n\n\n3/4/26\nProblem Set 5\n\n\n3/5/26\n12: t-test continued\n\n\n3/10/26\n13: t-test: Independent samples\n\n\n3/11/26\nProblem Set 6\n\n\n3/12/26\n14: t-test: Related samples\n\n\n3/17/26\n(no classes)\n\n\n3/18/26\n(no classes)\n\n\n3/19/26\n(no classes)\n\n\n3/24/26\nReview\n\n\n3/25/26\n-\n\n\n3/26/26\nEXAM 2\n\n\n3/31/26\n15: ANOVA\n\n\n4/1/26\nProblem Set 7\n\n\n4/2/26\n16: ANOVA continued\n\n\n4/7/26\n17: ANOVA: Repeated measures\n\n\n4/8/26\nProblem Set 8\n\n\n4/9/26\n18: ANOVA: 2-factor ANOVA\n\n\n4/14/26\n19: Correlation\n\n\n4/15/26\nProblem Set 9\n\n\n4/16/26\n20: Regression\n\n\n4/21/26\nPutting it all together\n\n\n4/22/26\nProblem Set 10\n\n\n4/23/26\nPutting it all together\n\n\n4/28/26\nReview\n\n\n4/29/26\n-\n\n\n4/30/26\nEXAM 3"
  },
  {
    "objectID": "index.html#course-format",
    "href": "index.html#course-format",
    "title": "PSYC BC1101 STATISTICS",
    "section": "Course format",
    "text": "Course format\n\nVideo lectures\nThere are no required readings. Instead, you will be required to watch recorded lectures before attending the associated class session. You must watch the video lecture before attending the associated class session. These video lectures will be around 15 to 25 minutes each. They will include ‘quizzes’ that appear during the video, requiring your response before you continue with the lecture. So even though the videos themselves are short, you should plan to spend an hour or more with each one including the time it takes you to answer the questions and complete the exercises included along the way. Panopto keeps a record of your responses, and your engagement will be required and graded (see Grading).\n\n\nClass time\nSince you will have watched the recorded lecture in advance of class, class time will be devoted to review, activities, discussions, and your questions to help cement the understanding you will have begun to develop from the watching the lecture. Class activities are designed to build on the lecture you watched in advance; you will not be able to get the most out of the class without watching the associated lecture first.\n\n\nRecitation\nWhile the lecture component of the course covers the material conceptually, recitations will focus on practical application: performing statistical analyses using computer software, specifically R. No prior experience with R is required. For a quick primer, see this page. Problem sets will be accessed via RStudio Cloud, a free, web-browser-based version of the RStudio software interface that facilitates coding and running analyses using the R coding language. Instructions and advice will be self-contained in the problem sets as comments and pre-written demonstration code, and additional help and guidance will be available during recitation sessions. You will complete the problem sets by writing and executing code to answer the problems. Problem sets will be due at the end of your recitation period. You should plan to begin the problem set in advance of recitation and come to recitation to receive assistance with any problems you run into or just to check you were doing things correctly and as effectively as possible.\n\n\nExams\nThere will be three multiple choice exams throughout the course (see class schedule). The exams will cover material from the lectures (not R). Each exam will cover approximately one-third of the course material. The lecture before each exam will be set aside for a review discussion. Also, on weeks with exams, there will be no recitation problem set due.\n\n\nExpected workload\nThe college usually expects each course credit to correspond to 3 hours of work in and/or outside of the classroom. Since this is a 4 credit course, that means a commitment of 12 hours per week. The in-person component consists of 2 lectures per week and one recitation, totaling 4.5 hours. Outside of class, you will watch recorded lectures (and complete associated quizzes/open-response questions) and work on problem sets before the associated classes, adding another ~5 hours. Revision for exams, participation via the class notes Google Doc, checking over graded problem sets, attendance of office hours, etc, will constitute the remainder of your time spent on this course."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "PSYC BC1101 STATISTICS",
    "section": "Grading",
    "text": "Grading\nYour final grade will be based on your scores for each of the following components, weighted as follows:\n\nExams: 50%\nProblem Sets: 30%\nParticipation: 20%\n\nNumeric scores will be rounded up or down to the nearest whole number and letter grades will be determined according to the following boundaries:\nLetter grade:  A+ A  A- B+ B  B- C+ C  C- D  F\nNumeric score: 97 93 90 87 83 80 77 73 70 60 &lt;60\n\nParticipation\nParticipation across the semester will contribute 20% of your final grade. Participation includes your engagement with the Panopto lectures and their in-lecture questions, as well as coming to class and recitation prepared to discuss and ask questions about the lecture and recitation material. You will be able to earn a passing grade by watching the lectures and completing the in-lecture quizzes, though earning an outstanding grade (i.e, A+) will require regular participation in discussions.\n\n\nProblem Sets\nIn total, the Recitation Problem Sets will contribute 30% of your final grade. Problem Sets will be due at the end of your recitation session. For each Problem Set, you will receive a grade of 0, 1, or 2, where 0 means you didn’t submit it; 1 means a submission with substantial omissions; and 2 means a valid, complete attempt. Note that in R, there is often more than one way to arrive at a correct solution. This grading scheme is intended to encourage you to explore different ways of doing things and take your best shot at solving all the problems, even if you are unsure whether you are doing things correctly. Full credit will be given where effort has been made, even if the answers are incorrect. You can show the effort you made, as well as highlighting aspects you don’t find clear, by commenting your code thoroughly.\n\n\nExams\nThe 3 multiple choice exams (see dates in class schedule) will contribute a total of 50% of your final grade."
  },
  {
    "objectID": "index.html#additional-resources",
    "href": "index.html#additional-resources",
    "title": "PSYC BC1101 STATISTICS",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nEmpirical Reasoning Center\nIf you feel like you need guidance with R outside of Recitation & office hours you can meet with fellows at Barnard’s Empirical Reasoning Center. Many of the fellows specialize in R and have walk-in hours. https://erc.barnard.edu/\n\n\nAcademic Accommodations and general wellness\nIt is always important to recognize the different pressures, burdens, and stressors you may be facing, whether personal, emotional, physical, financial, mental, or academic. The current circumstances may well add to these challenges for many people in many ways. The college recognizes this, and is prepared to provide assistance to students in need. Many of the available services and sources of help are being reshaped in response to the changing circumstances. Rather than include boilerplate text here or link to sources of information which may become outdated, I encourage you to seek advice from your advisor, Dean, or the Center for Accessibility Resources & Disability Services (CARDS), and to let me know of any issues you wish to share with me that you feel are impacting your ability to complete the course to the best of your ability."
  },
  {
    "objectID": "ojs/confidence-interval-coverage.html",
    "href": "ojs/confidence-interval-coverage.html",
    "title": "Confidence Interval Coverage",
    "section": "",
    "text": "CI = 80\n   Add to plot Random\n\n\n\n\n\n\n\n\njStat = require(\"../js/jstat.js\")\n\nchart = {\n\n  const w = 1050\n  const h = 600\n  const margin = {left: 50, right: 50, top: 50, bottom: 50}\n  \n  var x = d3.scaleLinear()\n    .domain([0, 30])\n    .range([margin.left, w - margin.right])\n  const y = d3.scaleLinear()\n    .domain([2, 12])\n    .range([h - margin.bottom, margin.top])\n    \n  const yAxis = d3.axisLeft(y).ticks(11);\n\n  var sampleArr = []\n  var meansArr = []\n  var ciArr = []\n  \n  const ciInput = document.getElementById('ci-input')\n  const sample = document.getElementById('m-input')\n  const sample1 = document.getElementById('sample1')\n  const sample2 = document.getElementById('sample2')\n  const sample3 = document.getElementById('sample3')\n  const button = document.getElementById('addToPlot')\n  const buttonRandom = document.getElementById('addRandom')\n  \n  var ciWidth = Number(ciInput.value)\n  \n  button.onclick = addSampleToPlot\n  \n  function addSampleToPlot() {\n  \n    var value1 = Number(sample1.value)\n    var value2 = Number(sample2.value)\n    var value3 = Number(sample3.value)\n    \n    var values = [value1, value2, value3]\n    var mean = getM(values)\n    var sd = getSD(values)\n    var ci = getCI(values, ciWidth)\n    var containsMu = ciContainsMu(mean, ci)\n    var n = sampleArr.length + 1\n    \n    \n    sampleArr.push({sample: values, mean: mean, ci: ci, containsMu: containsMu, id: n})\n    console.log(sampleArr)\n    \n    addCI(sampleArr.length, mean, ci)\n  }\n  \n  buttonRandom.onclick = function() {\n    sample1.value = Math.floor(Math.random() * 11 + 2)\n    sample3.value = Math.floor(Math.random() * 11 + 2)\n    sample2.value = Math.floor(Math.random() * 11 + 2)\n    addSampleToPlot()\n  }\n  \n  \n  ciInput.oninput = function() {\n    ciWidth = ciInput.value\n    updateCIs(ciWidth)\n  }\n\n  \n  \n  const svg = d3.select(\"#plot-container\").append(\"svg\")\n    .attr(\"width\", w)\n    .attr(\"height\", h)\n    &lt;!-- .style(\"background\", \"pink\") --&gt;\n    \n  const gridY = svg.append(\"g\")\n  \n  gridY.append(\"line\")\n    .attr(\"x1\", x(0))\n    .attr(\"x2\", x(30))\n    .attr(\"y1\", y(7))\n    .attr(\"y2\", y(7))\n    .style(\"stroke\", \"grey\")\n  \n  const axisY = svg.append(\"g\")\n    .call(yAxis)\n    .attr(\"transform\", `translate(${x(0)},0)`)\n    \n  const dots = svg.append(\"g\")\n  const lines = svg.append(\"g\")\n\n\n\n  \n  function addCI(n, point, ci) {\n  \n  &lt;!-- dot for the point estimate --&gt;\n    dots.append(\"circle\")\n      .attr(\"r\", 3)\n      .attr(\"cx\", x(n))\n      .attr(\"cy\", y(point))\n  \n  \n  &lt;!-- dot for the point estimate --&gt;\n   lines.append(\"line\")\n      .attr(\"x1\", x(n))\n      .attr(\"x2\", x(n))\n      .attr(\"y1\", y(point + ci))\n      .attr(\"y2\", y(point - ci))\n      .style(\"stroke\", ciContainsMu(point, ci))\n  }\n  \n\n\n\n  function updateCIs (confidence) {\n  \n  &lt;!-- take the array and recalculate all CIs --&gt;\n  \n  for (var i = 0; i &lt; sampleArr.length; i++) {\n    sampleArr[i].ci = getCI(sampleArr[i].sample, confidence)\n  }\n  \n  &lt;!-- then redraw all CIs on the svg --&gt;\n  lines.selectAll(\"line\").remove()\n  \n  lines.selectAll(\"line\")\n  .data(sampleArr)\n  .enter()\n    .append(\"line\")\n      .attr(\"x1\", d =&gt; x(d.id))\n      .attr(\"x2\", d =&gt; x(d.id))\n      .attr(\"y1\", d =&gt; y(d.mean + d.ci))\n      .attr(\"y2\", d =&gt; y(d.mean - d.ci))\n      .style(\"stroke\", d =&gt; ciContainsMu(d.mean, d.ci))\n  }\n  \n  \n  \n}\n\n\nfunction getM (array) {\n  const n = array.length\n  const mean = array.reduce((a, b) =&gt; a + b) / n\n  return mean\n}\n\nfunction getSD (array) {\n  const n = array.length\n  const df = n - 1\n  const mean = getM(array)\n  return Math.sqrt(array.map(x =&gt; Math.pow(x - mean, 2)).reduce((a, b) =&gt; a + b) / df)\n}\n\nfunction getCI (array, confidence) {\n  const pt = 1 - (100 - confidence)/100 * 0.5\n  const n = array.length\n  const sd = getSD(array)\n  const t = jStat.studentt.inv(pt, n - 1)\n  return t * (sd / Math.sqrt(n))\n}\n\nfunction ciContainsMu (point, ci) {\n  if (point + ci &gt; 7 && point - ci &lt; 7) return \"blue\"\n  else return \"red\"\n}"
  },
  {
    "objectID": "ojs/triplett-competition-machine.html",
    "href": "ojs/triplett-competition-machine.html",
    "title": "Competition Machine",
    "section": "",
    "text": "&lt;button id=\"practice-button\" class=\"nav\" onclick=\"practice();\"&gt;practice&lt;/button&gt;\n    &lt;button id=\"alone-button\" class=\"nav\" onclick=\"alone();\"&gt;race alone&lt;/button&gt;\n    &lt;button id=\"competition-button\" class=\"nav\" onclick=\"competition();\"&gt;race together&lt;/button&gt;\n\n\ngame = {\n\nvar start, myTimer, myTurner;\nvar sec = d3.select(\"#seconds\")\nvar cli = d3.select(\"#clicks\")\nvar flag = d3.select(\"#flag\")\nvar flag2 = d3.select(\"#flag2\")\nvar clicks = 0;\nconst target_clicks = 50;\nvar mode = practice;\n\nvar records = [{prev: 0, best: 0},{prev: 0, best: 0}]\n\nvar handleState = 0;\nconst handle = d3.select(\"#handle-text\");\nconst handle2 = d3.select(\"#handle2-text\");\nconst handleText = [\"┘\",\"└\"]\n\nfunction handleClicked() {\n    handleState++;\n    handle.text(handleText[handleState % 2]);\n}\n\nfunction turnHandle2() {\n    var handle2State = 0;\n    myTurner = setInterval( function(){\n        handle2State++;\n        handle2.text(handleText[handle2State % 2]);\n        if (handle2State &gt; target_clicks) clearInterval(myTurner);\n    }, 7000 / target_clicks);\n}\n\nfunction clicked() {\n    handleClicked();\n    \n    if (clicks==0) {startTimer(); turnHandle2();}\n    if (clicks &lt; target_clicks) {\n        clicks++;\n    cli.text(clicks);\n    flag.transition().duration(300).style(\"left\", `${clicks*(100/target_clicks)*1.00}%`);\n    } \n    if (clicks == target_clicks) stopTimer();\n}\n\nfunction startTimer() {\n    start = Date.now();\n    myTimer= setInterval( function(){\n        var delta = Date.now() - start; // milliseconds elapsed since start\n        sec.html(delta / 1000);\n    }, 10);\n\n    flag2.transition().ease(d3.easeLinear).duration(7000).style(\"left\", \"100%\")\n}\n\nfunction stopTimer() {\n    clearInterval(myTimer);\n}\n\nfunction practice() {\n    reset();\n    d3.selectAll(\".nav\").classed(\"selected\", false);\n    d3.select(\"#practice-button\").classed(\"selected\", true);\n    // d3.select(\"#practice\").style(\"display\", \"block\");\n    d3.select(\"#instructions\").style(\"display\", \"block\");\n    d3.select(\"#track2\").style(\"display\", \"none\");\n    d3.select(\"#time\").style(\"display\", \"none\");\n}\n\nfunction alone() {\n    reset();\n    d3.selectAll(\".nav\").classed(\"selected\", false);\n    d3.select(\"#alone-button\").classed(\"selected\", true);\n    d3.select(\"#time\").style(\"display\", \"block\");\n    d3.select(\"#track2\").style(\"display\", \"none\");\n    d3.select(\"#instructions\").style(\"display\", \"none\");\n}\n\nfunction competition() {\n    reset();\n    d3.selectAll(\".nav\").classed(\"selected\", false);\n    d3.select(\"#competition-button\").classed(\"selected\", true);\n    d3.select(\"#time\").style(\"display\", \"block\");\n    d3.select(\"#track2\").style(\"display\", \"block\");\n    d3.select(\"#instructions\").style(\"display\", \"none\");\n}\n\nfunction reset() {\n    clearInterval(myTimer);\n    clearInterval(myTurner);\n    sec.text(\"0.00\");\n    cli.text(0);\n    flag.transition().duration(1000).style(\"left\", \"0%\");\n    flag2.transition().duration(1000).style(\"left\", \"0%\");\n    clicks = 0;\n}\n\npractice();\n\n\n}"
  },
  {
    "objectID": "ojs/confidence-interval-interactive.html",
    "href": "ojs/confidence-interval-interactive.html",
    "title": "Confidence",
    "section": "",
    "text": "Confidence: 95%\nn = 30\nM =  \nSD =  \n\nσM = \n\n\n\n\n\n\njStat = require(\"https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js\")\n\n\n\n\n\n\n\nci = {\n\n  const w = 1050\n  const h = 600\n  const m = {t: 0, r: 0, l: 0, b: 100}\n  \n  const f = d3.format(\".2f\")\n  const x = d3.scaleLinear()\n    .range([0,w])\n  const y = d3.scaleLinear()\n    .range([h-m.b,m.t])\n  const line = d3.line()\n    .x(d =&gt; x(d.value))\n    .y(d =&gt; y(d.density))\n  const arrows = [{id: -1,points: [[0,0],[5,5],[5,-5]]},\n                  {id: 1, points: [[-5,5],[0,0],[-5,-5]]}]\n                  \n  const xAxis = d3.axisBottom(x)\n  \n  var confidence, n, mean, sd, std_err, critical, xlim;\n  \n  const confidenceInput = document.getElementById('confidence-input')\n  const nInput = document.getElementById('n-input')\n  const meanInput = document.getElementById('mean-input')\n  const sdInput = document.getElementById('sd-input')\n  \n  function updateCritical() {\n    var upper = 1 - (1 - confidence/100)*0.5\n    var lower = 1 - upper\n    critical = [jStat.normal.inv(lower, mean, std_err),\n                jStat.normal.inv(upper, mean, std_err)]\n  }\n  \n  function getParams() {\n    confidence = confidenceInput.value\n    n = nInput.value\n    mean = meanInput.value\n    sd = sdInput.value\n    std_err = sd / Math.sqrt(n)\n    updateCritical()\n    xlim = [mean-3*std_err,Number(mean+3*std_err)]\n    x.domain([xlim[0], xlim[1]])\n    y.domain([0, jStat.normal.pdf(0, mean, std_err)+0.01])\n    d3.select(\"#std-err-value\").text(f(std_err))\n  }\n  \n  \n  confidenceInput.oninput = function() {\n    confidence = confidenceInput.value\n    d3.select(\"#confidence-value\").text(confidence + \"%\")\n    updateCritical();\n    updateCurve();\n    updateClip();\n  };\n  \n  nInput.oninput = function() {\n    n = nInput.value\n    std_err = sd/Math.sqrt(n)\n    d3.select(\"#n-value\").text(n)\n    d3.select(\"#std-err-value\").text(f(std_err))\n    updateCritical();\n    updateCurve();\n    updateClip();\n  };\n    \n  meanInput.oninput = function() {\n    mean = Number(meanInput.value)\n    updateCritical();\n    updateCurve();\n    updateClip();\n  };\n  \n  sdInput.oninput = function() {\n    sd = sdInput.value\n    std_err = sd/Math.sqrt(n)\n    d3.select(\"#std-err-value\").text(f(std_err))\n    updateCritical();\n    updateCurve();\n    updateClip();\n  };\n  \n  function makeCurve(mean, std_err) {\n    var values = jStat(xlim[0], xlim[1], 210)[0],\n        arr = [];\n    for (var i in values) {\n      arr.push({\n          value: values[i], \n          density: jStat.normal.pdf(values[i], mean, std_err)\n      })\n    }\n    return arr;\n  }\n  \n  function updateCurve() {\n    xlim = [Number(mean-3*std_err),Number(mean+3*std_err)]\n    &lt;!-- console.log(xlim) --&gt;\n    x.domain([xlim[0], xlim[1]])\n    y.domain([0, jStat.normal.pdf(mean, mean, std_err)+0.01])\n    \n    axis.call(xAxis)\n    \n    curveLine.attr(\"d\", line(makeCurve(mean, std_err)))\n    curveFill.attr(\"d\", line(makeCurve(mean, std_err)))\n    \n    meanLine.select(\"line\")\n      .attr(\"x1\", x(mean)).attr(\"x2\", x(mean))\n      .attr(\"y1\", y(0)).attr(\"y2\", y(jStat.normal.pdf(mean, mean, std_err)))\n    meanLine.select(\"circle\")\n      .attr(\"cx\", x(mean)).attr(\"cy\", y(0)).attr(\"r\", 5)\n      \n    marginOfError.attr(\"transform\", `translate(0, ${y(jStat.normal.pdf(critical[0], mean, std_err)/2)})`)\n    marginOfError.select(\"line\")\n      .attr(\"x1\", x(critical[0])).attr(\"x2\", x(critical[1]))\n    marginOfError.select(\"#arrow-l\").attr(\"transform\", `translate(${x(critical[0])}, 0) scale(2)`)\n    marginOfError.select(\"#arrow-r\").attr(\"transform\", `translate(${x(critical[1])}, 0) scale(2)`)\n  }\n  \n  function updateClip() {\n    clip.attr(\"points\", [[x(critical[0]),0],[x(critical[0]),h],[x(critical[1]),h],[x(critical[1]),0]])\n  }\n  \n  const svg = d3.select(\"#confidence-interval\").append(\"svg\")\n    .attr(\"viewBox\", \"0 0 \" + w + \" \" + h)\n    .attr(\"preserveAspectRatio\", \"xMinYMin meet\")\n    .attr(\"class\", \"svg-content\")\n    \n  const axis = svg.append(\"g\").attr(\"transform\", `translate(0, ${y(0)})`)\n    .style(\"font-size\", \"2em\")\n  \n  const curveLine = svg.append(\"path\")\n    .style(\"fill\", \"none\")\n    .style(\"stroke\", \"black\")\n    .style(\"stroke-width\", 3)\n    .attr(\"class\", \"invertable\")\n    \n  const clip = svg.append(\"clipPath\").attr(\"id\", \"clip\").append(\"polygon\")\n\n  const curveFill = svg.append(\"path\")\n    .attr(\"clip-path\", \"url(#clip)\")\n    .style(\"fill\", \"lightblue\")\n    .style(\"stroke\", \"none\")\n    \n  const meanLine = svg.append(\"g\")\n  meanLine.append(\"line\")\n    .style(\"stroke\", \"black\")\n    .style(\"stroke-dasharray\", [10,10])\n  meanLine.append(\"circle\")\n    \n\n  const marginOfError = svg.append(\"g\")\n    .style(\"stroke\", \"black\")\n    &lt;!-- .classed(\"invertable\", true) --&gt;\n    \n  marginOfError.append(\"line\")\n    .style(\"stroke\", \"black\")\n\n  marginOfError.append(\"polygon\").attr(\"points\", arrows[0].points).attr(\"id\", \"arrow-l\")\n  marginOfError.append(\"polygon\").attr(\"points\", arrows[1].points).attr(\"id\", \"arrow-r\")\n\n\n\n  getParams();\n  updateClip();\n  updateCurve();\n  \n}"
  },
  {
    "objectID": "ojs/balance-beam2.html",
    "href": "ojs/balance-beam2.html",
    "title": "Balance beam",
    "section": "",
    "text": "Show/hide deviations\n\n\nShow/hide squared deviations\n\n\n\n\n\n\nwidth = 600\nheight = 600\nn_boxes = 5\nscale_width = 11\nbox_size = width / scale_width\nbeam_height = box_size / 4\nradius = box_size\n\n// multiply the deviations lined up at the bottom to fit on the screen\nmultiplier = 6/n_boxes\n\nbox_data_ = make_box_data(n_boxes)\n\n\nchart = {\n\n  let deviations_hidden = false;\n  let squared_deviations_hidden = false;\n  \n  &lt;!-- const controls = d3.select(\"#controls\") --&gt;\n  &lt;!-- const button = controls --&gt;\n  &lt;!--     .append(\"input\") --&gt;\n  &lt;!--     .attr(\"type\", \"button\") --&gt;\n  &lt;!--     .attr(\"name\", \"showDevs\") --&gt;\n  &lt;!--     .attr(\"value\", \"Toggle\") --&gt;\n  &lt;!--     .attr(\"onclick\", noDevs) --&gt;\n\n  function round_position(x) {\n    let interval = width / scale_width\n    return Math.round(x / interval)*interval\n  }\n\n  function rounded_position_index(x) {\n    let interval = width / scale_width\n    let x0 = Math.round(x / interval)*interval\n    return Math.round(x0/(width/scale_width))  \n  }\n  \n  let positions = d3.range(scale_width).map(i =&gt; (0))\n\n  function stack_boxes() {\n    for (let i = 0; i &lt; box_data.length; i++) {\n      box_data[i].level = positions[box_data[i].x0]\n      positions[box_data[i].x0]++\n    }\n  }\n\n  let starting_boxes = [1, 2, 6, 6, 10]\n  \n  // let box_data = d3.range(n_boxes).map(i =&gt; ({\n  //     // x: (Math.random() * (width - box_size * 2) + box_size),\n  //     x0: Math.floor(Math.random() *   scale_width),\n  //     y: -box_size, // height/2 - beam_height/2 - radius,\n  //     color: d3.schemeCategory10[i % 10],\n  //     level: 0\n  //     }))\n      \n  let box_data = box_data_;\n\n  for (let i = 0; i &lt; box_data.length; i++) box_data[i].x = box_data[i].x0 * box_size\n  \n  \n  let mean = box_data.reduce((total, next) =&gt; total + next.x + box_size/2, 0) / box_data.length\n  let pivot = mean\n  \n  box_data = box_data.sort(function(a, b) { return Math.abs(a.x - pivot) - Math.abs(b.x - pivot); })\n  for (let i = 0; i &lt; box_data.length; i++) {\n    box_data[i].id = i\n  }\n    \n    stack_boxes()\n    \n    \n  const svg = d3.select(\"#chart\").append(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height])\n      .attr(\"stroke-width\", 2)\n\n  const x_ = d3.scaleLinear()\n    .range([0, width])\n    .domain([0, 10])\n    \n  // draw the ground\n  svg.append(\"rect\")\n    .attr(\"width\", width)\n    .attr(\"height\", 50)\n    .attr(\"fill\", \"#f0f0f0\")\n    .attr(\"stroke\", \"none\")\n    .attr(\"transform\", `translate(0, ${height - 51})`)\n\n  const beam_and_boxes = svg.append(\"g\")\n    .attr(\"transform\", `translate(0, ${height - 50 - radius - beam_height})`)\n  \n  // draw the beam\n  beam_and_boxes.append(\"rect\")\n    .attr(\"width\", width)\n    .attr(\"height\",  beam_height)\n    .attr(\"rx\", 3)\n    .attr(\"fill\", \"black\")\n    // .attr(\"transform\", `translate(0, ${height/2 - beam_height / 2})`)\n\n  const boxes = beam_and_boxes.append(\"g\")\n  const deviations = beam_and_boxes.append(\"g\")\n  const deviations_vertical = deviations.append(\"g\")\n  const deviations_horizontal = deviations.append(\"g\")\n  const deviations_sum = svg.append(\"g\").attr(\"transform\", `translate(0, ${height - 25})`)\n  const deviations_sum_negative = deviations_sum.append(\"g\").attr(\"transform\", `translate(0, -10)`)\n  const deviations_sum_positive = deviations_sum.append(\"g\").attr(\"transform\", `translate(0, 10)`)\n  \n  // draw the boxes\n  boxes.selectAll(\"rect\")\n    .data(box_data)\n    .join(\"rect\")\n      .attr(\"x\", d =&gt; d.x)\n      .attr(\"y\", d =&gt; d.y - d.level*box_size)\n      .attr(\"width\", box_size)\n      .attr(\"height\", box_size)\n      .attr(\"rx\", 1)\n      .attr(\"fill\", d =&gt; d.color)\n      .attr(\"stroke\", null)\n      .call(d3.drag().on(\"start\", start_dragging_box)\n                     .on(\"drag\", dragging_box)\n                     .on(\"end\", stop_dragging_box));\n  \n  // draw a circle at the true mean point\n  const circle = beam_and_boxes.append(\"circle\")\n    .attr(\"cx\", mean)\n    .attr(\"cy\", beam_height/2)\n    .attr(\"r\", 5)\n    .attr(\"fill\", \"#777777\")\n    .call(d3.drag()\n             .on(\"start\", start_dragging_mean)\n             .on(\"drag\", dragging_mean)\n             .on(\"end\", stop_dragging_mean))\n  \n  // draw a triangle at the pivot point\n  const triangle = svg.append(\"polygon\")\n    .attr(\"points\", [[0, -radius/2], [radius/2, radius/2], [-radius/2, radius/2]])\n    .attr(\"fill\", \"red\")\n    .attr(\"transform\", `translate(${pivot}, ${height-50-radius/2})`)\n    .call(d3.drag()\n             .on(\"start\", start_dragging_mean)\n             .on(\"drag\", dragging_mean)\n             .on(\"end\", stop_dragging_mean))\n  \n    \n  draw_deviations()\n  \n\n  function start_dragging_mean(event, d) {triangle.attr(\"fill\", \"dodgerblue\")}\n  \n  function stop_dragging_mean(event, d) {triangle.attr(\"fill\", \"red\")}\n  \n  function dragging_mean(event, d) {\n      pivot = event.x\n      tip_scale()\n  }\n  \n  function tip_scale() {\n  \n    let angle = Math.abs((mean - pivot)*0.5);\n    let hypotenuse, direction;\n    \n    if (pivot &lt; mean) {\n      hypotenuse = width - pivot;\n      direction = 1;\n    } else {\n      hypotenuse = pivot;\n      direction = -1;\n    }\n    \n    let tri_angle = 90 - (Math.acos(radius / hypotenuse) * 180/Math.PI)\n    angle = direction * (Math.min(angle, tri_angle))\n    \n      triangle\n        .attr(\"transform\", `translate(${pivot}, ${height-50-radius/2})`)\n      beam_and_boxes\n        .attr(\"transform\", `translate(0, ${height-50-radius-beam_height}) rotate(${angle}, ${pivot}, ${0})`)\n      draw_deviations()\n  }\n\n  \n  function start_dragging_box(event, d) {\n    d3.select(this).raise().attr(\"stroke\", \"black\")\n    // console.log(\"x:\" + d.x + \" x0:\" + d.x0 + \" level:\" + d.level)\n  }\n\n  function dragging_box(event, d) {\n    box_data[d.id].x = event.x\n    // box_data[d.id].x0 = round_position(box_data[d.id].x) + box_size/2\n    \n    let current_position = box_data[d.id].x0\n    let current_level = box_data[d.id].level\n    let new_position = rounded_position_index(event.x)\n    \n    if(new_position != current_position) {\n      console.log(\"moved! from \" + current_position + \" to \" + new_position)\n      \n      // update the box's position\n      box_data[d.id].x0 = new_position\n      \n      // now this box should go on top of the stack for new_position\n      box_data[d.id].level = positions[new_position]\n      \n      // and update the total number of boxes in that position\n      positions[new_position]++\n      \n      // for the old position, reduce the number of boxes by one, and\n      // bump down any boxes that had a higher level that this box\n      positions[current_position]--\n      for (let i = 0; i &lt; box_data.length; i++) {\n        if (d.id==i) continue\n        if (box_data[i].x0==current_position && box_data[i].level &gt; current_level) {\n            box_data[i].level--\n            console.log(\"movin on down\")\n        } \n      }\n    }\n    \n    tip_scale()\n    // draw_deviations()\n  }\n\n  function stop_dragging_box(event, d) {\n    d3.select(this).attr(\"stroke\", null)\n  }\n  \n  \n\n\n  function draw_deviations() {\n    \n    mean = box_data.reduce((total, next) =&gt; total + next.x + box_size/2, 0) / box_data.length\n    \n    boxes.selectAll(\"rect\")\n      .attr(\"x\", d =&gt; d.x0 * box_size)\n      .attr(\"y\", d =&gt; d.y - d.level*box_size)\n    \n    circle.attr(\"cx\", mean)\n    \n    // calculate deviations\n    for (let i = 0; i &lt; box_data.length; i++) box_data[i].dev = (box_data[i].x0*box_size + (box_size/2)) - pivot\n    \n    let deviations_negative = box_data.filter(function(d){ return d.dev &lt; 0})\n    for (let i = 0; i &lt; deviations_negative.length; i++) {\n      if (i==0) deviations_negative[i].dev_start = 0\n      else deviations_negative[i].dev_start = deviations_negative[i-1].dev_end\n      deviations_negative[i].dev_end = deviations_negative[i].dev_start + Math.abs(deviations_negative[i].dev)\n    }\n    \n    let deviations_positive = box_data.filter(function(d){ return d.dev &gt; 0})\n    for (let i = 0; i &lt; deviations_positive.length; i++) {\n      if (i==0) deviations_positive[i].dev_start = 0\n      else deviations_positive[i].dev_start = deviations_positive[i-1].dev_end\n      deviations_positive[i].dev_end = deviations_positive[i].dev_start + Math.abs(deviations_positive[i].dev)\n    }\n    \n    deviations_vertical.selectAll(\"line\")\n      .data(box_data)\n      .join(\"line\")\n        .attr(\"x1\", d =&gt; box_size * d.x0 + box_size/2)\n        .attr(\"x2\", d =&gt; box_size * d.x0 + box_size/2)\n        .attr(\"y1\", d =&gt; d.y - d.level * box_size)\n        .attr(\"y2\", d =&gt; d.y - box_size/2 - d.level * box_size - d.id * 25)\n        .attr(\"stroke\", \"black\")\n        .attr(\"stroke-dasharray\", [5, 5])\n        \n    deviations_vertical\n      .append(\"line\")\n      .attr(\"x1\", pivot).attr(\"x2\", pivot)\n      .attr(\"y1\", 0).attr(\"y2\", -height)\n      .attr(\"stroke\", \"red\").attr(\"stroke-dasharray\", [10, 10])\n\n    deviations_horizontal.selectAll(\"line\")\n      .data(box_data)\n      .join(\"line\")\n        .attr(\"x1\", d =&gt; box_size * d.x0 + box_size/2)\n        .attr(\"x2\", pivot)\n        .attr(\"y1\", (d, i) =&gt; d.y - box_size/2 - d.level * box_size - d.id * 25)\n        .attr(\"y2\", (d, i) =&gt; d.y - box_size/2 - d.level * box_size - d.id * 25)\n        .attr(\"stroke\", d =&gt; d.color)\n        .attr(\"stroke-width\", 3)\n        \n    if (!squared_deviations_hidden) update_squared_deviations();\n    \n\n    deviations_sum_negative.selectAll(\"line\")\n      .data(deviations_negative)\n      .join(\"line\")\n        .attr(\"x1\", d =&gt; d.dev_start * multiplier)\n        .attr(\"x2\", d =&gt; d.dev_end * multiplier)\n        .attr(\"stroke\", d =&gt; d.color)\n        .attr(\"stroke-width\", 8)\n        \n    deviations_sum_positive.selectAll(\"line\")\n      .data(deviations_positive)\n      .join(\"line\")\n      .attr(\"x1\", d =&gt; d.dev_start * multiplier)\n        .attr(\"x2\", d =&gt; d.dev_end * multiplier)\n        .attr(\"stroke\", d =&gt; d.color)\n        .attr(\"stroke-width\", 8)\n  };\n  \n  \n  function update_squared_deviations() {\n      deviations_horizontal.selectAll(\"rect\")\n      .data(box_data)\n      .join(\"rect\")\n        .attr(\"x\", function(d) {if(d.dev &lt; 0) {return box_size * d.x0 + box_size/2} else {return pivot} })\n        .attr(\"y\", (d, i) =&gt; d.y - box_size/2 - d.level * box_size - d.id * 25 - Math.abs(d.dev))\n        .attr(\"width\", d =&gt; Math.abs(d.dev))\n        .attr(\"height\", d =&gt; Math.abs(d.dev))\n        .attr(\"height\", function(d){if(squared_deviations_hidden){return 0} else{return Math.abs(d.dev)}})\n        .attr(\"fill\", d =&gt; d.color)\n        .attr(\"opacity\", 0.5)\n        .attr(\"stroke\", \"none\")\n  }\n  \n  \n\n  function noDevs() {\n    deviations_hidden = !deviations_hidden\n    deviations.attr(\"opacity\", Number(!deviations_hidden))\n  }\n  \n  function noDevsSq() {\n    squared_deviations_hidden = !squared_deviations_hidden\n  \n    if (squared_deviations_hidden) {\n      deviations_horizontal\n        .selectAll(\"rect\")\n        .data(box_data)\n        .transition().duration(1000)\n        .attr(\"y\", (d, i) =&gt; d.y - box_size/2 - d.level * box_size - d.id * 25)\n        .attr(\"height\", 0)\n    } else {\n        deviations_horizontal\n          .selectAll(\"rect\")\n          .data(box_data)\n          .attr(\"x\", function(d) {if(d.dev &lt; 0) {return box_size * d.x0 + box_size/2} else {return pivot} })\n          .attr(\"width\", d =&gt; Math.abs(d.dev))\n          .transition().duration(1000)\n          .attr(\"y\", (d, i) =&gt; d.y - box_size/2 - d.level * box_size - d.id * 25 - Math.abs(d.dev))\n          .attr(\"height\", d =&gt; Math.abs(d.dev))\n    }\n    \n  }\n\n  noDevs()\n  noDevsSq()\n  d3.select(\"#showDevs\").on(\"click\", noDevs)\n  d3.select(\"#showDevsSq\").on(\"click\", noDevsSq)\n\n\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction make_box_data(n) {\n    return d3.range(n).map(i =&gt; ({\n      // x: (Math.random() * (width - box_size * 2) + box_size),\n      x0: Math.floor(Math.random() *   scale_width),\n      y: -box_size, // height/2 - beam_height/2 - radius,\n      color: d3.schemeCategory10[i % 10],\n      level: 0\n      }))\n  }"
  },
  {
    "objectID": "ojs/test-ojs.html",
    "href": "ojs/test-ojs.html",
    "title": "Random",
    "section": "",
    "text": "Random\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\njStat = require(\"https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;!-- viewof quantity = html`&lt;input type=\"range\" value=\"1\" min=\"1\" max=${samples.length}&gt;` --&gt;\nviewof quantity = Inputs.range([1, samples.length], {value: 1, step: 1})\n\n\n\n\n\n\n\n\n\n\nchart = {\n\n  const svg = d3.select(DOM.svg(width, height));\n\n  const g = svg.append(\"g\").attr(\"id\", \"boxes\")\n  \n  g.selectAll(\"rect\")\n    .data(samples.slice(0, quantity))\n    .enter()\n    .append(\"rect\")\n      .attr(\"class\", \"invertable\")\n      .attr(\"fill\", \"black\")\n      .attr(\"stroke\", \"none\")\n      .attr(\"x\", d =&gt; x(d.mean_bin - 0.5))\n      .attr(\"y\", d =&gt; y(d.count))\n      .attr(\"width\", (width-margin.left-margin.right)/100 * 0.9)\n      .attr(\"height\", (height-margin.top-margin.bottom)/ylim * 0.9)\n      .each(function (d, i) {\n            if (i === quantity-1) {\n              // put all your operations on the second element, e.g.\n              d3.select(this).attr(\"fill\", \"red\");    \n            }\n          });\n      \n  svg.append(\"text\")\n      .attr(\"id\", \"xaxis\")\n      .call(xAxis)\n      .attr(\"transform\", `translate(0,${y(0)})`)\n      .attr(\"class\", \"axis\")\n      .style(\"font-size\", \"0.5em\");\n \n  svg.append(\"g\").attr(\"id\", \"xaxis\")\n      .call(xAxis)\n      .attr(\"transform\", `translate(0,${y(0)})`)\n      .attr(\"class\", \"axis\")\n      .style(\"font-size\", \"0.5em\");\n      \n  svg.append(\"g\").attr(\"id\", \"yaxis\")\n      .call(yAxis)\n      .attr(\"transform\", `translate(${x(50)}, 0)`)\n      .attr(\"class\", \"axis\")\n      .style(\"font-size\", \"0.5em\");\n      \nreturn svg.node();\n\n}\n\n\n\n\n\n\n\n\n\n\nd = d3.select(\"div#blah\")\n  .style(\"font-family\", \"KaTeX_Main\")\n  .style(\"font-size\", \"1.1em\")\n  .html(\"Observations: \" + samples[quantity-1].sample.join(\", \") + '&lt;br/&gt;' + \"&lt;i&gt;M&lt;/i&gt; = \" + samples[quantity-1].mean)\n\n\n\n\n\n\n\nwidth = 500\nheight = 250\n\nylim = 50\n\nmargin = ({top: 20, right: 20, bottom: 20, left: 20})\n\n\nx = d3.scaleLinear()\n  .domain([50, 150])\n  .range([margin.left, width - margin.right])\n\nmax_y = Math.max(...samples.map(o =&gt; o.count))\n\ny = d3.scaleLinear()\n  .domain([0, ylim])\n  .range([height - margin.bottom, margin.top])\n\nxAxis = d3.axisBottom(x).ticks(8)\n\nyAxis = d3.axisLeft(y).ticks(3)"
  },
  {
    "objectID": "ojs/balance-beam.html",
    "href": "ojs/balance-beam.html",
    "title": "Balance beam",
    "section": "",
    "text": "width = 500\nheight = 500\nradius = 32\nbeam_height = radius/2\nn_boxes = 10\n\n\n\n\nchart = {\n\n  let box_data = d3.range(n_boxes).map(i =&gt; ({\n      x: (Math.random() * (width - radius * 2) + radius),\n      &lt;!-- x: Math.floor((Math.random() * 10), --&gt;\n      y: height/2 - beam_height/2 - radius,\n      overlaps: 0,\n      level: 0\n      }))\n      \n  for (let i = 1; i &lt; box_data.length; i++) {\n    let x = box_data[i].x\n    for (let j = 0; j &lt; box_data.length; j++) {\n      if (i==j) continue\n      if (box_data[i].level != box_data[j].level) continue\n      let y = box_data[j].x\n      if (x &lt; y + radius && x + radius &gt; y) {\n        box_data[i].level++\n        continue\n      }\n    }\n  }\n  \n  console.log(box_data)\n  \n    let mean = box_data.reduce((total, next) =&gt; total + next.x, 0) / box_data.length\n  let pivot = mean\n  \n  box_data = box_data.sort(function(a, b) { return Math.abs(a.x - pivot) - Math.abs(b.x - pivot); })\n  for (let i = 0; i &lt; box_data.length; i++) {\n    box_data[i].id = i\n    box_data[i].color = d3.schemeCategory10[i % 10]\n  }\n    \n    \n    \n  const svg = d3.create(\"svg\")\n      .attr(\"viewBox\", [0, 0, width, height])\n      .attr(\"stroke-width\", 2)\n\n  const x = d3.scaleLinear()\n    .range([0, width])\n    .domain([1, 10])\n\n  const beam_and_boxes = svg.append(\"g\")\n  \n  // draw the beam\n  beam_and_boxes.append(\"rect\")\n    .attr(\"x\", 0)\n    .attr(\"y\", height/2 - beam_height / 2)\n    .attr(\"width\", width)\n    .attr(\"height\",  beam_height)\n    .attr(\"rx\", 5)\n    .attr(\"fill\", \"black\")\n\n  const boxes = beam_and_boxes.append(\"g\")\n  const deviations = beam_and_boxes.append(\"g\")\n  const deviations_vertical = deviations.append(\"g\")\n  const deviations_horizontal = deviations.append(\"g\")\n  const deviations_sum = svg.append(\"g\").attr(\"transform\", `translate(0, ${height - 20})`)\n  const deviations_sum_negative = deviations_sum.append(\"g\")\n  const deviations_sum_positive = deviations_sum.append(\"g\").attr(\"transform\", `translate(0, 10)`)\n  \n  \n  boxes.selectAll(\"rect\")\n    .data(box_data)\n    .join(\"rect\")\n      .attr(\"x\", d =&gt; d.x)\n      .attr(\"y\", d =&gt; d.y - d.level*radius)\n      .attr(\"width\", radius)\n      .attr(\"height\", radius)\n      .attr(\"rx\", 3)\n      .attr(\"fill\", d =&gt; d.color)\n      .call(d3.drag().on(\"start\", start_dragging_box)\n                     .on(\"drag\", dragging_box)\n                     .on(\"end\", stop_dragging_box));\n  \n  // draw a circle at the true mean point\n  const circle = beam_and_boxes.append(\"circle\")\n    .attr(\"cx\", mean)\n    .attr(\"cy\", height/2)\n    .attr(\"r\", 5)\n    .attr(\"fill\", \"red\")\n  \n  // draw a triangle at the pivot point\n  const triangle = svg.append(\"polygon\")\n    .attr(\"points\", [[0, -radius/2], [radius/2, radius/2], [-radius/2, radius/2]])\n    .attr(\"fill\", \"red\")\n    .attr(\"transform\", `translate(${pivot}, ${height/2 + radius * 0.75})`)\n    .call(d3.drag()\n             .on(\"start\", start_dragging_mean)\n             .on(\"drag\", dragging_mean)\n             .on(\"end\", stop_dragging_mean))\n    \n    \n  circle.call(d3.drag()\n             .on(\"start\", start_dragging_mean)\n             .on(\"drag\", dragging_mean)\n             .on(\"end\", stop_dragging_mean)\n          )\n  \n\n  // draw the ground\n  svg.append(\"line\")\n    .attr(\"x1\", 0).attr(\"x2\", width)\n    .attr(\"y1\", height/2 + radius * 1.25).attr(\"y2\", height/2 + radius * 1.25)\n    .attr(\"stroke\", \"grey\")\n    \n  draw_deviations()\n  \n\n  return svg.node();\n  \n  \n  function start_dragging_mean(event, d) {circle.attr(\"fill\", \"dodgerblue\")}\n  \n  function stop_dragging_mean(event, d) {circle.attr(\"fill\", \"red\")}\n  \n  function dragging_mean(event, d) {\n      pivot = event.x\n      let angle = Math.min(Math.max((mean - pivot)*0.1, -7), 7)\n       \n      if (angle &gt; 6) {\n        let hypotenuse = width - pivot\n        angle = 90 - Math.acos(radius / hypotenuse) * 180/Math.PI\n      } else if (angle &lt; -6) {\n        let hypotenuse = pivot\n        angle = -1 * (90 - Math.acos(radius / hypotenuse) * 180/Math.PI)\n      }\n      \n      triangle.attr(\"transform\", `translate(${pivot}, ${height/2 + radius * 0.75})`)\n      beam_and_boxes.attr(\"transform\", `rotate(${angle}, ${pivot}, ${height/2})`)\n      draw_deviations()\n  }\n  \n  function start_dragging_box(event, d) {\n    d3.select(this).raise().attr(\"stroke\", \"black\")\n    console.log(d.id)\n  }\n\n  function dragging_box(event, d) {\n    d3.select(this).attr(\"x\", event.x)\n    box_data[d.id].x = event.x\n    \n    &lt;!-- let overlaps = 0 --&gt;\n    &lt;!-- for (let i = 0; i &lt; box_data.length; i++) { --&gt;\n    &lt;!--   if (i==d.id) continue --&gt;\n    &lt;!--   let other_x = box_data[i].x --&gt;\n    &lt;!--   if (event.x &lt; (other_x + radius) && (event.x + radius) &gt; other_x) overlaps++ --&gt;\n    &lt;!--   } --&gt;\n    &lt;!--   box_data[d.id].overlaps = overlaps --&gt;\n    \n    draw_deviations()\n  }\n\n  function stop_dragging_box(event, d) {\n    d3.select(this).attr(\"stroke\", null)\n  }\n  \n  function draw_deviations() {\n    \n    mean = box_data.reduce((total, next) =&gt; total + next.x, 0) / box_data.length\n    \n    &lt;!-- boxes.selectAll(\"rect\") --&gt;\n    &lt;!--   .attr(\"y\", d =&gt; d.y - d.overlaps*radius) --&gt;\n    \n    circle.attr(\"cx\", mean)\n    for (let i = 0; i &lt; box_data.length; i++) box_data[i].dev = box_data[i].x - pivot\n    &lt;!-- let box_data_ = box_data.sort(function(a, b) { return Math.abs(a.dev) - Math.abs(b.dev); }) --&gt;\n    &lt;!-- console.log(box_data_sorted) --&gt;\n    \n    let deviations_negative = box_data.filter(function(d){ return d.dev &lt; 0})\n    for (let i = 0; i &lt; deviations_negative.length; i++) {\n      if (i==0) deviations_negative[i].dev_start = 0\n      else deviations_negative[i].dev_start = deviations_negative[i-1].dev_end\n      deviations_negative[i].dev_end = deviations_negative[i].dev_start + Math.abs(deviations_negative[i].dev)\n    }\n    \n    let deviations_positive = box_data.filter(function(d){ return d.dev &gt; 0})\n    for (let i = 0; i &lt; deviations_positive.length; i++) {\n      if (i==0) deviations_positive[i].dev_start = 0\n      else deviations_positive[i].dev_start = deviations_positive[i-1].dev_end\n      deviations_positive[i].dev_end = deviations_positive[i].dev_start + Math.abs(deviations_positive[i].dev)\n    }\n    \n    \n    deviations_vertical.selectAll(\"line\")\n      .data(box_data)\n      .join(\"line\")\n        .attr(\"x1\", d =&gt; d.x + radius/2)\n        .attr(\"x2\", d =&gt; d.x + radius/2)\n        .attr(\"y1\", d =&gt; d.y)\n        .attr(\"y2\", (d, i) =&gt; height/2 - radius*2 - d.id * 20)\n        .attr(\"stroke\", \"black\")\n        .attr(\"stroke-dasharray\", [5, 5])\n        \n    deviations_horizontal.selectAll(\"line\")\n      .data(box_data)\n      .join(\"line\")\n        .attr(\"x1\", d =&gt; d.x + radius/2)\n        .attr(\"x2\", pivot)\n        .attr(\"y1\", (d, i) =&gt; height/2 - radius*2 - d.id * 20)\n        .attr(\"y2\", (d, i) =&gt; height/2 - radius*2 - d.id * 20)\n        .attr(\"stroke\", d =&gt; d.color)\n        .attr(\"stroke-width\", 3)\n        \n    deviations_sum_negative.selectAll(\"line\")\n      .data(deviations_negative)\n      .join(\"line\")\n        .attr(\"x1\", d =&gt; d.dev_start)\n        .attr(\"x2\", d =&gt; d.dev_end)\n        .attr(\"stroke\", d =&gt; d.color)\n        .attr(\"stroke-width\", 3)\n        \n    deviations_sum_positive.selectAll(\"line\")\n      .data(deviations_positive)\n      .join(\"line\")\n        .attr(\"x1\", d =&gt; d.dev_start)\n        .attr(\"x2\", d =&gt; d.dev_end)\n        .attr(\"stroke\", d =&gt; d.color)\n        .attr(\"stroke-width\", 3)\n  }\n  \n\n\n  \n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;!-- drag = { --&gt;\n\n&lt;!--   function dragstarted(event, d) { --&gt;\n&lt;!--     d3.select(this).raise().attr(\"stroke\", \"black\") --&gt;\n&lt;!--     console.log(d.id) --&gt;\n&lt;!--   } --&gt;\n\n&lt;!--   function dragged(event, d) { --&gt;\n&lt;!--     d3.select(this).attr(\"x\", event.x) --&gt;\n&lt;!--   } --&gt;\n\n&lt;!--   function dragended(event, d) { --&gt;\n&lt;!--     d3.select(this).attr(\"stroke\", null); --&gt;\n&lt;!--   } --&gt;\n\n&lt;!--   return d3.drag() --&gt;\n&lt;!--       .on(\"start\", dragstarted) --&gt;\n&lt;!--       .on(\"drag\", dragged) --&gt;\n&lt;!--       .on(\"end\", dragended); --&gt;\n&lt;!-- } --&gt;\n\n\ndrag_pivot = {\n\n  function dragstarted(event, d) {\n    d3.select(this).raise().attr(\"stroke\", \"red\");\n  }\n\n  function dragged(event, d) {\n    d3.select(this).attr(\"cx\", event.x)\n  }\n\n  function dragended(event, d) {\n    d3.select(this).attr(\"stroke\", null);\n  }\n\n  return d3.drag()\n      .on(\"start\", dragstarted)\n      .on(\"drag\", dragged)\n      .on(\"end\", dragended);\n}"
  },
  {
    "objectID": "ojs/unbiased-estimates.html",
    "href": "ojs/unbiased-estimates.html",
    "title": "(Un)Biased estimates",
    "section": "",
    "text": "Show: \\(M\\) \\(SD_{n}\\) \\(SD_{n - 1}\\)\n\n\n\n\n\n\n\n\n\n\n\njStat = require(\"https://cdn.jsdelivr.net/npm/jstat@latest/dist/jstat.min.js\")\n\n\n\n\n\n\n\nw = 800\nh = 400\n\nmaxWidth = 900;\nmaxHeight = 550;\n\ntimeseriesVertical = false;\n\npanelSpacing = 5;\n\npopulationPanelWidth = maxWidth * 0.6;\npopulationPanelHeight = maxHeight;\n\npopulationSubPanelProportion = 0.4;\nsampleSubPanelProportion = 0.05;\n\nestimatesSubPanelProportion = 0.55;\n\nestimatesPanelWidth = maxWidth - populationPanelWidth;\nestimatesPanelHeight = populationPanelHeight;\n\ntimeSeriesPanelWidth = maxWidth - populationPanelWidth;\n\n\n\nradius = 4; //1.4\nsampleSize = 5;\n\n    \nxScalePopulation = d3.scaleLinear()\n    .domain([-4, 4])\n    .range([0 + radius, (populationPanelWidth) - radius])\nyScalePopulation = d3.scaleLinear()\n    .domain([0, 300])\n    .range([populationPanelHeight * populationSubPanelProportion, 0])\n\nxScaleEstimates = d3.scaleLinear()\n    .domain([-1.25, 1.25])\n    .range([0, estimatesPanelWidth])\n    \nyScaleEstimates = d3.scaleLinear()\n    .domain([20, 0])\n    .range([maxHeight - 30, maxHeight * (1 - estimatesSubPanelProportion) + 30])\n\nxAxisEstimatesValues = [-2, -1, 0, 1, 2];\nxAxisEstimates = d3.axisBottom(xScalePopulation)\n  .tickValues(xAxisEstimatesValues)\n  .tickFormat(d =&gt; d)\n  .tickSize(-maxHeight * (1 - estimatesSubPanelProportion) - 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nupdate_svg = {\n\n  var sample = [];\n  var sampleData = [];\n  var sample_estimates = [];\n  var running_averages = [{param: \"population\", value: [0], id: [0]},\n                          {param: \"sample\",     value: [0], id: [0]},\n                          {param: \"mean\",       value: [0], id: [0]}];\n  \n  var nSamplesDrawn = 0;\n  \n  var legendSelected = [\"sample\", \"population\"]\n  \n  var timeX, timeY, biasLine, timeXAxis, timeYAxis;\n  var timeSvg, \n  timeScaleBias, \n  timeScaleId, \n  timeBiasAxis, \n  timeIdAxis, \n  timeseriesDataLayer,\n  timeseriesBiasAxisLayer,\n  timeseriesIdAxisLayer;\n\n  let isLargeScreen;\n\n  function newSample() {\n    \n    nSamplesDrawn++\n    \n    // pick random observations from the population by their index\n    for (var i = 0; i &lt; sampleSize; i++) {\n      let randomIndex = Math.floor(Math.random() * popData.length);\n      sample[i] = xScalePopulation.invert(popData[randomIndex].cx);\n      sampleData[i] = popData[randomIndex];\n    }\n    \n    var estimates = getSampleEstimates(sample)\n    estimates.map(d =&gt; d.id = nSamplesDrawn);\n    sample_estimates.push(estimates)\n    \n    updateRunningAverages(estimates);\n    updateBiasChart();\n    updateSampleCircles();\n    animateEstimates(sampleData, estimates);\n    updatePath();\n    updateVisibility();\n  }\n  \n  function updateBiasChart() {\n      yScaleEstimates.domain([nSamplesDrawn-20, nSamplesDrawn])\n      \n      biasDots.selectAll(\"path\").remove()\n      biasDots.selectAll(\"path\")\n          .data(sample_estimates.flat())\n          .enter()\n          .append(\"path\")\n            .attr(\"id\", d =&gt; d.param + \"-estimate\")\n            .attr(\"d\", d3.symbol().type(d3.symbolSquare).size(radius * radius * 2))\n            .attr(\"transform\", d =&gt; `translate(${xScalePopulation(d.value)}, ${yScaleEstimates(d.id + 1)}) rotate(45)`)\n            .attr(\"opacity\", d =&gt; (d.id === nSamplesDrawn ? 0 : 1))\n            .transition()\n            .attr(\"transform\", d =&gt; `translate(${xScalePopulation(d.value)}, ${yScaleEstimates(d.id)}) rotate(45)`)\n            \n  }\n  \n\n  \n  function updateSampleCircles() {\n  \n  let durationMultiplier = 5;\n  if (playing) durationMultiplier = 1;\n  \n    sampleCircles.selectAll('circle').remove()\n    sampleCircles.selectAll('circle')\n      .data(sampleData)\n      .enter().append(\"circle\")\n      .attr(\"class\", \"sample\")\n      .attr(\"r\", radius)\n      .attr(\"cx\", d =&gt; d.cx)\n      .attr(\"cy\", d =&gt; yScalePopulation(d.cy))\n      .attr(\"fill\", d =&gt; d.fill)\n      .transition()\n      .duration(d =&gt; d.cy * durationMultiplier)\n      .ease(d3.easeBounceOut)\n      .attr(\"cy\", populationPanelHeight * (populationSubPanelProportion + sampleSubPanelProportion) - radius)\n  }\n  \n  \n  \n  function animateEstimates(sampleData, estimates) {\n  \n    sampleEstimatesTemp.selectAll(\"path\").remove()\n    \n    var wait = Math.max(...sampleData.map(z =&gt; z.cy));\n    \n    for (let i = 0; i &lt; estimates.length; i++) {\n\n    var p = estimates[i].param;\n    var endPosition = estimates[i].value;\n    var dur = (playing ? 0 : 1000);\n    var convergeWait = (playing ? 0 : wait * 5);\n    var moveDownWait = (playing ? 250 : 0);\n    \n    sampleData.forEach((s) =&gt; {\n    \n    // first, place estimate symbols where each sample dot lands\n      sampleEstimatesTemp\n      .append(\"path\")\n      .attr(\"id\", p + \"-estimate\")\n      .attr(\"d\", d3.symbol().type(d3.symbolSquare).size(radius * radius * 2))\n        .attr(\"transform\", d =&gt; `translate(${s.cx}, ${populationPanelHeight * (populationSubPanelProportion + sampleSubPanelProportion) - radius}) rotate(45)`)\n        .attr(\"opacity\", 0)\n        \n    // then move them all to the estimate\n        .transition().duration(dur * 0.67).delay(convergeWait)\n        .attr(\"opacity\", 1)\n        .attr(\"transform\", d =&gt; `translate(${xScalePopulation(endPosition)}, ${populationPanelHeight * (populationSubPanelProportion + sampleSubPanelProportion) - radius}) rotate(45)`)\n        \n    // then move them down to the estimates tracker\n    .transition().duration((playing ? 250 : (dur * 0.33))).delay(0)\n      .ease(d3.easeCubicOut)\n        .attr(\"opacity\", 1)\n        .attr(\"transform\", `translate(${xScalePopulation(endPosition)}, ${yScaleEstimates(nSamplesDrawn)}) rotate(45)`)\n    })\n\n    }\n  }\n  \n  const sleep = (milliseconds) =&gt; {\n    return new Promise(resolve =&gt; setTimeout(resolve, milliseconds))\n  }\n  var playing = false;\n  function playButtonClicked() {\n    \n    playing = !playing; \n  \n  play_button.text(function(){\n    if(playing) {\n      return \"◼\"\n  } else {\n    return \"▶\"\n  }\n  })\n  \n  if (playing) {\n    continuouslyDrawSamples();\n  }\n  }\n  \n  function continuouslyDrawSamples() {\n    if (playing) {\n      newSample();\n      sleep(200).then(continuouslyDrawSamples);\n    }\n  }\n  \n    \n  var popData = [];\n  const color = d3.scaleOrdinal(d3.schemeCategory10);\n  for (let i = 0; i &lt; population.length; ++i) {\n    const cx = xScalePopulation(population[i]);\n    const cy = 10 + (dodge(cx) - radius - 1);\n    &lt;!-- const cy = yScalePopulation(dodge(cx)); --&gt;\n    const fill = color(i % 10);\n    popData.push({cx, cy, fill})\n  }\n  \n  \n  \n  const populationLabels = [{label: \"Population\", top: 0},\n                            {label: \"Sample\",     top: (panelSpacing /  maxHeight + populationSubPanelProportion) * 100},\n                            {label: \"Under/over-&lt;/br&gt;estimation of&lt;/br&gt;parameter\",     top: (panelSpacing /  maxHeight + populationSubPanelProportion + sampleSubPanelProportion) * 100}]\n  \n  const populationContainer = d3.select(\"#population-container\")\n    .style(\"position\", \"relative\")\n    &lt;!-- .style(\"height\", maxHeight) --&gt;\n    \n    // panel labels\n  populationContainer.selectAll(\"text\").data(populationLabels).enter()\n    .append(\"text\")\n    .style(\"position\", \"absolute\")\n    .html(d =&gt; d.label)\n    .style(\"top\", d =&gt; d.top + \"%\")\n    .style(\"line-height\", \"1em\")\n\n  const populationAndSampleSvg = d3.select(\"#population-container\")\n    .append(\"svg\").attr(\"id\", \"populationAndSample-svg\")\n    .attr(\"preserveAspectRatio\", \"xMinYMin meet\")\n    .attr(\"viewBox\", \"0 0 \" + (populationPanelWidth) + \" \" + populationPanelHeight)\n    \n    // panel backgrounds\n  populationAndSampleSvg.append(\"rect\")\n    .attr(\"width\", populationPanelWidth)\n    .attr(\"height\", populationPanelHeight * populationSubPanelProportion)\n    .attr(\"fill\", \"var(--population-panel-background)\")\n    .attr(\"rx\", 5)\n  populationAndSampleSvg.append(\"rect\")\n    .attr(\"width\", populationPanelWidth)\n    .attr(\"y\", panelSpacing + populationPanelHeight * populationSubPanelProportion)\n    .attr(\"height\", populationPanelHeight * sampleSubPanelProportion - panelSpacing)\n    .attr(\"fill\", \"var(--sample-panel-background)\")\n    .attr(\"rx\", 5)\n  populationAndSampleSvg.append(\"rect\")\n    .attr(\"width\", populationPanelWidth)\n    .attr(\"y\", panelSpacing + populationPanelHeight * (1 - estimatesSubPanelProportion))\n    .attr(\"height\", populationPanelHeight * estimatesSubPanelProportion - panelSpacing)\n    .attr(\"fill\", \"var(--estimates-panel-background)\")\n    .attr(\"rx\", 5)\n    \n\n    \n  const pop = populationAndSampleSvg.append(\"g\")\n  const parameters = populationAndSampleSvg.append(\"g\")\n  const sampleEstimates = populationAndSampleSvg.append(\"g\")\n  const sampleEstimatesTemp = populationAndSampleSvg.append(\"g\")\n  const sampleCircles = populationAndSampleSvg.append(\"g\")\n\n  const biasDots  = sampleEstimates.append(\"g\")\n  \n  pop.selectAll(\"circle\")\n      .data(popData)\n      .enter()\n      .append(\"circle\")\n        .attr(\"class\", \"pop\")\n        .attr(\"cx\", d =&gt; d.cx)\n        .attr(\"cy\", d =&gt; yScalePopulation(d.cy))\n        .attr(\"r\", radius)\n        .attr(\"fill\", d =&gt; d.fill)\n        \n    // estimates axis\n    \n  const estimatesAxis = sampleEstimates.append(\"g\")\n    .attr(\"transform\", `translate(0, ${yScaleEstimates(21)})`)\n  \n  estimatesAxis.append(\"rect\")\n    .attr(\"width\", populationPanelWidth)\n    .attr(\"height\", populationPanelHeight - yScaleEstimates(21))\n    .attr(\"fill\", \"var(--estimates-panel-background)\")\n    .attr(\"rx\", 5)\n  estimatesAxis.call(xAxisEstimates)\n  estimatesAxis.select(\".domain\").remove()\n  \n\n\n  \n  formatAxes(estimatesAxis.selectAll(\"line\"));\n  \n    \n  var legendStatus = [{param: \"mean\",       hide: true},\n                      {param: \"population\", hide: false},\n                      {param: \"sample\",     hide: false}]\n                      \n  \n  function updateLegendStatus(param) {\n    var index;\n    if (param===\"mean\") {index = 0;}\n    if (param===\"population\") {index = 1;}\n    if (param===\"sample\") {index = 2;}\n    legendStatus[index].hide = !legendStatus[index].hide\n\n    var classes = \"#\" + param + \"-estimate\"\n    \n    populationAndSampleSvg.selectAll(classes).classed(\"hide\", legendStatus[index].hide)\n    timeSvg.selectAll(\"#\" + param + \"-path\").classed(\"hide\", legendStatus[index].hide)\n    legend.classed(\"unselected\", d =&gt; d.hide)\n  }\n  \n    const legend = d3.selectAll(\".selector\")\n    legend\n      .data(legendStatus)\n      .classed(\"unselected\", d =&gt; d.hide)\n      .on(\"click\", function(event, d){updateLegendStatus(d.param);})\n  \n\n\n  \n  // buttons\n  const controls = d3.select(\"#controls-container\")\n  \n  const reset_button = controls.append(\"button\")\n    // .attr(\"class\", \"button invertable\")\n    // .attr(\"type\", \"button\")\n    .text(\"Reset\")\n    .on(\"click\", clearData)\n  \n  const button = controls.append(\"button\")\n    .text(\"Take one sample\")\n    .on(\"click\", newSample)\n    \n  const play_button = controls.append(\"button\")\n    .attr(\"id\", \"play-button\")\n    // .attr(\"class\", \"button invertable\")\n    .attr(\"x\", 50)\n    .attr(\"y\", h - 50)\n    .html(\"►\")\n    .on(\"click\", playButtonClicked)\n    \n\n\n// make timeseries chart\nconst timeChart = {\n    width: timeSeriesPanelWidth,\n    height: maxHeight,\n    margin: {left: 30, right: 30, top: 50, bottom: 60}\n}\n\nconst timeChartHorizontal = {\n    width: populationPanelWidth,\n    height: 300,\n    margin: {left: 30, right: 30, top: 50, bottom: 60}\n}\n\n  const timeseriesContainer = d3.select(\"#timeline-container\");\n  timeseriesContainer.style(\"position\", \"relative\")\n\n  updateTimeseriesDimensions(window.innerWidth);\n\n  // Re-render the chart whenever the window size changes\n  window.addEventListener(\"resize\", () =&gt; updateTimeseriesDimensions(window.innerWidth)); \n\n\n\n  function updateTimeseriesDimensions(winWidth) {\n\n    const largeScreen = winWidth &gt; 600;\n    if (largeScreen === isLargeScreen) return;\n    // Update the screen state\n    isLargeScreen = largeScreen;\n    \n  var params;\n  var orientation = (winWidth &gt; 600) ? \"vertical\" : \"horizontal;\"\n\n  console.log(orientation);\n\n  // first, set up chart dimensions and axes\n  if (winWidth &gt; 600) {\n\n    params = timeChart;\n\n    timeScaleBias = d3.scaleLinear()\n      .domain([-0.5, 0.5])\n      .range([params.margin.left, params.width - params.margin.right])\n    timeScaleId = d3.scaleLinear()\n      .domain([0, 200])\n      .range([params.margin.top, params.height - params.margin.bottom])\n    biasLine = function(x, y){\n        return d3.line()\n        .x(function(d,i) { return timeScaleBias(x[i]); })\n        .y(function(d,i) { return timeScaleId(y[i]); })\n        (Array(x.length));\n    }\n    timeBiasAxis = d3.axisTop(timeScaleBias)\n      .ticks(5)\n      .tickSize(-(params.height - params.margin.top - params.margin.bottom)) // 440\n    timeIdAxis = d3.axisRight(timeScaleId).tickSize(0)\n  } else {\n\n      params = timeChartHorizontal;\n\n      timeScaleBias = d3.scaleLinear()\n        .domain([-0.5, 0.5])\n        .range([params.height - params.margin.bottom, params.margin.top])\n        \n      timeScaleId = d3.scaleLinear()\n        .domain([0, 200])\n        .range([params.margin.left, params.width - params.margin.right])\n      biasLine = function(x, y){\n          return d3.line()\n          .x(function(d,i) { return timeScaleId(y[i]); })\n          .y(function(d,i) { return timeScaleBias(x[i]); })\n          (Array(x.length));\n      }\n    timeIdAxis = d3.axisBottom(timeScaleId)\n      .tickSize(0)\n    timeBiasAxis = d3.axisLeft(timeScaleBias)\n      .ticks(5).tickSize(-params.width - params.margin.left - params.margin.right)\n  }\n\n  // then instantiate the chart svg itself\n  d3.select(\"#timeline-container\").select(\"svg\").remove();\n  d3.select(\"#timeline-container\").selectAll(\"text\").remove();\n  timeSvg = makeTimeseriesChart(params);\n  timeseriesDataLayer = timeSvg.append(\"g\");\n  timeseriesBiasAxisLayer = timeSvg.append(\"g\");\n  timeseriesIdAxisLayer = timeSvg.append(\"g\");\n\n  // then place the axes\n  positionTimeAxes(orientation, params);\n\n  // then draw the current data\n  updatePath();\n}\n\n  \n\n  \n  function makeTimeseriesChart(params) {\n    \n    // text labels\n    timeseriesContainer.append(\"text\")\n    .style(\"position\", \"absolute\")\n    .style(\"left\", 0)\n    .text(\"Average under/over-estimation\")\n  \n  timeseriesContainer.append(\"text\")\n    .style(\"position\", \"absolute\")\n    .style(\"line-height\", \"1em\")\n    .style(\"bottom\", 0)\n    .style(\"right\", 0)\n    .style(\"text-align\", \"right\")\n    .html(\"Total&lt;br&gt;samples\")\n    \n    const svg = d3.select(\"#timeline-container\")\n    .append(\"svg\").attr(\"id\", \"timeline-svg\")\n    .attr(\"preserveAspectRatio\", \"xMinYMin meet\")\n    .attr(\"viewBox\", \"0 0 \" + params.width + \" \" + params.height)\n    \n    // background panel\n  svg.append(\"rect\")\n    .attr(\"width\", params.width)\n    .attr(\"height\", params.height)\n    .attr(\"rx\", 5)\n    .attr(\"fill\", \"var(--timeseries-panel-background)\")\n    \n    return svg;\n    \n  }\n  \n  \n  function positionTimeAxes(orientation, params) {\n    if (orientation === \"vertical\") {\n      timeseriesBiasAxisLayer.attr(\"transform\", `translate(0, ${params.margin.top})`)\n\n      timeseriesBiasAxisLayer.append(\"rect\")\n        .attr(\"width\", params.width)\n        .attr(\"y\", -params.margin.top)\n        .attr(\"height\", params.margin.top)\n        .attr(\"rx\", 5)\n        .attr(\"fill\", \"var(--timeseries-panel-background)\")\n\n        timeseriesIdAxisLayer.attr(\"transform\", `translate(${params.width - params.margin.right}, 0)`)\n  \n    } else {\n      timeseriesIdAxisLayer.attr(\"transform\", `translate(0, ${params.height - params.margin.bottom})`)\n\n      // todo: this isn't positioned correctly\n      timeseriesBiasAxisLayer.append(\"rect\")\n        .attr(\"x\", -params.margin.left)\n        .attr(\"width\", params.margin.left)\n        .attr(\"height\", params.height)\n        .attr(\"rx\", 5)\n        .attr(\"fill\", \"var(--timeseries-panel-background)\")\n      \n      timeseriesBiasAxisLayer.attr(\"transform\", `translate(${params.margin.left}, 0)`)\n        \n      \n    }\n    timeseriesIdAxisLayer.call(timeIdAxis)\n    timeseriesBiasAxisLayer.call(timeBiasAxis);\n    timeseriesBiasAxisLayer.select(\".domain\").remove();\n    timeseriesIdAxisLayer.select(\".domain\").remove();\n    \n    formatAxes(timeSvg.selectAll(\"line\"));\n    \n    timeSvg.selectAll(\"line\").classed(\"dark\", true)\n  }\n  \n  \n  function updatePath() {\n      timeseriesDataLayer.selectAll(\"g\").remove()\n      \n      if (nSamplesDrawn &gt; 201) {\n        timeScaleId.domain([nSamplesDrawn - 200, nSamplesDrawn]);\n        // timeIdAxis = d3.axisRight(timeScaleId).tickSize(0);\n        timeseriesIdAxisLayer.call(timeIdAxis);\n        timeseriesIdAxisLayer.select(\".domain\").remove();\n      }\n      \n      timeseriesDataLayer.selectAll(\"g\")\n        .data(running_averages)\n        .enter()\n        .append(\"g\")\n        .attr(\"class\", \"bias-paths\")\n        .append(\"path\")\n          .attr(\"d\", d =&gt; biasLine(d.value.slice(1), d.id.slice(1)))\n          .attr(\"id\", d =&gt; d.param + \"-path\")\n\n  }\n  \n\n\nfunction clearData() {\n    sample = [];\n    sample_estimates = [];\n    running_averages = [{param: \"population\",   value: [0], id: [0]},\n                          {param: \"sample\",     value: [0], id: [0]},\n                          {param: \"mean\",       value: [0], id: [0]}];\n    nSamplesDrawn = 0;\n    \n    timeScaleId.domain([0, 200]);\n    timeseriesIdAxisLayer.call(timeIdAxis);\n    timeseriesIdAxisLayer.select(\".domain\").remove();\n    \n    sampleCircles.selectAll('circle').remove()\n    sampleEstimates.selectAll(\"path\").remove()\n    sampleEstimatesTemp.selectAll(\"path\").remove()\n    timeseriesDataLayer.selectAll(\"path\").remove()\n  }\n  \n  function updateRunningAverages(estimates) {\n  \n      var cur_n = nSamplesDrawn\n      var prev_n = cur_n - 1\n      \n      var old = running_averages[0].value[prev_n]\n      var new_pop = ((old * prev_n) + estimates[0].value)/cur_n\n      running_averages[0].value.push(new_pop)\n      \n      var old = running_averages[1].value[prev_n]\n      var new_sam = ((old * prev_n) + estimates[1].value)/cur_n\n      running_averages[1].value.push(new_sam)\n      \n      var old = running_averages[2].value[prev_n]\n      var new_mea = ((old * prev_n) + estimates[2].value)/cur_n\n      running_averages[2].value.push(new_mea)\n    \n      running_averages[0].id.push(cur_n)\n      running_averages[1].id.push(cur_n)\n      running_averages[2].id.push(cur_n)\n\n}\n\n  function updateVisibility() {\n  \n    var params = [\"mean\", \"population\", \"sample\"]\n    \n    for (var i = 0; i &lt; 3; i++) {\n      var param = params[i]\n      var elementIds = \"#\" + param + \"-estimate, #\" + param + \"-line\"\n      \n      populationAndSampleSvg.selectAll(elementIds).classed(\"hide\", legendStatus[i].hide)\n      &lt;!-- populationAndSampleSvg.selectAll(\"#\" + param + \"-estimate\").classed(\"hide\", legendStatus[i].hide) --&gt;\n      timeSvg.selectAll(\"#\" + param + \"-path\").classed(\"hide\", legendStatus[i].hide)\n    }\n  }\n  \n}\n\n\n\n\n\n\n\ndodger = radius =&gt; {\n  const radius2 = radius ** 1.9;\n  const bisect = d3.bisector(d =&gt; d.x);\n  const circles = [];\n  return x =&gt; {\n    const l = bisect.left(circles, x - radius);\n    const r = bisect.right(circles, x + radius, l);\n    let y = 0;\n    for (let i = l; i &lt; r; ++i) {\n      const { x: xi, y: yi } = circles[i];\n      const x2 = (xi - x) ** 2;\n      const y2 = (yi - y) ** 2;\n      if (radius2 &gt; x2 + y2) {\n        y = yi + Math.sqrt(radius2 - x2) + 1e-6;\n        i = l - 1;\n        continue;\n      }\n    }\n    circles.splice(bisect.left(circles, x, l, r), 0, { x, y });\n    &lt;!-- populationPanelHeight * 0.7 - d.cy + (radius * 2) --&gt;\n    return y;\n  };\n}\n\ndodge = dodger(radius * 2 + 0.75);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction mean(array) {\n    return array.reduce((a, b) =&gt; a + b) / array.length;\n}\n\nfunction sample_variance(array) {\n    const n = array.length\n    const m = mean(array)\n    return array.map(x =&gt; Math.pow(x - m, 2)).reduce((a, b) =&gt; a + b) / (n - 1);\n}\n\nfunction population_variance(array) {\n    const n = array.length\n    const m = mean(array)\n    return array.map(x =&gt; Math.pow(x - m, 2)).reduce((a, b) =&gt; a + b) / n;\n}\n\nfunction get_descriptives (array) {\n    return {mean: mean(array),\n            sample_variance: sample_variance(array) - 1, \n            population_variance: population_variance(array) - 1}\n}\n\nfunction getNewData (array) {\n    \n    return {sample_estimates: getSampleEstimates(array)\n            &lt;!-- running_averages: getRunningAverages(array) --&gt;\n            }\n}\n\nfunction getSampleEstimates(array) {\n    return [{param: \"population\", value: population_variance(array) - 1},\n            {param: \"sample\",     value: sample_variance(array) - 1},\n            {param: \"mean\",       value: mean(array)}]\n}\n\n\nfunction formatAxes(elements) {\n  elements._groups[0].forEach((l) =&gt; {\n    if (l.__data__ === 0) {\n      l.classList = \"axis-major\";\n    } else {\n      l.classList = \"axis-minor\";\n    }\n  })\n}"
  },
  {
    "objectID": "ojs/galton-board.html",
    "href": "ojs/galton-board.html",
    "title": "Galton Board",
    "section": "",
    "text": "Matter = require(\"https://cdnjs.cloudflare.com/ajax/libs/matter-js/0.19.0/matter.min.js\")\n\njStat = require(\"../js/jstat.js\")\n\n\nboard = {\n\n\n// https://codesandbox.io/s/github/rjoxford/MatterJSGaltonBoard\n// https://www.tylermw.com/plinko-statistics-insights-from-the-bean-machine/\n\n\nlet width = 700;\nlet height = 600;\nlet x0 = width / 2;\n\n// ball properties\nconst ballRadius = 4;\nlet y_start = 0;\n\nlet generation_speed = 20;\nlet nBalls = 650;\nlet mass = 100;\nlet density = 1;\n\n// peg board properties\nlet rows = 20;\nlet y_peg_start = 20;\nlet pegGap = 6.5 * ballRadius;\nlet pegRadius = 0.5 * ballRadius;\nlet xGap = pegGap;\nlet yGap = 0.6 * xGap;\nlet pegAngle = 0; // Math.PI / 4;\nlet gap_between_pegs_and_buckets = 0;\n\n// funnel properties\nconst funnelTostartGap = yGap;\nconst funnelWallLength = 600;\nconst funnelAngle = Math.PI / 3;\nconst funnelOpening = 5 * ballRadius;\n\n// physics properties\nlet restitution = 0.5; // bounciness\nlet friction = 0.01;\nlet frictionAir = 0.045;\nlet frictionStatic = 0;\n\n\nlet intervalId;\n\n\nvar {Engine, Render, Runner, \n    Composite, Composites, Common, \n    MouseConstraint, Mouse, Events, \n    World, Bodies, Body} = Matter;\n\nlet engine, render, runner, world;\n\n\n\nfunction initialize() {\n    // create engine\n    engine = Engine.create({\n        enableSleeping: true\n    }),\n        world = engine.world;\n    \n    // create renderer\n    render = Render.create({\n        element: document.getElementById(\"board\"),\n        engine: engine,\n        options: {\n            width: width,\n            height: height,\n            background: \"#ffffff\",\n            wireframes: false,\n            showSleeping: false\n        }\n    });\n    Render.run(render);\n\n    // engine.gravity.y = 1;\n    // engine.timing.timeScale = 1;\n    \n    // create runner\n    runner = Runner.create();\n    Runner.run(runner, engine);\n    render.canvas.addEventListener(\"mousedown\", reset);\n    render.canvas.position = \"absolute\";\n}\n\n\n\n// Create top funnel\nlet leftBumper_x =  x0 - (funnelWallLength * Math.cos(funnelAngle) + funnelOpening) / 2;\nlet rightBumper_x = x0 + (funnelWallLength * Math.cos(funnelAngle) + funnelOpening) / 2;\nlet bumper_y = y_peg_start - ((funnelWallLength * Math.sin(funnelAngle)) / 2 - funnelTostartGap);\nconsole.log(bumper_y)\n\nlet createFunnel = () =&gt; {\n\n        let leftBumper = Bodies.rectangle(leftBumper_x, bumper_y, funnelWallLength, 3, {\n            restitution,\n            friction: 0,\n            frictionStatic: 0,\n            isStatic: true\n        });\n        Matter.Body.rotate(leftBumper, funnelAngle);\n\n        let rightBumper = Bodies.rectangle(rightBumper_x, bumper_y, funnelWallLength, 3, {\n            restitution: 0.6,\n            friction: 0,\n            frictionStatic: 0,\n            isStatic: true\n        });\n        Matter.Body.rotate(rightBumper, -funnelAngle);\n\n        Matter.Composite.add(world, [leftBumper, rightBumper]);\n}\n\n\nfunction make_balls() {\n\n    let total = nBalls;\n    clearInterval(intervalId);\n\n    intervalId = setInterval(() =&gt; {\n        let balls = [];\n        if (total-- &gt; 0) {\n            const circle = Bodies.circle(x0 + (-0.5 + Math.random()) * 1, -20, ballRadius, {\n                label: \"circle\",\n                friction: 0.001,\n                restitution,\n                mass,\n                slop: 0.05,\n                density,\n                frictionAir,\n                sleepThreshold: Infinity,\n                render: {\n                    fillStyle: d3.schemeCategory10[total % 10]\n                }\n            });\n            // Matter.Events.on(circle, \"sleepStart\", () =&gt; {\n            //     Matter.Body.setStatic(circle, true);\n            // });\n            \n            Matter.Composite.add(world, circle);\n        }\n    }, generation_speed);\n}\n\nlet existingBalls = () =&gt; {\n    return world.bodies.filter((body) =&gt; body.label === \"circle\");\n  };\n\nconst makeStaticInterval = setInterval(() =&gt; {\n    existingBalls().forEach(function(ball) {\n      let ballHeight = ball.position.y;\n      let ballSpeed = ball.speed;\n      let minHeight = 350; // height - (floorHeight + wallHeight);\n      if (ballHeight &gt; minHeight && ballSpeed &lt; 0.02) {\n        // ball.render.opacity = 0.5;\n        Body.setStatic(ball, true);\n      }\n    });\n  }, 200);\n\n\nfunction make_pegs() {\n    const pegs = [];\n    const spacingY = ballRadius*4;\n    const spacingX = ballRadius*4;\n    var i, j, lastI;\n    for (i = 0; i &lt; rows; i++) {\n        for (j = 1; j &lt; i; j++) {\n            pegs.push(\n                // Bodies.rectangle(\n                Bodies.circle(\n                    x0 + (j * xGap - i * (xGap / 2)),\n                    y_peg_start + i * yGap,\n                    pegRadius,\n                    // ballRadius * 1.2,\n                    // 2,\n                    {\n                        angle: pegAngle,\n                        isStatic: true,\n                        friction: 0,\n                        frictionStatic: 0,\n                        render: {\n                            fillStyle: \"#000000\"\n                        },\n                    chamfer: {\n                        radius: [ballRadius * 0.2, ballRadius * 0.2, 0, 0]\n                    }\n        })\n            );\n        }\n        lastI = i;\n    }\n    // bins\n    for (i = 0; i &lt; rows; i++) {\n        Matter.Composite.add(\n            world,\n            Bodies.rectangle(\n\n                x0 - (rows - 1) * (xGap / 2) + i * xGap,\n                y_peg_start + rows * yGap + gap_between_pegs_and_buckets + (height-(y_peg_start + rows * yGap))/2,\n                4,\n                (height-(y_peg_start + rows * yGap)),\n                {\n                    isStatic: true,\n                    density: 1000,\n                    mass: 1000,\n                    slop: 0,\n                    render: {\n                        fillStyle: \"#000000\",\n                        visible: true\n                    },\n                    chamfer: {\n                        radius: [ballRadius * 0.4, ballRadius * 0.4, 0, 0]\n                    }\n                }\n            )\n        );\n    }\n    // ground\n    Matter.Composite.add(\n        world,\n        Bodies.rectangle(400, height, 1000, 10, {\n            isStatic: true,\n            render: {\n                fillStyle: \"#000000\",\n                visible: true\n            }\n        })\n    );\n\n\n    World.add(world, pegs);\n}\n\nconst canvas = d3.select(\"#overlay\")\n.append(\"canvas\")\n.attr(\"id\", \"overlay\")\n.attr(\"position\", \"absolute\")\n.attr(\"width\", width)\n.attr(\"height\", height);\n\nconst ctx = canvas.node().getContext('2d');\ncanvas.on(\"mousedown\", reset);\n\nfunction drawNormalDistribution() {\n\n    ctx.strokeStyle = 'red';\n    ctx.lineWidth = 3;\n    ctx.beginPath();\n    ctx.moveTo(0, height - 5);\n\n    let yMultiplier = (height-(y_peg_start + rows * yGap));\n    var values = jStat(-4, 4, 210)[0]\n\n\n    for (var i in values) {\n        let value = values[i];\n        let density = jStat.normal.pdf(value, 0, 0.9);\n        ctx.lineTo((value + 4)*(width/8), height-(density*2.2*yMultiplier) - 5);\n        ctx.stroke();\n    }\n}\n\nfunction reset() {\n    Composite.clear(world);\n    Engine.clear(engine);\n    Render.stop(render);\n    Runner.stop(runner);\n    render.canvas.remove();\n    render.canvas = null;\n    render.context = null;\n    render.textures = {};\n    console.log('reset clicked');\n    \n    initialize();\n    make_pegs();\n    make_balls();\n    createFunnel();\n    drawNormalDistribution();\n}\n\n\n//\n\ninitialize();\nmake_pegs();\nmake_balls();\ncreateFunnel();\ndrawNormalDistribution();\n  \n}"
  },
  {
    "objectID": "ojs/confidence-interval.html",
    "href": "ojs/confidence-interval.html",
    "title": "Confidence Interval",
    "section": "",
    "text": "chart = { \n\n  d3.select(\"#ci-container\")\n    .call(addCIPlot, {test_type: \"single\",\n                     point_estimate: 322.59,\n                     standard_deviation: 45.31,\n                     n: 23,\n                     ci: 95,\n                     disable_controls: false})\n                     \n}\n\n\n\n\n\n\n\naddCIPlot = function(selection, params) {\n  selection\n    .call(ciParams, params)\n    .call(ciPlot, params)\n}\n\n\n\n\n\n\n\nciParams = function(selection, params) {\n    \n  selection.append(\"div\")\n  .attr(\"id\", \"ci-controls\")\n    .call(inputDropdown, params)\n    .call(slider, {id: 'ci', label: 'CI: ', value: params.ci, hide_slider: params.disable_controls})\n    .call(textInput, {id: 'point-estimate', label: 'Point estimate: ', value: params.point_estimate, disabled: params.disable_controls})\n    .call(textInput, {id: 'standard-deviation', label: 'Standard deviation: ', value: params.standard_deviation})\n    .call(stdErrSpan)\n    .call(slider, {id: 'n', label: 'n = ', value: params.n, hide_slider: params.disable_controls})\n  \n  d3.select(\"#test\").property(\"value\", params.test_type)\n  d3.selectAll(\"#input-point-estimate, #input-standard-deviation\").property(\"disabled\", params.disable_controls)\n     \n}\n\n\n\n\n\n\n\nciPlot = function(selection, params) {\n\n  const w = 1050, h = 600;\n  const margin = {bottom: 30};\n  const f = d3.format(\".2f\");\n  \n  var point_estimate = params.point_estimate\n  var sd = params.standard_deviation\n  var confidence = params.ci\n  var n = params.n\n  var df = computeDf(n, params.test_type)\n  var std_err = computeStdErr(sd, n, params.test_type)\n  var xlim\n  var ci_x_lim\n  d3.select(\"#value-standard-error\").text(f(std_err))\n  \n  const x = d3.scaleLinear()\n    .range([0, w])\n  const t = d3.scaleLinear()\n    .range([0, w])\n  const y = d3.scaleLinear()\n    .domain([0, 0.43])\n    .range([h - margin.bottom, 0])\n  const line = d3.line()\n    .x(d =&gt; t(d.value))\n    .y(d =&gt; y(d.density));\n  const xAxis = d3.axisBottom(x);\n  const tAxis = d3.axisBottom(t).tickSize(10);\n  \n  function makeCurve(limits) {\n    var arr = [];\n    var x = jStat(limits[0], limits[1], 210)[0];\n    for (var i = 0; i &lt; x.length; i++) {\n      arr.push({value: x[i], density: jStat.studentt.pdf(x[i], df)})\n    }\n    return arr\n  }\n  \n\n  const svg = selection.append(\"svg\")\n    .attr(\"viewBox\", \"0 0 \" + w + \" \" + h)\n    .attr(\"preserveAspectRatio\", \"xMinYMin meet\")\n    &lt;!-- .attr(\"width\", w).attr(\"height\", h) --&gt;\n    \n  const axis = svg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(0, ${y(0)})`)\n    &lt;!-- .style(\"color\", \"steelblue\") --&gt;\n    &lt;!-- .call(xAxis); --&gt;\n    \n  const axis_t = svg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", `translate(0, ${y(0)})`)\n    &lt;!-- .style(\"font-size\", \"1em\"); --&gt;\n    \n  const randomId = Math.floor(Math.random() * 10000);\n  const defs = svg.append(\"defs\")\n  const mask = defs.append(\"mask\").attr(\"id\", \"mask-\" + randomId)\n  const mask_rect = mask.append(\"rect\")\n  .attr(\"height\", h)\n  .style(\"fill\", \"white\")\n  \n  \n  const ci_fill = svg.append(\"path\")\n    .attr(\"mask\", \"url(#mask-\" + randomId + \")\")\n    .style(\"stroke\", \"none\").style(\"fill\", \"lightblue\")\n  \n  const ci_curve = svg.append(\"path\")\n    .attr(\"class\", \"invertable\")\n    .style(\"stroke\", \"black\")\n    .style(\"fill\", \"none\")\n    .style(\"stroke-width\", 4)\n  \n  const ci_line = svg.append(\"line\")\n    .style(\"stroke\", \"black\")\n    .style(\"stroke-dasharray\", [5,5])\n  const ci_point_estimate = svg.append(\"line\")\n    .style(\"stroke\", \"black\")\n    .style(\"stroke-dasharray\", [5,5])\n  const ci_point_estimate_dot = svg.append(\"circle\")\n    .style(\"stroke\", \"none\")\n    .style(\"fill\", \"black\")\n    .attr(\"cx\", w/2)\n    .attr(\"cy\", y(0))\n    .attr(\"r\", 5)\n    \n  const ci_limit_labels = svg.append(\"g\")\n  const ci_limit_lower = ci_limit_labels.append(\"text\")\n  const ci_limit_upper = ci_limit_labels.append(\"text\").style(\"text-anchor\", \"end\")\n  \n  const inputCI =  d3.select(\"#input-ci\")\n  const inputN =  d3.select(\"#input-n\")\n  const inputPoint =  d3.select(\"#input-point-estimate\")\n  const inputSd =  d3.select(\"#input-standard-deviation\")\n  const inputTest =  d3.select(\"#test\")\n  \n  function updatePlot() {\n    var test = inputTest.property(\"value\");\n    point_estimate = Number(inputPoint.property(\"value\"));\n    sd = Number(inputSd.property(\"value\"));\n    n = Number(inputN.property(\"value\"));\n    std_err = computeStdErr(sd, n, test);\n    df = computeDf(n, test);\n    confidence = Number(inputCI.property(\"value\"));\n    updateSliderTextValues();\n    updateCI();\n  }\n  \n  function updateSliderTextValues() {\n    d3.select(\"#value-standard-error\").text(f(std_err));\n    d3.select(\"#value-ci\").text(confidence);\n    d3.select(\"#value-n\").text(n);\n  }\n  \n  inputCI.on(\"input\", updatePlot)\n  inputN.on(\"input\", updatePlot)\n  inputPoint.on(\"input\", updatePlot)\n  inputSd.on(\"input\", updatePlot)\n  inputTest.on(\"change\", updatePlot)\n  \n  function computeDf(n, test) {\n    if (test==\"independent\") return n - 2;\n    else return n - 1;\n  }\n  \n  function computeStdErr(sd, n, test) {\n    var variance = sd * sd;\n    console.log(\"var = \" + variance)\n    if (test==\"independent\") return Math.sqrt(variance/(n/2) + variance/(n/2));\n    else return sd / Math.sqrt(n);\n  }\n  \n  function updateCI() {\n\n    var x_lim = [point_estimate - 3 * sd, point_estimate + 3 * sd];\n    var t_lim = [x_to_t(x_lim[0], point_estimate, std_err),\n                 x_to_t(x_lim[1], point_estimate, std_err)]\n    x.domain(x_lim);\n    axis.call(xAxis);\n    \n    t.domain(t_lim)\n    &lt;!-- axis_t.call(tAxis); --&gt;\n\n    var critical_t = [jStat.studentt.inv((1 - (confidence/100)) / 2, df),\n                      jStat.studentt.inv(1-(1 - (confidence/100)) / 2, df)]\n    var critical_x = [t_to_x(critical_t[0], point_estimate, std_err),\n                      t_to_x(critical_t[1], point_estimate, std_err)]\n    var line_height = jStat.studentt.pdf(critical_t[0], df) / 2\n\n    ci_limit_lower\n      .text(f(critical_x[0]))\n      .attr(\"x\", x(critical_x[0]))\n      .attr(\"y\", y(line_height))\n    ci_limit_upper\n      .text(f(critical_x[1]))\n      .attr(\"x\", x(critical_x[1]))\n      .attr(\"y\", y(line_height))\n\n    ci_fill.attr(\"d\", line(makeCurve([-15, 15])))\n    ci_curve.attr(\"d\", line(makeCurve(t_lim)))\n\n    mask_rect\n      .attr(\"x\", t(critical_t[0]))\n      .attr(\"width\", t(critical_t[1]) - t(critical_t[0]));\n\n    ci_line\n      .attr(\"transform\", `translate(0, ${y(line_height)})`)\n      .attr(\"x1\", t(critical_t[0])).attr(\"x2\", t(critical_t[1]))\n    ci_point_estimate\n      .attr(\"transform\", `translate(${t(0)}, 0)`)\n      .attr(\"y1\", y(jStat.studentt.pdf(0, df))).attr(\"y2\", y(0))\n  }\n  \n  updateCI();\n}\n\n\n\n\n\n\n\ninputDropdown = function(selection, params) {\n  selection.append(\"div\")\n    .attr(\"id\", \"test-dropdown\")\n    .classed(\"hide-element\", params.disable_controls)\n    .html(`\n  &lt;label for=\"test\"&gt;Test type:&lt;/label&gt;\n  &lt;select id=\"test\" name=\"test\"&gt;\n    &lt;option value=\"single\"&gt;Single-sample&lt;/option&gt;\n    &lt;option value=\"independent\" selected&gt;Independent-samples&lt;/option&gt;\n    &lt;option value=\"related\"&gt;Related-samples&lt;/option&gt;\n  &lt;/select&gt;\n  `)\n}\n\ntextInput = function(selection, params) {\n  selection.append(\"div\").html(`\n  \n  &lt;label for=\"input-${params.id}\" class = \"text-input-label\" style=\"\"&gt;${params.label}\n&lt;input type=\"text\" id=\"input-${params.id}\" class=\"text-input-box math\" name=\"${params.id}\" value=\"${params.value}\" style=\"width: 3em;\"&gt;\n&lt;/label&gt;`)\n}\n\nslider = function(selection, params) {\n\n  if (params.hide_slider) var visibility = \"hidden\"\n  else var visbility = \"visible\"\n  \n  selection.append(\"div\").html(`\n  \n  &lt;label for=\"input-${params.id}\" style=\"font-family: KaTeX_Main; font-size: 1em; font-style: italic; height: 1em;\"&gt;${params.label}&lt;/label&gt;\n&lt;span id=\"value-${params.id}\" class=\"math\" style=\"display: inline-block; width: 1em;\"&gt;${params.value}&lt;/span&gt;\n&lt;input data-prevent-swipe type=\"range\" id=\"input-${params.id}\" name=\"scale\" value=\"${params.value}\" min=\"2\" max=\"100\" step=\"1\" style=\"border: none; height: 1em; font-family: KaTeX_Main; font-size: 1em; margin-left: 0.5em; width: 50%; vertical-align: unset; visibility: ${visibility};\"&gt;\n  `)\n}\n\nstdErrSpan = function(selection, params) {\n  selection.append(\"div\").html(`\n    Standard error: &lt;span id=\"value-standard-error\"&gt;&lt;/span&gt;\n  `)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt_to_x = function(t, mu, std_err) {\n  return mu + t * std_err;\n}\n\nx_to_t = function(x, mu, std_err) {\n  return (x - mu) / std_err;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njStat = require(\"../js/jstat.js\")"
  },
  {
    "objectID": "ojs/nhst-diagram.html",
    "href": "ojs/nhst-diagram.html",
    "title": "NHST",
    "section": "",
    "text": "Treatment\n\n\nOriginal population\n\\(\\mu, \\sigma\\)\n\n\nTreated population\nUnknown \\(\\mu, \\sigma\\)\n\n\nSample\n\n\nTreated sample\n\\(M, SD\\)"
  },
  {
    "objectID": "ojs/utils.html",
    "href": "ojs/utils.html",
    "title": "OJS Utilities",
    "section": "",
    "text": "nhstDiagram = function(selection) {\n return false\n}\n\n\n\n\n\n\n\n\n\n\nlikert = { \n\n  d3.selectAll(\"#likert-container\").text(\"\")\n  \n  const container = d3.selectAll(\"#likert-container\")\n    .style(\"border\", \"1px solid var(--text-color)\")\n    .style(\"border-radius\", \"10px\")\n    .style(\"padding\", \"1em\")\n  \n  \n  const itemData = [{name: \"1. A lot less than usual\"},\n                 {name: \"2. A little less than usual\"},\n                 {checked: \"checked\", name: \"3. About average\"},\n                 {name: \"4. A little more than usual\"},\n                 {name: \"5. A lot more than usual\"}]\n  \n  container.append(\"p\").text(\"What is your current level of happiness?\").style(\"font-weight\", \"bold\")\n  \n  const items = container.selectAll(\"span\")\n    .data(itemData)\n    .enter().append(\"span\")\n    .html(d =&gt; '&lt;label class=\"likert-label\"&gt;&lt;input class=\"likert-button\" type=\"radio\" name=\"flexRadioDefault\"' + d.checked + '&gt;' + d.name + '&lt;/label&gt;&lt;br&gt;')\n    \n    container.selectAll(\".likert-button\")\n      .style(\"margin-left\", \"1em\")\n      .style(\"margin-right\", \"0.5em\")\n      .style(\"width\", \"1em\")\n      .style(\"height\", \"1em\")\n      \n    items.style(\"line-height\", \"1.5em\")\n}"
  },
  {
    "objectID": "recitation.html",
    "href": "recitation.html",
    "title": "Recitation",
    "section": "",
    "text": "You will use posit.cloud to access problem sets as Quarto documents, using RStudio to write R code and answer questions. You’ll just need to sign up for a free account.\nI’ll make the problem sets available in due course."
  },
  {
    "objectID": "recitation.html#the-general-workflow",
    "href": "recitation.html#the-general-workflow",
    "title": "Recitation",
    "section": "The general workflow",
    "text": "The general workflow\nR is a programming language well-suited to interactive data exploration and analysis. It is widely used for psychological research, which is why we’ll use it as our tool for learning and applying the statistical techniques covered in the lectures.\nIt might seem daunting if you’ve have no experience with coding, but the basic idea is that you have some data, like you are familiar with from a regular Excel or Google Sheets spreadsheet, and you perform operations on your data using functions a lot like you would in Excel/Sheets. For example, you might compute an average in Sheets by typing =AVERAGE(A1:A10). In R you might type mean(my_data$column_a). The specifics of the function names are different, but the basic idea is the same.\n\nRStudio\nRStudio is the interface we’ll use to write and run R code and see its output. The basic interface has 4 panels, each with a few tabs:\n\n\nTop-left: Code editor / data viewer\n\nYou will edit your problem set document here\nYou can run code within code chunks by clicking on a line of code and pressing Ctrl/Cmd + Enter on your keyboard, or pressing the green “play” button at the top of the chunk\n\nBottom-left: R console\n\nYou can type code directly and run it by pressing enter.\nYou won’t be saving your code as a document like when you type in in the editor, so this is useful for testing something simple out\n\nTop-right: Environment\n\nAs you execute code you may be creating objects like sets of numbers of data.frames. Those objects will appear here.\nYou can click the name of some objects, like data.frames, and it will open a view of the data as a tab in the editor pane\n\nBottom-right: Files/folders, Plots, Viewer, help window\n\nYou can navigate the file tree, and when you “Render” your document you will see how it looks as a fully-formatted web page.\n\n\nOnce you start working on problem sets, the workflow is to open the relevant .qmd file from the Files page (e.g. problem-set-1.qmd, and begin by clicking the little green play button on the code chunk you’ll see at the top of the document. That will give you something like this:\n\n\n\n\n\nYou’ll have the editable problem set .qmd open on the left, and instructions in the Viewer pane on the right. That should make it easy to follow instructions and add your own code solutions and text on the left. When you are finished, the final step is to click “Render” at the top of the editor pane to produce a PDF version of your problem set which you can upload to Canvas. You can also render as you go to check your code is working and the PDF looks as you expect. To get the instructions back, but click play on that first code chunk again."
  },
  {
    "objectID": "recitation.html#fundamentals-of-r",
    "href": "recitation.html#fundamentals-of-r",
    "title": "Recitation",
    "section": "Fundamentals of R",
    "text": "Fundamentals of R\nThe problem sets include instructions and are designed to walk you through everything; no prior knowledge or experience is required. That said, here are some of the basics of working with R code to help you get started.\n\nAssignment\nR has a fancy assignment operator: &lt;-.1 You assign things to a name by typing something like:\n\nname &lt;- thing\n\nThe thing there might be a set of numbers, an entire dataset, or something else. Giving it a name allows to you perform subsequent operations more easily, and choosing appropriate names makes your code easier to understand.\n\noriginal_numbers &lt;- 1:10\noriginal_numbers\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ndoubled_numbers &lt;- original_numbers * 2\ndoubled_numbers\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\n\nFunctions\nAlmost everything happens inside functions.\n\nmean(original_numbers)\n\n[1] 5.5\n\nmean(doubled_numbers)\n\n[1] 11\n\n\nYou can also nest functions inside one another.\n\nsqrt(mean(original_numbers))\n\n[1] 2.345208\n\n\nA function generally has one or more “arguments”, to which you supply parameters. For example, the mean() function’s first argument is the set of numbers you want to compute the mean of; in the previous examples original_numbers and doubled_numbers were the parameters I supplied. You don’t necessarily have to type the name of the argument, but it can be helpful. The seq() function, for example, produces a sequence of numbers according to three arguments, from, to, and by.\n\nseq(from = 1, to = 10, by = 2)\n\n[1] 1 3 5 7 9\n\n\nWhen you don’t type the names of the arguments, R matches them by position, so this gives exactly the same output as the previous line of code:\n\nseq(1, 10, 2)\n\n[1] 1 3 5 7 9\n\n\nYou can get help with a function (to see what arguments it accepts, for example) by typing a question mark followed by the function name (without parentheses) in your console.\n\n?mean\n\nRunning the code will bring up the function’s help documentation in RStudio’s Help pane.\n\n\nPiping\nYou can string together different operations in a pipeline using the pipe operator: |&gt;.2 The result of each line of code gets “piped” into the function on the next line as its first argument. For example, below I take some data (named my_data) and perform a series of operations, first changing its shape using pivot_longer(), then creating summary statistics for the mean and standard deviation separately by a grouping-variable, then I pipe the summary statistics into ggplot() to create a graph with a geom_col() layer for the geometry.\n\nmy_data |&gt; \n  pivot_longer(everything(),\n               names_to = \"condition\",\n               values_to = \"score\") |&gt; \n  summarize(mean = mean(score),\n            .by = condition) |&gt; \n  ggplot(aes(x = condition, y = mean)) +\n  geom_col()\n\n\n\n\n\n\n\n\nEt voilà, we have a serviceable graph of group means!\nThere’s a lot going on there, and the specifics will become clearer as you work on the problem sets. But using the pipe operator this way can make for relatively readable code."
  },
  {
    "objectID": "recitation.html#footnotes",
    "href": "recitation.html#footnotes",
    "title": "Recitation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMost other coding languages tend to use a boring = for assignment. Sure it’s nice not having to type an extra character, but there’s a keyboard shortcut to quickly add an &lt;- in RStudio: Option/Alt + -. And philosophically, the &lt;- arrow conveys the inherent directionality of the assignment operation. The object is assigned to the name; the object and its name are not equal and so the = arguably gives a misleading impression of the two things being one and the same. (Also, to let you in on a secret, = also works for assignment in R.)↩︎\nIf you’re looking at R code from elsewhere (e.g. looking up help online) you may see a different pipe: %&gt;%. The |&gt; pipe, called the “native” pipe, was only included as a feature of base R relatively recently. Until then, the %&gt;% pipe was provided by an external package (called magrittr. Get it?). In practice the pipes work similarly, so you can often just replace %&gt;% with |&gt; and it’ll work fine, but it’s worth being aware of.↩︎"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#independent-samples",
    "href": "slides/14_related-samples-t-test.html#independent-samples",
    "title": "PSYC BC1101",
    "section": "Independent-samples",
    "text": "Independent-samples\n\n\n\n“Between-participants” design\nTwo treated samples containing different people\nIndividual differences contribute to variability"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#related-samples",
    "href": "slides/14_related-samples-t-test.html#related-samples",
    "title": "PSYC BC1101",
    "section": "Related-samples",
    "text": "Related-samples\n\n\n\nRepeated-measures\n\n“Within-participants” design\nTwo treatment conditions, but same individuals in both\nRecord two scores per individual (one per condition)\nIndividual differences cannot contribute to difference between groups"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#related-samples-1",
    "href": "slides/14_related-samples-t-test.html#related-samples-1",
    "title": "PSYC BC1101",
    "section": "Related-samples",
    "text": "Related-samples\n\n\n\nMatched-subjects\n\nTwo samples of different people; each individual in sample A is “matched” on relevant variables with an individual in sample B\nUses same statistical procedures as repeated-measures\nBut requires twice as many participants as within-p’s design"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#advantages-disadvantages",
    "href": "slides/14_related-samples-t-test.html#advantages-disadvantages",
    "title": "PSYC BC1101",
    "section": "Advantages & disadvantages",
    "text": "Advantages & disadvantages\n\nAdvantages of related-samples design\n\nRequires fewer subjects (not true of matched subjects)\nAble to study changes over time\nReduces or eliminates individual differences as a source of variability; therefore less variability in scores\n\nDisadvantages of repeated-measures design\n\nFactors besides treatment may cause subject’s score to change during the time between measurements\nParticipation in first treatment may influence score in the second treatment (order effects)\nCounterbalancing is a way to control time-related or order effects\nParticipants can drop out"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#logic",
    "href": "slides/14_related-samples-t-test.html#logic",
    "title": "PSYC BC1101",
    "section": "Logic",
    "text": "Logic\n\n\n\n\n\n\n\nSample A\nSample B\n\n\n\n\n54\n43\n\n\n67\n57\n\n\n38\n39\n\n\n46\n41\n\n\n42\n36\n\n\n\n\n\n\n\n\n\n\n\nSample A\nSample B\nD\n\n\n\n\n54\n43\n-11\n\n\n67\n57\n-10\n\n\n38\n39\n1\n\n\n46\n41\n-5\n\n\n42\n36\n-6"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#equations-1",
    "href": "slides/14_related-samples-t-test.html#equations-1",
    "title": "PSYC BC1101",
    "section": "Equations",
    "text": "Equations\n\n\\(t = \\dfrac{\\text{sample statistic} - \\text{population parameter}}{\\text{estimated standard error}}\\)\n\n\n\n\nSingle sample:\n\n\\(t = \\dfrac{M-\\mu}{s_M}\\)\n\n\n\nIndependent samples:\n\n\\(t = \\dfrac{(M_1-M_2)-(\\mu_1-\\mu_2)}{s_{M_1-M_2}}\\)\n\n\n\nRelated samples:\n\n\\(t = \\dfrac{M_D-\\mu_D}{s_{M_D}}\\)"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#calculating-related-samples-t",
    "href": "slides/14_related-samples-t-test.html#calculating-related-samples-t",
    "title": "PSYC BC1101",
    "section": "Calculating related-samples \\(t\\)",
    "text": "Calculating related-samples \\(t\\)\n\n\\(df = n-1\\) (number of difference scores minus \\(1\\))\n\n\n\nDifference scores:\n\n\\(D = X_B - X_A\\)\n\n\n\nMean of difference scores:\n\n\\(M_D = \\dfrac{\\Sigma D}{n}\\)\n\n\n\nStandard error of difference scores:\n\n\\(S_{M_D} = \\dfrac{s_D}{\\sqrt{n}}\\)\n\n\n\n\\(t\\) statistic:\n\n\\(t = \\dfrac{M_D - \\mu_D}{s_{M_D}}\\)"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#triplett",
    "href": "slides/14_related-samples-t-test.html#triplett",
    "title": "PSYC BC1101",
    "section": "Triplett",
    "text": "Triplett\n\nE.g. Norman Triplett (1898)\n\nPerforming alone/in competition\n\n\n\n\n\n\nTriplett, N. (1898). The dynamogenic factors in pacemaking and competition. The American Journal of Psychology, 9(4), 507-533. https://doi.org/10.2307/1412188"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#triplett-data",
    "href": "slides/14_related-samples-t-test.html#triplett-data",
    "title": "PSYC BC1101",
    "section": "Triplett data",
    "text": "Triplett data"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#as-independent-samples",
    "href": "slides/14_related-samples-t-test.html#as-independent-samples",
    "title": "PSYC BC1101",
    "section": "As independent samples",
    "text": "As independent samples\n\n\n\n\n\n\n\nAlone\n\\(X-M\\)\n\\((X-M)^2\\)\n\n\n\n\n54\n4.6\n21.16\n\n\n67\n17.6\n309.76\n\n\n38\n-11.4\n129.96\n\n\n46\n-3.4\n11.56\n\n\n42\n-7.4\n54.76\n\n\n\\(M = 49.40\\)\n\n\\(SS = 527.20\\)\n\n\n\n\n\\(s^2 = 131.80\\)\n\n\n\n\n\\(s = 11.48\\)\n\n\n\n\n\n\n\n\n\n\n\nCompetition\n\\(X-M\\)\n\\((X-M)^2\\)\n\n\n\n\n43\n-0.2\n0.04\n\n\n57\n13.8\n190.44\n\n\n39\n-4.2\n17.64\n\n\n41\n-2.2\n4.84\n\n\n36\n-7.2\n51.84\n\n\n\\(M = 43.20\\)\n\n\\(SS = 264.80\\)\n\n\n\n\n\\(s^2 = 66.20\\)\n\n\n\n\n\\(s = 8.14\\)"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#as-independent-samples-1",
    "href": "slides/14_related-samples-t-test.html#as-independent-samples-1",
    "title": "PSYC BC1101",
    "section": "As independent samples",
    "text": "As independent samples\n\nStep 2: Decision criteria\n\n\\(\\text{With } \\alpha = .05, t_{critical} (8) = \\pm 2.31\\)\n\nStep 3: Calculate\n\n\n\\(df = N - 2 = 10 - 2 = 8\\)\n\\(s^2_p = \\dfrac{SS_1 + SS_2}{df_1 + df_2} = \\dfrac{527.2 + 264.8}{4 + 4} = 99\\)\n\\(s_{M_1-M_2} = \\sqrt{\\dfrac{s_p^2}{n_1}+\\dfrac{s_p^2}{n_2}} = \\sqrt{\\dfrac{99}{5}+\\dfrac{99}{5}} = 6.29\\)\n\\(t = \\dfrac{(M_1-M_2)-(\\mu_1-\\mu_2)}{s_{M_1-M_2}} = \\dfrac{49.4 - 43.2}{6.29} = 0.99\\)"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#as-related-samples",
    "href": "slides/14_related-samples-t-test.html#as-related-samples",
    "title": "PSYC BC1101",
    "section": "As related samples",
    "text": "As related samples\n\n\n\n\n\nParticipant\nAlone\nComp\n\\(D\\)\n\\(D-M_D\\)\n\\((D-M_D)^2\\)\n\n\n\n\nViolet F.\n54\n43\n-11\n-4.8\n23.04\n\n\nAnna P.\n67\n57\n-10\n-3.8\n14.44\n\n\nWillie H.\n38\n39\n1\n7.2\n51.84\n\n\nBessie V.\n46\n41\n-5\n1.2\n1.44\n\n\nHoward C.\n42\n36\n-6\n0.2\n0.04\n\n\n\n\n\n\n\n\n\n\n\\(M_D = -6.2\\)\n\n\n\\(SS = 90.8\\)\n\\(s^2 = 22.7\\)\n\\(s = 4.76\\)"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#as-related-samples-1",
    "href": "slides/14_related-samples-t-test.html#as-related-samples-1",
    "title": "PSYC BC1101",
    "section": "As related samples",
    "text": "As related samples\n\n\\(\\text{With } \\alpha = .05, t_{critical} (4) = \\pm 2.78\\)\n\\(df = n-1 = 5 - 1 = 4\\)\n\\(S_{M_D} = \\dfrac{s_D}{\\sqrt{n}} = \\dfrac{4.76}{\\sqrt{5}} = 2.13\\)\n\\(t = \\dfrac{M_D - \\mu_D}{s_{M_D}} = \\dfrac{-6.2}{2.13} = -2.91\\)"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#effect-size",
    "href": "slides/14_related-samples-t-test.html#effect-size",
    "title": "PSYC BC1101",
    "section": "Effect size",
    "text": "Effect size\n\nStep 4b: Effect size\n\nIf the result was significant\nCohen’s \\(d\\) for related-samples \\(t\\)-test:\n\n\n\\[\\begin{align}\n\\text{Estimated Cohen's } d &= \\dfrac{\\text{mean of difference scores}}{\\text{SD of difference scores}} \\\\\n&= \\dfrac{M_D}{s_D} \\\\\n&= \\dfrac{-6.2}{4.76} = -1.3\n\\end{align}\\]"
  },
  {
    "objectID": "slides/14_related-samples-t-test.html#report-results",
    "href": "slides/14_related-samples-t-test.html#report-results",
    "title": "PSYC BC1101",
    "section": "Report results",
    "text": "Report results\n\nWhen performing in competition, children completed the race faster on average \\((M = 43.2\\); \\(SD = 8.14)\\) than when performing alone \\((M = 49.4\\); \\(SD = 11.48)\\). A related-samples found the difference to be statistically significant; \\(t(4) =\\) \\(-2.91\\), \\(p &lt;.05\\), \\(d = 1.30\\)."
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#terminology",
    "href": "slides/18_factorial-ANOVA.html#terminology",
    "title": "PSYC BC1101",
    "section": "Terminology",
    "text": "Terminology\n\nFactor\n\nThe variable that designates the groups being compared\nBetween-participants / within-participants\n\nLevels\n\nIndividual conditions or values that make up a factor\n\nFactorial design\n\nA study that combines two (or more) factors, each with two (or more) levels\nCan be manipulated between-participants or within-participants (or mixed-factorial design)\nE.g. 2x2 between-participants ANOVA"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#logic-1",
    "href": "slides/18_factorial-ANOVA.html#logic-1",
    "title": "PSYC BC1101",
    "section": "Logic",
    "text": "Logic\n\n\n\n\n\n\nFactor A\n\n\n\\(A_1\\)\n\\(A_2\\)\n\n\n\n\nFactor B\n\\(B_1\\)\n\\(A_1 B_1\\)\n\\(A_2 B_1\\)\n\n\n\\(B_2\\)\n\\(A_1 B_2\\)\n\\(A_2 B_2\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#logic-2",
    "href": "slides/18_factorial-ANOVA.html#logic-2",
    "title": "PSYC BC1101",
    "section": "Logic",
    "text": "Logic\n\n\n\n\n\n\n\nFactor A\n\n\n\\(A_1\\)\n\\(A_2\\)\n\n\n\n\nFactor B\n\\(B_1\\)\n\\(A_1 B_1\\)\n\\(A_2 B_1\\)\n\n\n\\(B_2\\)\n\\(A_1 B_2\\)\n\\(A_2 B_2\\)\n\n\n\n\n\n\nThree hypotheses tested by three \\(F\\)-ratios\n\nMain effect of Factor \\(A\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#logic-3",
    "href": "slides/18_factorial-ANOVA.html#logic-3",
    "title": "PSYC BC1101",
    "section": "Logic",
    "text": "Logic\n\n\n\n\n\n\nFactor A\n\n\n\\(A_1\\)\n\\(A_2\\)\n\n\n\n\nFactor B\n\\(B_1\\)\n\\(A_1 B_1\\)\n\\(A_2 B_1\\)\n\n\n\\(B_2\\)\n\\(A_1 B_2\\)\n\\(A_2 B_2\\)\n\n\n\n\n\n\nThree hypotheses tested by three \\(F\\)-ratios\n\nMain effect of Factor \\(B\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#logic-4",
    "href": "slides/18_factorial-ANOVA.html#logic-4",
    "title": "PSYC BC1101",
    "section": "Logic",
    "text": "Logic\n\n\n\n\n\n\nFactor A\n\n\n\\(A_1\\)\n\\(A_2\\)\n\n\n\n\nFactor B\n\\(B_1\\)\n\\(A_1 B_1\\)\n\\(A_2 B_1\\)\n\n\n\\(B_2\\)\n\\(A_1 B_2\\)\n\\(A_2 B_2\\)\n\n\n\n\n\n\nThree hypotheses tested by three \\(F\\)-ratios\n\nInteraction between \\(A\\) and \\(B\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#logic-5",
    "href": "slides/18_factorial-ANOVA.html#logic-5",
    "title": "PSYC BC1101",
    "section": "Logic",
    "text": "Logic\n\n\n\n\n\n\nFactor A\n\n\n\\(A_1\\)\n\\(A_2\\)\n\n\n\n\nFactor B\n\\(B_1\\)\n\\(A_1 B_1\\)\n\\(A_2 B_1\\)\n\n\n\\(B_2\\)\n\\(A_1 B_2\\)\n\\(A_2 B_2\\)\n\n\n\n\n\n\nThree hypotheses tested by three \\(F\\)-ratios\n\nEach tested with same basic \\(F\\)-ratio structure\n\n\n\n\\(F = \\dfrac{\\textrm{variance between treatments}}{\\textrm{variance expected with no treatment effect}}\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#example-data",
    "href": "slides/18_factorial-ANOVA.html#example-data",
    "title": "PSYC BC1101",
    "section": "Example data",
    "text": "Example data\n\n\n\n\n\n\nSnack\n\n\nBanana\nCandy\n\n\n\n\nTest\nMath\n7, 9, 8, 9\n5, 3, 4, 4\n\n\nReaction time\n5, 4, 6, 5\n6, 6, 5, 5"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#partitioning-variance",
    "href": "slides/18_factorial-ANOVA.html#partitioning-variance",
    "title": "PSYC BC1101",
    "section": "Partitioning variance",
    "text": "Partitioning variance\n\n\n\nTotal variance\n\n\nVariance between treatments\n\\(MS_{between \\ treatments}\\)\n\n\nVariance within groups\n\\(MS_{within}\\)\n\n\n\nFactor A\n\\(MS_{A}\\)\n\n\nFactor B\n\\(MS_{B}\\)\n\n\nInteraction\n\\(MS_{A*B}\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#partitioning-variance-1",
    "href": "slides/18_factorial-ANOVA.html#partitioning-variance-1",
    "title": "PSYC BC1101",
    "section": "Partitioning variance",
    "text": "Partitioning variance\n\n\n\n\\(SS_{total}\\)\n\\(SS_{between}\\)  \\(SS_{within}\\)\n\n\n\\(SS_A\\) \\(SS_B\\) \\(SS_{A*B}\\)\n\n\n\\(df_{total}\\)\n\\(df_{between}\\)  \\(df_{within}\\)\n\n\\(df_A\\) \\(df_B\\) \\(df_{A*B}\\)\n\n\n\n\n\\(SS_{total} = \\Sigma X^2 - \\dfrac{G^2}{N}\\)\n\\(SS_{within} = \\Sigma SS_{within \\ each \\ treatment}\\)\n\\(SS_{between} = \\Sigma \\dfrac{T^2}{n} - \\dfrac{G^2}{N}\\)\n\\(SS_{A} = \\Sigma \\dfrac{T^2_{col}}{n_{col}} - \\dfrac{G^2}{N}\\)\n\\(SS_{B} = \\Sigma \\dfrac{T^2_{row}}{n_{row}} - \\dfrac{G^2}{N}\\)\n\\(SS_{A*B} = SS_{between}-SS_{A}-SS_{B}\\)\n\n\\(df_{total} = N-1\\)\n\\(df_{within} = N-k\\)\n\\(df_{between} = k-1\\)\n\\(df_{A} = k_A-1\\)\n\\(df_{B} = k_B-1\\)\n\\(df_{A*B} = df_{between}-df_A-df_B\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#summary-table",
    "href": "slides/18_factorial-ANOVA.html#summary-table",
    "title": "PSYC BC1101",
    "section": "Summary table",
    "text": "Summary table\n\n\n\n\n\nSource\n\\(SS\\)\n\\(df\\)\n\\(MS\\)\n\\(F\\)\n\n\n\n\nBetween treatments\n\n\n\n\n\n\n   Factor A\n\n\n\n\n\n\n   Factor B\n\n\n\n\n\n\n   A * B interaction\n\n\n\n\n\n\nWithin treatments\n\n\n\n\n\n\nTotal"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-1.-hypotheses",
    "href": "slides/18_factorial-ANOVA.html#step-1.-hypotheses",
    "title": "PSYC BC1101",
    "section": "Step 1. Hypotheses",
    "text": "Step 1. Hypotheses\n\n\n\n\n\n\nSnack\n\n\nBanana\nCandy\n\n\n\n\nTest\nMath\n7, 9, 8, 9\n5, 3, 4, 4\n\n\nReaction time\n5, 4, 6, 5\n6, 6, 5, 5\n\n\n\n\n\n\n\nMain effect of Snack Type\n\n\\(H_0\\): \\(\\mu_{banana} = \\mu_{candy}\\)\n\\(H_1\\): \\(\\mu_{banana} \\ne \\mu_{candy}\\)\n\nMain effect of Test Type\n\n\\(H_0\\): \\(\\mu_{math} = \\mu_{RT}\\)\n\\(H_1\\): \\(\\mu_{math} \\ne \\mu_{RT}\\)\n\nSnack * Test interaction\n\n\\(H_0\\): No interaction\n\\(H_1\\): There is an interaction"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-2.-critical-region",
    "href": "slides/18_factorial-ANOVA.html#step-2.-critical-region",
    "title": "PSYC BC1101",
    "section": "Step 2. Critical region",
    "text": "Step 2. Critical region\n\nStep 2. Critical region\n\nNumerators: \\(\\begin{align} &df_{A} = k_A-1 = 1 \\\\ &df_{B} = k_B-1 = 1 \\\\ &df_{A*B} = k-1 - df_A - df_B = 1 \\end{align}\\)\nDenominator: \\(df_{within} = N-k  = 12\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha = .05\\)\n\n\n\\(df_{numerator}\\)\n\n\n\n\\(df_{denominator}\\)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n5\n6.61\n5.79\n5.41\n5.19\n5.05\n4.95\n4.88\n4.82\n4.77\n4.74\n\n\n6\n5.99\n5.14\n4.76\n4.53\n4.39\n4.28\n4.21\n4.15\n4.10\n4.06\n\n\n7\n5.59\n4.74\n4.35\n4.12\n3.97\n3.87\n3.79\n3.73\n3.68\n3.64\n\n\n8\n5.32\n4.46\n4.07\n3.84\n3.69\n3.58\n3.50\n3.44\n3.39\n3.35\n\n\n9\n5.12\n4.26\n3.86\n3.63\n3.48\n3.37\n3.29\n3.23\n3.18\n3.14\n\n\n10\n4.96\n4.10\n3.71\n3.48\n3.33\n3.22\n3.13\n3.07\n3.02\n2.98\n\n\n11\n4.84\n3.98\n3.59\n3.36\n3.20\n3.10\n3.01\n2.95\n2.90\n2.85\n\n\n12\n4.75\n3.88\n3.49\n3.26\n3.11\n3.00\n2.91\n2.85\n2.80\n2.75\n\n\n13\n4.67\n3.81\n3.41\n3.18\n3.02\n2.92\n2.83\n2.77\n2.71\n2.67\n\n\n14\n4.60\n3.74\n3.34\n3.11\n2.96\n2.85\n2.76\n2.70\n2.65\n2.60\n\n\n15\n4.54\n3.68\n3.29\n3.06\n2.90\n2.79\n2.71\n2.64\n2.59\n2.54"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-3.-calculations",
    "href": "slides/18_factorial-ANOVA.html#step-3.-calculations",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculations",
    "text": "Step 3. Calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnack\n\n\nBanana\nCandy\n\n\n\n\nTest\nMath\n\\(T = 33\\)\n\\(SS = 2.75\\)\n\\(T = 16\\)\n\\(SS = 2\\)\n\n\nReaction time\n\\(T = 20\\)\n\\(SS = 2\\)\n\\(T = 22\\)\n\\(SS = 1\\)\n\n\n\n\n\n\n\\(T_{col}\\) \\(53\\) \\(38\\)\n\n\n\n\\(T_{row}\\)\n\\(49\\)\n\\(42\\)\n\n\n\\(N = 16\\)\n\\(n = 4\\)\n\\(k = 4\\)\n\\(k_A = 2\\)\n\\(k_B = 2\\)\n\\(G = 91\\)\n\\(\\Sigma X^2 = 565\\)\n\n\n\\[\\begin{align}\ndf_{total} &= N-1 = 15 \\\\\ndf_{within} &= N-k  = 12 \\\\\ndf_{between} &= k-1  = 3\\\\\ndf_{A} &= k_A-1  = 1\\\\\ndf_{B} &= k_B-1  = 1\\\\\ndf_{A*B} &= df_{between} - df_A - df_B  = 1\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-3.-calculations-1",
    "href": "slides/18_factorial-ANOVA.html#step-3.-calculations-1",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculations",
    "text": "Step 3. Calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnack\n\n\nBanana\nCandy\n\n\n\n\nTest\nMath\n\\(T = 33\\)\n\\(SS = 2.75\\)\n\\(T = 16\\)\n\\(SS = 2\\)\n\n\nReaction time\n\\(T = 20\\)\n\\(SS = 2\\)\n\\(T = 22\\)\n\\(SS = 1\\)\n\n\n\n\n\n\n\\(T_{col}\\) \\(53\\) \\(38\\)\n\n\n\n\\(T_{row}\\)\n\\(49\\)\n\\(42\\)\n\n\n\\(N = 16\\)\n\\(n = 4\\)\n\\(k = 4\\)\n\\(k_A = 2\\)\n\\(k_B = 2\\)\n\\(G = 91\\)\n\\(\\Sigma X^2 = 565\\)\n\n\n\\[\\begin{align}\nSS_{total} &= \\Sigma X^2 - \\dfrac{G^2}{N} = 47.44\\\\\nSS_{within} &= \\Sigma SS_{within \\ each \\ treatment} = 7.75\\\\\nSS_{between} &= \\Sigma \\dfrac{T^2}{n} - \\dfrac{G^2}{N} = 39.69 \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-3.-calculations-2",
    "href": "slides/18_factorial-ANOVA.html#step-3.-calculations-2",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculations",
    "text": "Step 3. Calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnack\n\n\nBanana\nCandy\n\n\n\n\nTest\nMath\n\\(T = 33\\)\n\\(SS = 2.75\\)\n\\(T = 16\\)\n\\(SS = 2\\)\n\n\nReaction time\n\\(T = 20\\)\n\\(SS = 2\\)\n\\(T = 22\\)\n\\(SS = 1\\)\n\n\n\n\n\n\n\\(T_{col}\\) \\(53\\) \\(38\\)\n\n\n\n\\(T_{row}\\)\n\\(49\\)\n\\(42\\)\n\n\n\\(N = 16\\)\n\\(n = 4\\)\n\\(k = 4\\)\n\\(k_A = 2\\)\n\\(k_B = 2\\)\n\\(G = 91\\)\n\\(\\Sigma X^2 = 565\\)\n\n\n\\[\\begin{align}\nSS_{A} &= \\Sigma \\dfrac{T^2_{col}}{n_{col}} - \\dfrac{G^2}{N} = 14.06\\\\\nSS_{B} &= \\Sigma \\dfrac{T^2_{row}}{n_{row}} - \\dfrac{G^2}{N} = 3.06\\\\\nSS_{A*B} &= \\Sigma SS_{between}-SS_A - SS_B = 22.56\n\\end{align}\\]"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-3.-calculations-3",
    "href": "slides/18_factorial-ANOVA.html#step-3.-calculations-3",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculations",
    "text": "Step 3. Calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnack\n\n\nBanana\nCandy\n\n\n\n\nTest\nMath\n\\(T = 33\\)\n\\(SS = 2.75\\)\n\\(T = 16\\)\n\\(SS = 2\\)\n\n\nReaction time\n\\(T = 20\\)\n\\(SS = 2\\)\n\\(T = 22\\)\n\\(SS = 1\\)\n\n\n\n\n\n\n\\(T_{col}\\) \\(53\\) \\(38\\)\n\n\n\n\\(T_{row}\\)\n\\(49\\)\n\\(42\\)\n\n\n\\(N = 16\\)\n\\(n = 4\\)\n\\(k = 4\\)\n\\(k_A = 2\\)\n\\(k_B = 2\\)\n\\(G = 91\\)\n\\(\\Sigma X^2 = 565\\)\n\n\n\\(MS_{A} = \\dfrac{SS_{A}}{df_{A}} = 14.06 \\ \\ \\ \\ \\ \\ \\ \\ MS_{B} = \\dfrac{SS_{B}}{df_{B}} = 3.06\\)\n\\(MS_{A*B} = \\dfrac{SS_{A*B}}{df_{A*B}} = 22.56\\)\n\\(MS_{within} = \\dfrac{SS_{within}}{df_{within}} = 0.65\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-3.-calculations-4",
    "href": "slides/18_factorial-ANOVA.html#step-3.-calculations-4",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculations",
    "text": "Step 3. Calculations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSnack\n\n\nBanana\nCandy\n\n\n\n\nTest\nMath\n\\(T = 33\\)\n\\(SS = 2.75\\)\n\\(T = 16\\)\n\\(SS = 2\\)\n\n\nReaction time\n\\(T = 20\\)\n\\(SS = 2\\)\n\\(T = 22\\)\n\\(SS = 1\\)\n\n\n\n\n\n\n\\(T_{col}\\) \\(53\\) \\(38\\)\n\n\n\n\\(T_{row}\\)\n\\(49\\)\n\\(42\\)\n\n\n\\(N = 16\\)\n\\(n = 4\\)\n\\(k = 4\\)\n\\(k_A = 2\\)\n\\(k_B = 2\\)\n\\(G = 91\\)\n\\(\\Sigma X^2 = 565\\)\n\n\n\\(F_A = \\dfrac{MS_{A}}{MS_{within}} = 21.77\\)\n\\(F_B = \\dfrac{MS_{B}}{MS_{within}} = 4.74\\)\n\\(F_{A*B} = \\dfrac{MS_{A*B}}{MS_{within}} = 34.94\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-4a.-decision",
    "href": "slides/18_factorial-ANOVA.html#step-4a.-decision",
    "title": "PSYC BC1101",
    "section": "Step 4a. Decision",
    "text": "Step 4a. Decision\n\nFor each \\(F\\)-ratio, reject or fail to reject \\(H_0\\)\n\nCompare calculated \\(F\\) to corresponding \\(F_{critical}\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-4b.-effect-size",
    "href": "slides/18_factorial-ANOVA.html#step-4b.-effect-size",
    "title": "PSYC BC1101",
    "section": "Step 4b. Effect size",
    "text": "Step 4b. Effect size\n\n\\(\\eta^2_{partial}\\)\n\nPercentage of variability not explained by other factors\nSeparate effect size for each \\(F\\)-ratio\n\n\n\n\\(\\eta^2_A = \\dfrac{SS_A}{SS_{total}-SS_{B} - SS_{A*B}} = 0.64\\)\n\\(\\eta^2_B = \\dfrac{SS_B}{SS_{total}-SS_{A} - SS_{A*B}} = 0.28\\)\n\\(\\eta^2_{A*B} = \\dfrac{SS_{A*B}}{SS_{total}-SS_{A} - SS_{B}} = 0.74\\)"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#step-5.-report-results",
    "href": "slides/18_factorial-ANOVA.html#step-5.-report-results",
    "title": "PSYC BC1101",
    "section": "Step 5. Report results",
    "text": "Step 5. Report results\n\nDescriptives (usually in a table or graph)\nResults of hypothesis test for all three tests\n\n\n\n\nTo examine the influence of snack and test type on performance, a 2-factor ANOVA was conducted with test scores as the dependent variable and Snack Type and Test Type as between-participants independent variables. There was no significant main effect of Test Type \\((F (1, 12) = 4.74\\), \\(p &gt; .05)\\). There was, however, significant main effect of Snack Type \\((F (1, 12) = 21.77\\), \\(p &lt; .05\\), \\(\\eta^2 = .64)\\); overall, performance was superior in the Banana condition. Moreover, there was a significant interaction between Snack Type and Test Type \\((F (1, 12) = 34.94\\), \\(p &lt; .05\\), \\(\\eta^2 = .74)\\); performance on the math test was affected by snack type to a greater extent than performance on the reaction time test. The pattern of means and standard deviations is shown in Table 1. Trends are illustrated in Figure 1.\n\n\n\n\n\nFigure 1. Test performance by snack and test type"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs",
    "href": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs",
    "title": "PSYC BC1101",
    "section": "Interpreting interaction graphs",
    "text": "Interpreting interaction graphs\n\n\nSlope indicates main effect of factor on x-axis\nDistance between lines indicates main effect of other factor\nParallel lines indicate no interaction"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-1",
    "href": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-1",
    "title": "PSYC BC1101",
    "section": "Interpreting interaction graphs",
    "text": "Interpreting interaction graphs"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-2",
    "href": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-2",
    "title": "PSYC BC1101",
    "section": "Interpreting interaction graphs",
    "text": "Interpreting interaction graphs"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-3",
    "href": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-3",
    "title": "PSYC BC1101",
    "section": "Interpreting interaction graphs",
    "text": "Interpreting interaction graphs"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-4",
    "href": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-4",
    "title": "PSYC BC1101",
    "section": "Interpreting interaction graphs",
    "text": "Interpreting interaction graphs"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-5",
    "href": "slides/18_factorial-ANOVA.html#interpreting-interaction-graphs-5",
    "title": "PSYC BC1101",
    "section": "Interpreting interaction graphs",
    "text": "Interpreting interaction graphs"
  },
  {
    "objectID": "slides/18_factorial-ANOVA.html#your-turn",
    "href": "slides/18_factorial-ANOVA.html#your-turn",
    "title": "PSYC BC1101",
    "section": "Your turn",
    "text": "Your turn\n\n\n\nCome up with your own example\n\n2 IVs with 2 levels each\n1 DV (the thing you measure, e.g. test performance)\nSketch graph of expected results"
  },
  {
    "objectID": "slides/08_sampling.html#roadmap",
    "href": "slides/08_sampling.html#roadmap",
    "title": "PSYC BC1101",
    "section": "Roadmap",
    "text": "Roadmap\n\nSo far…\n\n\\(z\\)-scores describe the location of a single score in a sample or in a population\nNormal distributions: precisely quantify probability of obtaining certain scores\n\nMoving forward…\n\nQuantifying probability of obtaining certain sample statistics"
  },
  {
    "objectID": "slides/08_sampling.html#sampling-error-1",
    "href": "slides/08_sampling.html#sampling-error-1",
    "title": "PSYC BC1101",
    "section": "Sampling error",
    "text": "Sampling error\n\n\n\n\nError: Discrepancy between a sample statistic and the population parameter\n\n\nimport { pop } from \"./02_variables.qmd\";"
  },
  {
    "objectID": "slides/08_sampling.html#sampling-error-2",
    "href": "slides/08_sampling.html#sampling-error-2",
    "title": "PSYC BC1101",
    "section": "Sampling error",
    "text": "Sampling error\n\n\n\nDiscrepancy between a sample statistic and the population parameter\n\nE.g. Opinion polling\nsee Pew explainer"
  },
  {
    "objectID": "slides/08_sampling.html#sampling-error-iq",
    "href": "slides/08_sampling.html#sampling-error-iq",
    "title": "PSYC BC1101",
    "section": "Sampling error: IQ",
    "text": "Sampling error: IQ"
  },
  {
    "objectID": "slides/08_sampling.html#characteristics",
    "href": "slides/08_sampling.html#characteristics",
    "title": "PSYC BC1101",
    "section": "Characteristics",
    "text": "Characteristics\n\nShape\n\nThe distribution will be approximately normal\nSample \\(M\\)s are representative of population \\(\\mu\\)\nMost means will be close to \\(\\mu\\); means far from \\(\\mu\\) are rare\n\nCenter\n\nThe center/average of the distribution will be close to \\(\\mu\\)\n\\(M\\) is a unbiased statistic\nOn average, \\(M = \\mu\\)\n\nVariability\n\nRelated to sample size, \\(n\\)\nThe larger the sample, the less the variability\nLarger samples are more representative"
  },
  {
    "objectID": "slides/08_sampling.html#example-height-distribution",
    "href": "slides/08_sampling.html#example-height-distribution",
    "title": "PSYC BC1101",
    "section": "Example: height distribution",
    "text": "Example: height distribution"
  },
  {
    "objectID": "slides/08_sampling.html#example-height-distribution-1",
    "href": "slides/08_sampling.html#example-height-distribution-1",
    "title": "PSYC BC1101",
    "section": "Example: height distribution",
    "text": "Example: height distribution\n\n\n\n\n\n\n\n\nSample\nX1\nX2\nM\n\n\n\n\n1\n60\n60\n60\n\n\n2\n62\n60\n61\n\n\n3\n64\n60\n62\n\n\n4\n66\n60\n63\n\n\n5\n60\n62\n61\n\n\n6\n62\n62\n62\n\n\n7\n64\n62\n63\n\n\n8\n66\n62\n64\n\n\n9\n60\n64\n62\n\n\n10\n62\n64\n63\n\n\n11\n64\n64\n64\n\n\n12\n66\n64\n65\n\n\n13\n60\n66\n63\n\n\n14\n62\n66\n64\n\n\n15\n64\n66\n65\n\n\n16\n66\n66\n66\n\n\n\n\n\n\nSampling distribution (\\(n = 2\\))\n\n\n\\(p(M &lt; 61) =\\ ?\\)\n\\(p(62 \\le M \\le 64) =\\ ?\\)\n\\(p(M &gt; 65) =\\ ?\\)"
  },
  {
    "objectID": "slides/08_sampling.html#example-height-distribution-2",
    "href": "slides/08_sampling.html#example-height-distribution-2",
    "title": "PSYC BC1101",
    "section": "Example: height distribution",
    "text": "Example: height distribution\n\n\n\n\nNow we can calculate variability of sample means\n\nSince we obtained every sample mean\nUse population SD formula\n\n\n\n\n\n\n\n\n\\(X\\)\n\\(X-M\\)\n\\((X-M)^2\\)\n\n\n\n\n60\n-3\n9\n\n\n61\n-2\n4\n\n\n62\n-1\n1\n\n\n63\n0\n0\n\n\n61\n-2\n4\n\n\n62\n-1\n1\n\n\n63\n0\n0\n\n\n64\n1\n1\n\n\n62\n-1\n1\n\n\n63\n0\n0\n\n\n64\n1\n1\n\n\n65\n2\n4\n\n\n63\n0\n0\n\n\n64\n1\n1\n\n\n65\n2\n4\n\n\n66\n3\n9\n\n\n\\(M = 63.00\\)\n\n\\(SS = 40.00\\)\n\n\n\n\n\\(\\sigma^2 = 2.50\\)\n\n\n\n\n\\(\\sigma = 1.58\\)"
  },
  {
    "objectID": "slides/08_sampling.html#central-limit-theorem-1",
    "href": "slides/08_sampling.html#central-limit-theorem-1",
    "title": "PSYC BC1101",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nFor any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the distribution of sample means for sample size \\(n\\) will have…\n\nAn expected mean \\(\\mu_M\\) of \\(\\mu\\)\nA standard deviation of \\(\\dfrac{\\sigma} {\\sqrt{n}}\\)\nAnd will approach a normal distribution as \\(n\\) approaches infinity"
  },
  {
    "objectID": "slides/08_sampling.html#shape",
    "href": "slides/08_sampling.html#shape",
    "title": "PSYC BC1101",
    "section": "Shape",
    "text": "Shape\n\nAlmost perfectly normal in either of two conditions\n\nThe population from which the samples are selected is a normal distribution\nOr…\nSample \\(n\\)s are relatively large\n\n\n\n\n…what is relatively large?\n\nAs \\(n\\) approaches infinity, distribution of sample means approaches a normal distribution\nBut by \\(n = 30\\) means pile up symmetrically around \\(\\mu\\)\nPopulation distribution does not need to be normal; can be skewed, flat, bimodal, whatever"
  },
  {
    "objectID": "slides/08_sampling.html#mean",
    "href": "slides/08_sampling.html#mean",
    "title": "PSYC BC1101",
    "section": "Mean",
    "text": "Mean\n\nMean of the distribution of sample means is called the expected value of \\(M\\) ( \\(\\mu_M\\) )\n\nOn average, \\(M = \\mu_M  = \\mu\\)\n\\(M\\) is unbiased\nIf we only have a single sample \\(M\\), our best guess at the (unknown) population mean should always be the (known) sample mean\nBut we can acknowledge variability…"
  },
  {
    "objectID": "slides/08_sampling.html#variability",
    "href": "slides/08_sampling.html#variability",
    "title": "PSYC BC1101",
    "section": "Variability",
    "text": "Variability\n\nStandard deviation of the sample means\n\n“Standard error of the mean”; \\(\\sigma_M\\)\nMeasure of how well a sample mean estimates its population mean\nHow much sampling error we can expect; how much distance is expected on average between \\(M\\) and \\(\\mu\\)\n\n\n\n\\(\\sigma_M = \\dfrac{\\sigma}{\\sqrt{n}}\\) or \\(\\dfrac{\\sqrt{\\sigma^2}}{\\sqrt{n}}\\) or \\(\\sqrt{\\dfrac{\\sigma^2}{n}}\\)"
  },
  {
    "objectID": "slides/08_sampling.html#variability-1",
    "href": "slides/08_sampling.html#variability-1",
    "title": "PSYC BC1101",
    "section": "Variability",
    "text": "Variability"
  },
  {
    "objectID": "slides/08_sampling.html#variability-2",
    "href": "slides/08_sampling.html#variability-2",
    "title": "PSYC BC1101",
    "section": "Variability",
    "text": "Variability"
  },
  {
    "objectID": "slides/08_sampling.html#variability-3",
    "href": "slides/08_sampling.html#variability-3",
    "title": "PSYC BC1101",
    "section": "Variability",
    "text": "Variability\n\nμ =  \nσ =  \nn = 30\n\nσM ="
  },
  {
    "objectID": "slides/08_sampling.html#variability-heights-sampling-dist",
    "href": "slides/08_sampling.html#variability-heights-sampling-dist",
    "title": "PSYC BC1101",
    "section": "Variability: heights sampling dist",
    "text": "Variability: heights sampling dist\n\n\n\n\n\n\n\n\nSample\nX1\nX2\nM\n\n\n\n\n1\n60\n60\n60\n\n\n2\n62\n60\n61\n\n\n3\n64\n60\n62\n\n\n4\n66\n60\n63\n\n\n5\n60\n62\n61\n\n\n6\n62\n62\n62\n\n\n7\n64\n62\n63\n\n\n8\n66\n62\n64\n\n\n9\n60\n64\n62\n\n\n10\n62\n64\n63\n\n\n11\n64\n64\n64\n\n\n12\n66\n64\n65\n\n\n13\n60\n66\n63\n\n\n14\n62\n66\n64\n\n\n15\n64\n66\n65\n\n\n16\n66\n66\n66\n\n\n\n\n\n\nSampling distribution (\\(n = 2\\))\n\n\n\\(\\sigma_M = \\dfrac{\\sigma}{\\sqrt{n}} = \\dfrac{2.24}{\\sqrt{2}} = 1.58\\)"
  },
  {
    "objectID": "slides/08_sampling.html#summary",
    "href": "slides/08_sampling.html#summary",
    "title": "PSYC BC1101",
    "section": "Summary",
    "text": "Summary\n\nSummary\n\nDistribution of sample means for samples of size \\(n\\) will have…\n\na mean of \\(\\mu_M\\)\nstandard deviation \\(\\sigma_M = \\sigma / \\sqrt{n}\\)\nShape will be normal if population is normally distributed, or \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/02_variables.html#measuring-things",
    "href": "slides/02_variables.html#measuring-things",
    "title": "PSYC BC1101",
    "section": "Measuring things",
    "text": "Measuring things\n\nHow many books are red"
  },
  {
    "objectID": "slides/02_variables.html#measuring-things-1",
    "href": "slides/02_variables.html#measuring-things-1",
    "title": "PSYC BC1101",
    "section": "Measuring things",
    "text": "Measuring things\n\nHow much grain is in a bushel?"
  },
  {
    "objectID": "slides/02_variables.html#making-statistics-1",
    "href": "slides/02_variables.html#making-statistics-1",
    "title": "PSYC BC1101",
    "section": "Making statistics",
    "text": "Making statistics\n\nIn the United States today half of all children (35.6 million) live in a household where a parent or other adult uses tobacco, drinks heavily or uses illicit drugs1\n\n\nOther questions…\n\nHow many students are smokers?\nYoung people, narcissism, anxiety, depression 2\n\n\nThe National Center on Addiction and Substance Abuse at Columbia University, 2005See Singal, 2016"
  },
  {
    "objectID": "slides/02_variables.html#constructs-operational-definitions",
    "href": "slides/02_variables.html#constructs-operational-definitions",
    "title": "PSYC BC1101",
    "section": "Constructs & operational definitions",
    "text": "Constructs & operational definitions\n\nConstruct: Extroversion\nOperational definition: Big 5 questions\n\n\n\nConstruct: Intelligence\nOperational definition: IQ test\n\n\n\n\nConstruct: Height\nOperational definition: How far the top of your head is from the floor according to a tape measure"
  },
  {
    "objectID": "slides/02_variables.html#operationalizing-variables",
    "href": "slides/02_variables.html#operationalizing-variables",
    "title": "PSYC BC1101",
    "section": "Operationalizing variables",
    "text": "Operationalizing variables\n\nUsually more than one way we could measure & record data\nResult in different types of data, and potentially different applicable analyses\nHow to decide on operational definition?\n\nAspects to consider:\n\nType of variable (discrete / continuous)\nScale of measurement (nominal / ordinal / interval / ratio)"
  },
  {
    "objectID": "slides/02_variables.html#types-of-variable",
    "href": "slides/02_variables.html#types-of-variable",
    "title": "PSYC BC1101",
    "section": "Types of variable",
    "text": "Types of variable\n\n\n\nDiscrete\n\nCount as whole numbers\nSeparate, indivisible categories\nNo values exist between neighboring categories\nE.g. number of children/cats/tvs; positive cases; hospital admissions\n\n\n\n\nContinuous\n\nCan be measured with decimals\nHas infinite number of possible values\nEvery interval is divisible into infinite number of parts\nE.g. height, time, temperature\n\n\n\n\n\n\nbar_xScale = d3.scaleBand()\n  .domain([1,2,3,4,5])\n  .range([m, w - m])\n\nbar_yScale = d3.scaleLinear()\n  .domain([0, 3])\n  .range([h - m, m])\n    \nbar_chart = {\n\n  const svg = d3.select(DOM.svg(w, h))\n    .attr(\"class\", \"invertable\")\n    .attr(\"lazy-load\", true)\n  \n  const axis_lines = svg.append(\"g\")\n    .attr(\"stroke\", \"black\")\n    .attr(\"fill\", \"none\")\n    \n  const bars = svg.append(\"g\")\n    .attr(\"fill\", \"black\")\n    .attr(\"stroke\", \"none\")\n  \n  bars.selectAll(\"rect\")\n    .data(bar_data)\n    .enter()\n      .append(\"rect\") \n      .attr(\"transform\", \"rotate(180)\")\n      .attr(\"transform-origin\", \"center center\")\n      .attr(\"x\", d =&gt; bar_xScale(d.value)+2.5)\n      .attr(\"y\", m)\n      .attr(\"width\", (w-2*m)/5-5)\n      .attr(\"height\", d =&gt; d.density*(h-m*2)/3)\n      \n\n  svg.on('click',function(){\n   bars.selectAll(\"rect\")\n    .attr(\"height\", 0)\n    .transition()\n    .duration(300)\n    .delay(function(d, i){return (6-i)*300})\n    .attr(\"height\", d =&gt; d.density*(h-m*2)/3);\n  })\n  \n\n  axis_lines.selectAll(\"line\")\n    .data(line_data)\n    .enter()\n      .append(\"line\")\n        .attr(\"x1\", d =&gt; d.x1)\n        .attr(\"x2\", d =&gt; d.x2)\n        .attr(\"y1\", d =&gt; d.y1)\n        .attr(\"y2\", d =&gt; d.y2)\n    \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxScale = d3.scaleLinear()\n  .domain([-3, 3])\n  .range([margins.left, w - m])\n\nyScale = d3.scaleLinear()\n  .domain([0, 0.4])\n  .range([h - m, m])\n  \nsmooth_curve = {\n  const svg_curve = d3.select(DOM.svg(w, h))\n    .attr(\"class\", \"invertable\");\n  \n  const axis_lines = svg_curve.append(\"g\")\n    .attr(\"stroke\", \"black\")\n    .attr(\"fill\", \"none\")\n  \n  var p = svg_curve.append(\"path\")\n    .datum(curve_data)\n    .attr(\"stroke\", \"black\")\n    .attr(\"stroke-width\", 2)\n    .attr(\"fill\", \"none\")\n    .attr(\"d\", line)\n    \n  var totalLength = p.node().getTotalLength();\n\n  svg_curve.on(\"click\", function(){\n    p.transition()\n      .duration(0)\n      .attr(\"stroke-dasharray\", totalLength + \" \" + totalLength)\n      .attr(\"stroke-dashoffset\", totalLength)\n      .transition()\n      .duration(2000)\n      .attr(\"stroke-dashoffset\", 0)})\n        \n  axis_lines.selectAll(\"line\")\n    .data(line_data)\n    .enter()\n      .append(\"line\")\n        .attr(\"x1\", d =&gt; d.x1)\n        .attr(\"x2\", d =&gt; d.x2)\n        .attr(\"y1\", d =&gt; d.y1)\n        .attr(\"y2\", d =&gt; d.y2)\n    \n  return svg_curve.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw = 400\nh = 200\n\nm = 20\nhalf_m = m * 0.5\n\nmargins = ({top: 20, right: 20, bottom: 20, left: 20})\n\nline_data = [\n    {x1: half_m, x2: w - half_m, y1: h - half_m, y2: h - half_m},\n    {x1: half_m, x2: half_m, y1: h - half_m, y2: half_m}\n  ]\n  \nline = d3.line()\n    .x(d =&gt; xScale(d.value))\n    .y(d =&gt; yScale(d.density))"
  },
  {
    "objectID": "slides/02_variables.html#scales-of-measurement",
    "href": "slides/02_variables.html#scales-of-measurement",
    "title": "PSYC BC1101",
    "section": "Scales of measurement",
    "text": "Scales of measurement\n\n\n\n\n\n\n\n\n\n\nScale\nCharacteristics\nExamples\n\n\n\n\nNominal\nNamed categories\nNo quantitative distinctions\nGender\nEye color\nExperimental condition\n\n\nOrdinal\nOrdered categories\nIndicates direction, but not size of difference\nRank\nClothing sizes\nOlympic medals\n\n\nInterval\nOrdered categories\nEqual intervals between categories\nArbitrary or absent zero point\nTemperature (Celcius/Fahrenheit)\nGolf scores\n\n\nRatio\nOrdered categories\nEqual interval between categories\nAbsolute zero point\nTemperature (Kelvin)\nNumber of correct answers\nResponse time"
  },
  {
    "objectID": "slides/02_variables.html#likert-scales",
    "href": "slides/02_variables.html#likert-scales",
    "title": "PSYC BC1101",
    "section": "Likert scales",
    "text": "Likert scales\n\nWhat is your current level of happiness?\n\nA lot less than usual\nA little less than usual\nAbout average\nA little more than usual\nA lot more than usual\n\n\n\nBarry, D. (2017). Do not use averages with Likert scale data. https://bookdown.org/Rmadillo/likert/"
  },
  {
    "objectID": "slides/02_variables.html#draw-sample-make-inference",
    "href": "slides/02_variables.html#draw-sample-make-inference",
    "title": "PSYC BC1101",
    "section": "Draw sample, make inference",
    "text": "Draw sample, make inference"
  },
  {
    "objectID": "slides/02_variables.html#terminology",
    "href": "slides/02_variables.html#terminology",
    "title": "PSYC BC1101",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nPopulations\n\nPopulation parameters\nUsually Greek symbols\ne.g. \\(\\mu\\); \\(N\\)\nInferential statistics\n\n\n\n\nSamples\n\nSample statistics\nUsually letters\ne.g. \\(M\\); \\(n\\)\nDescriptive statistics"
  },
  {
    "objectID": "slides/03_frequency.html#statistics",
    "href": "slides/03_frequency.html#statistics",
    "title": "PSYC BC1101",
    "section": "Statistics",
    "text": "Statistics\n\nA bunch of numbers looking for an argument. 1\n\n\nMathematical procedures used to collect, organize, summarize & interpret information\n\nProvide standardized evaluation procedures\nTell us about patterns of interest in data\n\nEtymology\n\nStatistics comes from status, meaning state\nThe state of the state\nCensus, birth/death rate, incomes, unemployment etc\n\n\nJones, 2011"
  },
  {
    "objectID": "slides/03_frequency.html#how-often-does-something-occur",
    "href": "slides/03_frequency.html#how-often-does-something-occur",
    "title": "PSYC BC1101",
    "section": "How often does something occur?",
    "text": "How often does something occur?\n\nE.g. Political polling"
  },
  {
    "objectID": "slides/03_frequency.html#frequency-distributions",
    "href": "slides/03_frequency.html#frequency-distributions",
    "title": "PSYC BC1101",
    "section": "Frequency distributions",
    "text": "Frequency distributions\n\nA frequency distribution…\n\nOrganizes and displays data\nConveys how scores are distributed\nCan be either a table or a graph\n\nShows:\n\nCategories that make up the scale\nFrequency, or number of observations, in each category\nAnd/or proportion/percentage/cumulative percent of scores in each category"
  },
  {
    "objectID": "slides/03_frequency.html#simple-example",
    "href": "slides/03_frequency.html#simple-example",
    "title": "PSYC BC1101",
    "section": "Simple example",
    "text": "Simple example\n\nFrom raw scores\n\n\n\n2 1 2 3 4 3 3 2 2 1 5 3 4 1 2\n\n\n\n\n\nTo this\n\n\n\n\n\n\n\n\\(X\\)\n\\(f\\)\n\n\n\n\n1\n3\n\n\n2\n5\n\n\n3\n4\n\n\n4\n2\n\n\n5\n1\n\n\n\n\n\n\n\n\nOr this"
  },
  {
    "objectID": "slides/03_frequency.html#frequency-tables-1",
    "href": "slides/03_frequency.html#frequency-tables-1",
    "title": "PSYC BC1101",
    "section": "Frequency tables",
    "text": "Frequency tables\n\n\n\n\nMidterm scores: 41 43 44 45 48 50 51 51 52 52 52 53 53 53 54 54 55 55 55 55 55 56 56 56 56 56 57 57 57 57 58 58 58 59 59 59 59 59 59 59\n\n\n\nRegular frequency table not always appropriate\n\nLarge number of scores, low frequencies\n\n\n\n\nSolution: Grouped frequency tables\n\nEasier to understand\nBut lose information\n\n\n\n\n\n\n\n\n\n\\(X\\)\n\\(f\\)\n\n\n\n\n41\n1\n\n\n42\n0\n\n\n43\n1\n\n\n44\n1\n\n\n45\n1\n\n\n46\n0\n\n\n47\n0\n\n\n48\n1\n\n\n49\n0\n\n\n50\n1\n\n\n51\n2\n\n\n52\n3\n\n\n53\n3\n\n\n54\n2\n\n\n55\n5\n\n\n56\n5\n\n\n57\n4\n\n\n58\n3\n\n\n59\n7"
  },
  {
    "objectID": "slides/03_frequency.html#grouped-frequency-table",
    "href": "slides/03_frequency.html#grouped-frequency-table",
    "title": "PSYC BC1101",
    "section": "Grouped frequency table",
    "text": "Grouped frequency table\n\nWhat’s the range of scores?\nHow can you turn that into about 10 groups? (using a simple number, e.g. 2, 5, 10…)\nWhat should we make the bottom score of each interval? (so that the bottom is a multiple of width, i.e, start at 10, not 11)\nList intervals in \\(X\\) column, frequencies in \\(f\\) column\n(optional) Create columns for proportion, percent, cumulative percent"
  },
  {
    "objectID": "slides/03_frequency.html#grouped-frequency-table-1",
    "href": "slides/03_frequency.html#grouped-frequency-table-1",
    "title": "PSYC BC1101",
    "section": "Grouped frequency table",
    "text": "Grouped frequency table\n\n\n\nGrouped frequency table\n\nBins all same width\nWidth is a simple number (2)\nBottom score is multiple of width (i.e, divisible by 2)\nProduces good number of bins\n\n\n\n\n\n\n\n\n\\(X\\)\n\\(f\\)\n\n\n\n\n40-41\n1\n\n\n42-43\n1\n\n\n44-45\n2\n\n\n46-47\n0\n\n\n48-49\n1\n\n\n50-51\n3\n\n\n52-53\n6\n\n\n54-55\n7\n\n\n56-57\n9\n\n\n58-59\n10"
  },
  {
    "objectID": "slides/03_frequency.html#frequency-graphs",
    "href": "slides/03_frequency.html#frequency-graphs",
    "title": "PSYC BC1101",
    "section": "Frequency graphs",
    "text": "Frequency graphs\n\n\n\nHistogram, frequency polygon, bar chart, curve\n\nAppropriate type depends on:\n\nLevel of measurement (nominal; ordinal; interval; ratio)\nDescribing sample or population?\nWant to show more than one group?"
  },
  {
    "objectID": "slides/03_frequency.html#bar-graph",
    "href": "slides/03_frequency.html#bar-graph",
    "title": "PSYC BC1101",
    "section": "Bar graph",
    "text": "Bar graph\n\n\n\nFor nominal or ordinal data\nCategories on \\(x\\)-axis, frequency on \\(y\\)-axis\nSpaces between adjacent bars indicates separate categories"
  },
  {
    "objectID": "slides/03_frequency.html#histogram",
    "href": "slides/03_frequency.html#histogram",
    "title": "PSYC BC1101",
    "section": "Histogram",
    "text": "Histogram\n\n\n\nFor interval or ratio data\nScores/bins on \\(x\\)-axis, frequency on \\(y\\)-axis\nHeight corresponds to frequency\nBars centered on category"
  },
  {
    "objectID": "slides/03_frequency.html#grouped-histogram",
    "href": "slides/03_frequency.html#grouped-histogram",
    "title": "PSYC BC1101",
    "section": "Grouped histogram",
    "text": "Grouped histogram\n\n\n\n\n\n\n\n\n\n\\(X\\)\n\\(f\\)\n\n\n\n\n40-41\n1\n\n\n42-43\n1\n\n\n44-45\n2\n\n\n46-47\n0\n\n\n48-49\n1\n\n\n50-51\n3\n\n\n52-53\n6\n\n\n54-55\n7\n\n\n56-57\n9\n\n\n58-59\n10"
  },
  {
    "objectID": "slides/03_frequency.html#frequency-polygon",
    "href": "slides/03_frequency.html#frequency-polygon",
    "title": "PSYC BC1101",
    "section": "Frequency polygon",
    "text": "Frequency polygon\n\n\n\nBasically the same as a histogram\n\nScores on the \\(X\\)-axis\nFrequency on \\(Y\\)-axis\nDot above the center of each interval\nConnect dots with a line\nClose the polygon with lines to the \\(Y = 0\\) point\nCan also be used with grouped frequency distribution data"
  },
  {
    "objectID": "slides/03_frequency.html#frequency-polygon-1",
    "href": "slides/03_frequency.html#frequency-polygon-1",
    "title": "PSYC BC1101",
    "section": "Frequency polygon",
    "text": "Frequency polygon\n\nUseful for comparing distributions"
  },
  {
    "objectID": "slides/03_frequency.html#frequency-polygon-2",
    "href": "slides/03_frequency.html#frequency-polygon-2",
    "title": "PSYC BC1101",
    "section": "Frequency polygon",
    "text": "Frequency polygon\n\n…where overlapping histograms are harder to understand"
  },
  {
    "objectID": "slides/03_frequency.html#population-curve",
    "href": "slides/03_frequency.html#population-curve",
    "title": "PSYC BC1101",
    "section": "Population curve",
    "text": "Population curve\n\n\n\nUsed for population distributions\n\nWhen population is large, scores for each individual are usually not known\nSmooth curve indicates exact scores were not used\nConvey relative frequency"
  },
  {
    "objectID": "slides/05_variability.html#variability-example-1",
    "href": "slides/05_variability.html#variability-example-1",
    "title": "PSYC BC1101",
    "section": "Variability example",
    "text": "Variability example\n\n\n\n\n\nLeptokurtotic\n\n\n\n\nPlatykurtotic"
  },
  {
    "objectID": "slides/05_variability.html#variability-1",
    "href": "slides/05_variability.html#variability-1",
    "title": "PSYC BC1101",
    "section": "Variability",
    "text": "Variability\n\nLike the mean\n\nA descriptive statistic\nSingle number to summarize dataset\n\nUnlike the mean\n\nRather than describing the middle of the data, variability describes the spread of the data\nHigher variability means greater differences between scores\n\nPurpose\n\nQuantify how well an individual score represents the distribution\nImportant for inferential stats"
  },
  {
    "objectID": "slides/05_variability.html#measures-of-variability",
    "href": "slides/05_variability.html#measures-of-variability",
    "title": "PSYC BC1101",
    "section": "Measures of variability",
    "text": "Measures of variability\n\nQuantitative distance measures based on the differences between scores\n\nEach has different characteristics\n\nRange\n\nDescribes the spread of scores\nDistance of most extreme scores from each other\n\n\\(SS\\), Variance, and Standard Deviation\n\nCompanion concepts, but different things\nDescribe distance of scores from the mean\nSmall values: low variability; scores clustered close to mean\nHigher values: greater variability; scores widely scattered"
  },
  {
    "objectID": "slides/05_variability.html#range-1",
    "href": "slides/05_variability.html#range-1",
    "title": "PSYC BC1101",
    "section": "Range",
    "text": "Range\n\nDifference between lowest & highest scores\n\nDistance covered by the scores in a distribution\nRange = \\(X_{max} - X_{min}\\)\n\n\n\nYou: 0, 4, 5, 5, 6, 10\n\n\\(10 - 0 = 10\\)\n\nFriend: 0, 1, 5, 5, 9, 10\n\n\\(10 - 0 = 10\\)"
  },
  {
    "objectID": "slides/05_variability.html#range-2",
    "href": "slides/05_variability.html#range-2",
    "title": "PSYC BC1101",
    "section": "Range",
    "text": "Range\n\nCharacteristics\n\nDoes not consider all the data\nBased only on two scores: most extreme values\nImprecise, unreliable measure of variability\nNot often useful for descriptive/inferential stats\n\nBut checking range & min/max values can be useful for finding mistakes in data input\n\nImpossible range / min & max values"
  },
  {
    "objectID": "slides/05_variability.html#definitions",
    "href": "slides/05_variability.html#definitions",
    "title": "PSYC BC1101",
    "section": "Definitions",
    "text": "Definitions\n\nDeviation\n\nDistance from the mean: deviation score = \\(X – \\mu\\)\n\n\\(SS\\): Sum of squares\n\nSum of squared deviations\n\nVariance\n\nThe mean squared deviation\nAverage squared distance from the mean\nCalculation differs for population and samples\n\nStandard deviation\n\nThe square root of the variance\nProvides a measure of the average (standard) distance of scores from the mean"
  },
  {
    "objectID": "slides/05_variability.html#approach",
    "href": "slides/05_variability.html#approach",
    "title": "PSYC BC1101",
    "section": "Approach",
    "text": "Approach\n\nDetermine the deviation of each score\n\nDistance from the mean\nDeviation score = \\(X - \\mu\\)\n\n\n\n\nTo find “average deviation” just sum the deviations and divide by \\(n\\)?\n\nDead end; always sums to \\(0\\)"
  },
  {
    "objectID": "slides/05_variability.html#calculations",
    "href": "slides/05_variability.html#calculations",
    "title": "PSYC BC1101",
    "section": "Calculations",
    "text": "Calculations\n\n\n\nFind the deviation for each score\n\n\n\\(X - \\mu\\)\n\n\n\n\nSquare deviations\n\n\n\\((X - \\mu)^2\\)\n\n\n\n\nSum the squared deviations\n\n\n\\(SS = \\Sigma(X - \\mu)^2\\)\n\n\n\n\nFind average of squared deviations\n\n\n\\(\\sigma^2 = \\dfrac{SS}N\\)\n\n\n\n\nTake square root of variance\n\n\n\\(\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\dfrac{SS}N}\\)"
  },
  {
    "objectID": "slides/05_variability.html#calculating-by-hand",
    "href": "slides/05_variability.html#calculating-by-hand",
    "title": "PSYC BC1101",
    "section": "Calculating by hand",
    "text": "Calculating by hand\n\n\n\n\n\n\n\n\\(X\\)\n\\(X-M\\)\n\\((X-M)^2\\)\n\n\n\n\n0\n-5\n25\n\n\n4\n-1\n1\n\n\n5\n0\n0\n\n\n5\n0\n0\n\n\n6\n1\n1\n\n\n10\n5\n25\n\n\n\\(M = 5.00\\)\n\n\\(SS = 52.00\\)\n\n\n\n\n\\(\\sigma^2 = 8.67\\)\n\n\n\n\n\\(\\sigma = 2.94\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\n\\(X-M\\)\n\\((X-M)^2\\)\n\n\n\n\n0\n-5\n25\n\n\n1\n-4\n16\n\n\n5\n0\n0\n\n\n5\n0\n0\n\n\n9\n4\n16\n\n\n10\n5\n25\n\n\n\\(M = 5.00\\)\n\n\\(SS = 82.00\\)\n\n\n\n\n\\(\\sigma^2 = 13.67\\)\n\n\n\n\n\\(\\sigma = 3.70\\)"
  },
  {
    "objectID": "slides/05_variability.html#sum-of-squared-deviations",
    "href": "slides/05_variability.html#sum-of-squared-deviations",
    "title": "PSYC BC1101",
    "section": "Sum of squared deviations",
    "text": "Sum of squared deviations\n\nVery important concept! Especially later\n\n\n\n\nDefinitional formula\n\nFind each deviation score \\((X – \\mu)\\)\nSquare each deviation score \\((X–\\mu)^2\\)\nSum up the squared deviations \\(\\Sigma(X–\\mu)^2\\)\n\n\n\n\\(SS = \\Sigma(X - \\mu)^2\\)\n\n\n\nComputational formula\n\nSquare each score & sum the squared scores\nFind the sum of scores, square it, divide by \\(N\\)\nSubtract the second part from the first\n\n\n\n\\(SS = \\Sigma X^2 - \\dfrac{(\\Sigma X)^2}N\\)"
  },
  {
    "objectID": "slides/05_variability.html#underestimation-soup",
    "href": "slides/05_variability.html#underestimation-soup",
    "title": "PSYC BC1101",
    "section": "Underestimation: Soup",
    "text": "Underestimation: Soup"
  },
  {
    "objectID": "slides/05_variability.html#underestimation-height",
    "href": "slides/05_variability.html#underestimation-height",
    "title": "PSYC BC1101",
    "section": "Underestimation: Height",
    "text": "Underestimation: Height"
  },
  {
    "objectID": "slides/05_variability.html#calculating-variability-of-samples",
    "href": "slides/05_variability.html#calculating-variability-of-samples",
    "title": "PSYC BC1101",
    "section": "Calculating variability of samples",
    "text": "Calculating variability of samples\n\nSimple solution: divide \\(SS\\) by \\(n – 1\\) instead of \\(n\\)\n\nProduces unbiased estimate of the population variance"
  },
  {
    "objectID": "slides/05_variability.html#variability-equations-for-samples",
    "href": "slides/05_variability.html#variability-equations-for-samples",
    "title": "PSYC BC1101",
    "section": "Variability equations for samples",
    "text": "Variability equations for samples\n\n\n\nFind the deviation for each score\n\n\n\\(X - M\\)\n\n\n\n\nSquare deviations\n\n\n\\((X - M)^2\\)\n\n\n\n\nSum the squared deviations\n\n\n\\(SS = \\Sigma(X - M)^2\\)\n\n\n\n\nFind average of squared deviations\n\n\n\\(s^2 = \\dfrac{SS} {\\color{red}{n-1}}\\)\n\n\n\n\nTake square root of variance\n\n\n\\(SD = \\sqrt{s^2} = \\sqrt{\\dfrac{SS}{\\color{red}{n-1}}}\\)"
  },
  {
    "objectID": "slides/05_variability.html#degrees-of-freedom-1",
    "href": "slides/05_variability.html#degrees-of-freedom-1",
    "title": "PSYC BC1101",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\n\n\n🂠\n\n\n🂠\n\n\n🂠"
  },
  {
    "objectID": "slides/05_variability.html#degrees-of-freedom-2",
    "href": "slides/05_variability.html#degrees-of-freedom-2",
    "title": "PSYC BC1101",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\n\n\n🃓\n\n\n🂠\n\n\n🂠"
  },
  {
    "objectID": "slides/05_variability.html#degrees-of-freedom-3",
    "href": "slides/05_variability.html#degrees-of-freedom-3",
    "title": "PSYC BC1101",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\n\n\n🃓\n\n\n🃕\n\n\n🂠"
  },
  {
    "objectID": "slides/05_variability.html#degrees-of-freedom-4",
    "href": "slides/05_variability.html#degrees-of-freedom-4",
    "title": "PSYC BC1101",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\n\nWhy \\(n – 1\\)?\n\n\\(M = 5\\)\n\\(n = 3\\)\n\nIf you know the first 2 scores:\n\n3, 5\n\\(M = \\dfrac{\\Sigma X}{N} = \\dfrac{3 + 5 + X}{3}\\)\nSo \\(X = 3*M - 3 - 5 = 7\\)\n\nThere is only 1 possible value that \\(X\\) can be\n\nIt is not free to vary\nWe’ve lost 1 degree of freedom"
  },
  {
    "objectID": "slides/05_variability.html#star-wars-variability",
    "href": "slides/05_variability.html#star-wars-variability",
    "title": "PSYC BC1101",
    "section": "Star Wars variability",
    "text": "Star Wars variability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetaCritic.com\n\n\nIMDb.com\n\n\n\nMovie\nN\nM\nSD\nN\nM\nSD\n\n\n\n\nStar Wars: Episode IV - A New Hope\n24\n90.1\n16.7\n1,302,317\n85.0\n16.4\n\n\nStar Wars: Episode V - The Empire Strikes Back\n25\n83.5\n20.8\n1,230,844\n85.9\n16.3\n\n\nStar Wars: Episode VI - Return of the Jedi\n24\n63.8\n25.2\n1,006,122\n82.6\n15.5\n\n\nStar Wars: Episode I - The Phantom Menace\n36\n56.4\n22.9\n769,435\n65.3\n20.3\n\n\nStar Wars: Episode II - Attack of the Clones\n39\n60.3\n22.0\n678,519\n66.3\n20.2\n\n\nStar Wars: Episode III - Revenge of the Sith\n40\n71.4\n19.7\n751,751\n76.1\n18.8\n\n\nStar Wars: Episode VII - The Force Awakens\n55\n80.8\n14.2\n898,767\n77.7\n19.6\n\n\nStar Wars: Episode VIII - The Last Jedi\n56\n84.1\n13.5\n602,045\n66.6\n24.5\n\n\nStar Wars: Episode IX - The Rise of Skywalker\n61\n57.2\n16.5\n418,656\n64.7\n23.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMetaCritic.com\n\n\nIMDb.com\n\n\n\nMovie\nN\nM\nSD\nN\nM\nSD\n\n\n\n\nStar Wars: Episode IV - A New Hope\n24\n90.1\n16.7\n1,302,317\n85.0\n16.4\n\n\nStar Wars: Episode V - The Empire Strikes Back\n25\n83.5\n20.8\n1,230,844\n85.9\n16.3\n\n\nStar Wars: Episode VI - Return of the Jedi\n24\n63.8\n25.2\n1,006,122\n82.6\n15.5\n\n\nStar Wars: Episode I - The Phantom Menace\n36\n56.4\n22.9\n769,435\n65.3\n20.3\n\n\nStar Wars: Episode II - Attack of the Clones\n39\n60.3\n22.0\n678,519\n66.3\n20.2\n\n\nStar Wars: Episode III - Revenge of the Sith\n40\n71.4\n19.7\n751,751\n76.1\n18.8\n\n\nStar Wars: Episode VII - The Force Awakens\n55\n80.8\n14.2\n898,767\n77.7\n19.6\n\n\nStar Wars: Episode VIII - The Last Jedi\n56\n84.1\n13.5\n602,045\n66.6\n24.5\n\n\nStar Wars: Episode IX - The Rise of Skywalker\n61\n57.2\n16.5\n418,656\n64.7\n23.4"
  },
  {
    "objectID": "slides/19_correlation.html#limitations-of-correlation",
    "href": "slides/19_correlation.html#limitations-of-correlation",
    "title": "PSYC BC1101",
    "section": "Limitations of correlation",
    "text": "Limitations of correlation\n\nCorrelations are not usually useful for demonstrating causation\n\nEstablishing causation requires experiments in which something is manipulated\n\nE.g. coffee consumption & intelligence\n\nPerhaps…\n\nA causes B\nB causes A\nSomething else (C) causes A and B\n\n\n\n\nCorley, J., et al. (2010). Caffeine consumption and cognitive function at age 70: The Lothian Birth Cohort 1936 study. Psychosomatic medicine 72(2), 206-214. https://doi.org/10.1097/PSY.0b013e3181c92a9c"
  },
  {
    "objectID": "slides/19_correlation.html#logic",
    "href": "slides/19_correlation.html#logic",
    "title": "PSYC BC1101",
    "section": "Logic",
    "text": "Logic\n\n\n\n\n\n\n\nParticipant\nSleep\nScore\n\n\n\n\nA\n4\n5\n\n\nB\n5\n8\n\n\nC\n7\n8\n\n\nD\n8\n10\n\n\nE\n11\n9"
  },
  {
    "objectID": "slides/19_correlation.html#pearsons-r",
    "href": "slides/19_correlation.html#pearsons-r",
    "title": "PSYC BC1101",
    "section": "Pearson’s \\(r\\)",
    "text": "Pearson’s \\(r\\)\n\nMost widely used correlation statistic\n\nMeasures the degree and the direction of the linear relationship between two variables\n\nVariability & covariability\n\nVariability: How much each variable varies\nCovariability: How much \\(X\\) and \\(Y\\) vary in tandem\nAre changes in \\(X\\) associated with corresponding changes in \\(Y\\)?\nE.g. as \\(X\\) increases, \\(Y\\) tends to increase (positive)\nOr, as \\(X\\) increases, \\(Y\\) tends to decrease (negative)"
  },
  {
    "objectID": "slides/19_correlation.html#pearsons-r-1",
    "href": "slides/19_correlation.html#pearsons-r-1",
    "title": "PSYC BC1101",
    "section": "Pearson’s \\(r\\)",
    "text": "Pearson’s \\(r\\)\n\\(r = \\dfrac{SP}{\\sqrt{SS_X SS_Y}} = \\dfrac{\\textrm{covariability of X and Y}}{\\textrm{variability of X and Y separately}}\\)\n\n\n\nPerfect linear relationship…\n\nIf every change in \\(X\\) has a corresponding change in \\(Y\\), variability separately = covariability\nCorrelation will be \\(–1.00\\) or \\(+1.00\\)"
  },
  {
    "objectID": "slides/19_correlation.html#step-1.-state-hypotheses",
    "href": "slides/19_correlation.html#step-1.-state-hypotheses",
    "title": "PSYC BC1101",
    "section": "Step 1. State hypotheses",
    "text": "Step 1. State hypotheses\n\nCorrelation coefficient \\(r\\) is computed for sample data\nHypotheses concerns relationship in the population\nGreek letter \\(\\rho\\) (rho) represents population correlation\nNon-directional:\n\n\\(H_0: \\rho = 0\\)\n\\(H_1: \\rho \\ne 0\\)\n\nDirectional:\n\n\\(H_0: \\rho \\le 0\\); \\(H_1: ρ &gt; 0\\)\nOr…\n\\(H_0: \\rho \\ge 0\\); \\(H_1: \\rho &lt; 0\\)"
  },
  {
    "objectID": "slides/19_correlation.html#step-2.-critical-region",
    "href": "slides/19_correlation.html#step-2.-critical-region",
    "title": "PSYC BC1101",
    "section": "Step 2. Critical region",
    "text": "Step 2. Critical region\n\n\n\nCritical value for \\(r\\) is a type of \\(t\\) statistic\n\n\\(df = n - 2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProportion\nin 1 tail\n\n\n\n0.1\n\n\n\n0.05\n\n\n\n0.025\n\n\n\n0.01\n\n\n\n0.005\n\n\n\nProportion\nin 2 tails\n0.2\n0.1\n0.05\n0.02\n0.01\n\n\n\n\n1\n3.078\n6.314\n12.706\n31.821\n63.657\n\n\n2\n1.886\n2.920\n4.303\n6.965\n9.925\n\n\n3\n1.638\n2.353\n3.182\n4.541\n5.841\n\n\n4\n1.533\n2.132\n2.776\n3.747\n4.604\n\n\n5\n1.476\n2.015\n2.571\n3.365\n4.032\n\n\n6\n1.440\n1.943\n2.447\n3.143\n3.707\n\n\n7\n1.415\n1.895\n2.365\n2.998\n3.499\n\n\n\\(df\\)       8\n1.397\n1.860\n2.306\n2.896\n3.355\n\n\n9\n1.383\n1.833\n2.262\n2.821\n3.250\n\n\n10\n1.372\n1.812\n2.228\n2.764\n3.169\n\n\n11\n1.363\n1.796\n2.201\n2.718\n3.106\n\n\n12\n1.356\n1.782\n2.179\n2.681\n3.055\n\n\n13\n1.350\n1.771\n2.160\n2.650\n3.012\n\n\n14\n1.345\n1.761\n2.145\n2.624\n2.977\n\n\n15\n1.341\n1.753\n2.131\n2.602\n2.947\n\n\n...\n...\n...\n...\n...\n..."
  },
  {
    "objectID": "slides/19_correlation.html#step-3.-calculate-statistic",
    "href": "slides/19_correlation.html#step-3.-calculate-statistic",
    "title": "PSYC BC1101",
    "section": "Step 3. Calculate statistic",
    "text": "Step 3. Calculate statistic\n\n\\(r = \\dfrac{SP}{\\sqrt{SS_X SS_Y}}\\)\n\n\n\n\nNumerator: Sum of Products, \\(SP\\)\n\nMultiply deviations by one another\n\n\nDefinitional formula\n\n\\(SP = \\Sigma(X - M_X)(Y - M_Y)\\)\n\n\n\nDenominator: Sum of Squares, \\(SS\\)\n\nMultiply deviations by themselves\n\n\nDefinitional formula\n\\[\\begin{align}\nSS &= \\Sigma(X - M_X)^2 \\\\\n   &= \\Sigma(X - M_X)(X - M_X)\n\\end{align}\\]\n\n\n\nComputational formula\n\n\\(SP = \\Sigma XY - \\dfrac{\\Sigma X \\Sigma Y}{n}\\)\n\n\nComputational formula\n\\[\\begin{align}\nSS &= \\Sigma X^2 - \\dfrac{(\\Sigma X)^2}{n}\\\\\n   &= \\Sigma X X - \\dfrac{(\\Sigma X)(\\Sigma X)}{n}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/19_correlation.html#calculating-r",
    "href": "slides/19_correlation.html#calculating-r",
    "title": "PSYC BC1101",
    "section": "Calculating \\(r\\)",
    "text": "Calculating \\(r\\)\n\n\n\n\n\n\\(X\\)\n\\(X-M_X\\)\n\\((X-M_X)^2\\)\n\\(Y\\)\n\\(Y-M_Y\\)\n\\((Y-M_Y)^2\\)\n\\(P\\)\n\n\n\n\n4\n-3\n9\n5\n-3\n9\n9\n\n\n5\n-2\n4\n8\n0\n0\n0\n\n\n7\n0\n0\n8\n0\n0\n0\n\n\n8\n1\n1\n10\n2\n4\n2\n\n\n11\n4\n16\n9\n1\n1\n4\n\n\n\\(M_X = 7.00\\)\n\n\\(SS_X = 30.00\\)\n\\(M_Y = 8.00\\)\n\n\\(SS_Y = 14.00\\)\n\\(SP = 15.00\\)\n\n\n\n\n\\(s^2_X = 7.50\\)\n\n\n\\(s^2_Y = 3.50\\)\n\n\n\n\n\n\\(s_X = 2.74\\)\n\n\n\\(s_Y = 1.87\\)\n\n\n\n\n\n\n\n\\(r = \\dfrac{SP}{\\sqrt{SS_X SS_Y}}\\)"
  },
  {
    "objectID": "slides/19_correlation.html#calculating-r-1",
    "href": "slides/19_correlation.html#calculating-r-1",
    "title": "PSYC BC1101",
    "section": "Calculating \\(r\\)",
    "text": "Calculating \\(r\\)\n\n\n\n\n\n\\(X\\)\n\\(X-M_X\\)\n\\((X-M_X)^2\\)\n\\(Y\\)\n\\(Y-M_Y\\)\n\\((Y-M_Y)^2\\)\n\\(P\\)\n\n\n\n\n4\n-3\n9\n5\n-3\n9\n9\n\n\n5\n-2\n4\n8\n0\n0\n0\n\n\n7\n0\n0\n8\n0\n0\n0\n\n\n8\n1\n1\n10\n2\n4\n2\n\n\n11\n4\n16\n9\n1\n1\n4\n\n\n\\(M_X = 7.00\\)\n\n\\(SS_X = 30.00\\)\n\\(M_Y = 8.00\\)\n\n\\(SS_Y = 14.00\\)\n\\(SP = 15.00\\)\n\n\n\n\n\\(s^2_X = 7.50\\)\n\n\n\\(s^2_Y = 3.50\\)\n\n\n\n\n\n\\(s_X = 2.74\\)\n\n\n\\(s_Y = 1.87\\)\n\n\n\n\n\n\n\n\\(r = \\dfrac{SP}{\\sqrt{SS_X SS_Y}}\\)"
  },
  {
    "objectID": "slides/19_correlation.html#calculating-r-2",
    "href": "slides/19_correlation.html#calculating-r-2",
    "title": "PSYC BC1101",
    "section": "Calculating \\(r\\)",
    "text": "Calculating \\(r\\)\n\n\n\n\n\n\\(X\\)\n\\(X-M_X\\)\n\\((X-M_X)^2\\)\n\\(Y\\)\n\\(Y-M_Y\\)\n\\((Y-M_Y)^2\\)\n\\(P\\)\n\n\n\n\n4\n-3\n9\n5\n-3\n9\n9\n\n\n5\n-2\n4\n8\n0\n0\n0\n\n\n7\n0\n0\n8\n0\n0\n0\n\n\n8\n1\n1\n10\n2\n4\n2\n\n\n11\n4\n16\n9\n1\n1\n4\n\n\n\\(M_X = 7.00\\)\n\n\\(SS_X = 30.00\\)\n\\(M_Y = 8.00\\)\n\n\\(SS_Y = 14.00\\)\n\\(SP = 15.00\\)\n\n\n\n\n\\(s^2_X = 7.50\\)\n\n\n\\(s^2_Y = 3.50\\)\n\n\n\n\n\n\\(s_X = 2.74\\)\n\n\n\\(s_Y = 1.87\\)\n\n\n\n\n\n\n\n\\(r = \\dfrac{SP}{\\sqrt{SS_X SS_Y}} = \\dfrac{15}{\\sqrt{30*14}} = 0.73\\)"
  },
  {
    "objectID": "slides/19_correlation.html#calculating-r-computation-approach",
    "href": "slides/19_correlation.html#calculating-r-computation-approach",
    "title": "PSYC BC1101",
    "section": "Calculating \\(r\\): computation approach",
    "text": "Calculating \\(r\\): computation approach\n\n\n\n\n\n\\(X\\)\n\\(X^2\\)\n\\(Y\\)\n\\(Y^2\\)\n\\(XY\\)\n\n\n\n\n4\n16\n5\n25\n20\n\n\n5\n25\n8\n64\n40\n\n\n7\n49\n8\n64\n56\n\n\n8\n64\n10\n100\n80\n\n\n11\n121\n9\n81\n99\n\n\n\\(\\Sigma X = 35\\)\n\\(\\Sigma X^2 = 275\\)\n\\(\\Sigma Y = 40\\)\n\\(\\Sigma Y^2 = 334\\)\n\\(\\Sigma XY = 295\\)\n\n\n\n\n\n\n\\(SS_X = \\Sigma X^2 - \\dfrac{(\\Sigma X)^2}{n} = 275 - \\dfrac{35^2}{5} = 30\\)\n\\(SS_Y = \\Sigma Y^2 - \\dfrac{(\\Sigma Y)^2}{n} = 334 - \\dfrac{40^2}{5} = 14\\)\n\\(SP = \\Sigma XY - \\dfrac{\\Sigma X \\Sigma Y}{n} = 295 - \\dfrac{35 \\cdot 40}{5} = 15\\)\n\\(r = \\dfrac{SP}{\\sqrt{SS_X SS_Y}} = \\dfrac{15}{\\sqrt{30 \\cdot 14}} = 0.73\\)"
  },
  {
    "objectID": "slides/19_correlation.html#step-4.-make-decision",
    "href": "slides/19_correlation.html#step-4.-make-decision",
    "title": "PSYC BC1101",
    "section": "Step 4. Make decision",
    "text": "Step 4. Make decision\n\nHypothesis test for \\(r\\) is a type of \\(t\\) test\n\n\n\\(t = \\dfrac{\\text{sample statistic} - \\text{population parameter}}{\\text{standard error}}\\)\n\n\n\\(t = \\dfrac{r - \\rho}{s_r}\\)\n\n\\(s_r = \\sqrt{\\dfrac{1 - r^2}{n-2}}\\)\n\\(t = \\dfrac{0.73}{\\sqrt{\\dfrac{1-0.73^2}{5 - 2}}} = 1.86\\)"
  },
  {
    "objectID": "slides/19_correlation.html#step-5.-report-results",
    "href": "slides/19_correlation.html#step-5.-report-results",
    "title": "PSYC BC1101",
    "section": "Step 5. Report results",
    "text": "Step 5. Report results\n\nValue of correlation, \\(r\\)\n\\(df\\) (or sample size)\n\\(p\\)-value or \\(\\alpha\\) level\n\n\n“More sleep was associated with higher test scores; however, the correlation did not reach statistical significance; \\(r (3)  = .73, p &gt; .05\\).”"
  },
  {
    "objectID": "slides/19_correlation.html#interpreting-effect-size",
    "href": "slides/19_correlation.html#interpreting-effect-size",
    "title": "PSYC BC1101",
    "section": "Interpreting effect size",
    "text": "Interpreting effect size\n\n\nCohen (1977)\n\n\n\n\n\n\\(d\\)\n\\(r\\)\nDescription\n\n\n\n\n0.3\n0.1\nSmall\n\n\n0.5\n0.3\nMedium\n\n\n0.8\n0.5\nLarge\n\n\n\n\n\n\nFunder & Ozer (2019)\n\n\n\n\n\n\\(d\\)\n\\(r\\)\nDescription\n\n\n\n\n0.10\n0.05\nVery small for the explanation of single events but potentially consequential in longer run\n\n\n0.20\n0.10\nStill small at the level of single events but potentially more ultimately consequential\n\n\n0.40\n0.20\nMedium effect of some explanatory and practical use even in the short run\n\n\n0.60\n0.30\nLarge effect that is potentially powerful in both the short and the long run\n\n\n0.80\n0.40\nA very large effect in the context of psychological research; likely to be a gross overestimate\n\n\n\n\n\n\n\nFunder, D. C., & Ozer, D. J. (2019). Evaluating Effect Size in Psychological Research: Sense and Nonsense. Advances in Methods and Practices in Psychological Science, 2(2), 156–168. https://doi.org/10.1177/2515245919847202"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#recap-1",
    "href": "slides/09_hypothesis-testing.html#recap-1",
    "title": "PSYC BC1101",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\n\n\n\nSample\nX1\nX2\nM\n\n\n\n\n1\n60\n60\n60\n\n\n2\n62\n60\n61\n\n\n3\n64\n60\n62\n\n\n4\n66\n60\n63\n\n\n5\n60\n62\n61\n\n\n6\n62\n62\n62\n\n\n7\n64\n62\n63\n\n\n8\n66\n62\n64\n\n\n9\n60\n64\n62\n\n\n10\n62\n64\n63\n\n\n11\n64\n64\n64\n\n\n12\n66\n64\n65\n\n\n13\n60\n66\n63\n\n\n14\n62\n66\n64\n\n\n15\n64\n66\n65\n\n\n16\n66\n66\n66\n\n\n\n\n\n\n\nSampling distribution\n\nFind probabilities of sample means\n\n\n\n\n\\(p(M &lt; 61) =\\ ?\\)"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#central-limit-theorem",
    "href": "slides/09_hypothesis-testing.html#central-limit-theorem",
    "title": "PSYC BC1101",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nTells us sampling distribution characteristics without having to take all possible samples\n\n\n\\(\\mu_M = \\mu  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\sigma_M = \\dfrac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#together",
    "href": "slides/09_hypothesis-testing.html#together",
    "title": "PSYC BC1101",
    "section": "Together…",
    "text": "Together…\n\n\n\nSample statistics, normal distributions, probability, Central Limit Theorem\n\nWe can find \\(z\\)-score for any sample mean\nUsing characteristics of sampling distribution of the mean \\((\\mu_M\\) and \\(\\sigma_M)\\)\nPosition of given sample mean in the population of all possible sample means\nThen find probability (using Unit Normal Table / pnorm(), just like for regular \\(z\\)-scores)\n\n\n\n\n\\(z = \\dfrac{M-\\mu_M}{\\sigma_M}\\)"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#spiderman",
    "href": "slides/09_hypothesis-testing.html#spiderman",
    "title": "PSYC BC1101",
    "section": "Spiderman",
    "text": "Spiderman"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#inferences-spiderman",
    "href": "slides/09_hypothesis-testing.html#inferences-spiderman",
    "title": "PSYC BC1101",
    "section": "Inferences: Spiderman",
    "text": "Inferences: Spiderman\n\nAre Peter Parker’s RTs “noticeably different?”\n\n\\(z = -2.5\\)\nCan state precise probability of observing a \\(z\\)-score that (or more) extreme\n\n\n\n\n\n\n\n\npnorm(-2.5)\n\n[1] 0.006209665\n\npnorm(159, mean = 284, sd = 50)\n\n[1] 0.006209665"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#spidermen",
    "href": "slides/09_hypothesis-testing.html#spidermen",
    "title": "PSYC BC1101",
    "section": "Spidermen",
    "text": "Spidermen"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#inferences-spidermen",
    "href": "slides/09_hypothesis-testing.html#inferences-spidermen",
    "title": "PSYC BC1101",
    "section": "Inferences: spidermen",
    "text": "Inferences: spidermen\n\nA sample of spidermen (spidermans?)\n\nHow likely is a particular sample mean, given the population characteristics?\nPopulation \\(\\mu = 284; \\sigma = 50\\)\n\n\n\n\n\nSingle score of \\(X = 159\\)\nFind position of that score in population distribution and find probability\n\n\\(z = \\dfrac{X-\\mu}{\\sigma} = \\dfrac{159 - 284}{50} = -2.5\\)\n\npnorm(-2.5)\n\n[1] 0.006209665\n\n\n\n\nSample mean of \\(M = 159\\); \\(n = 5\\)\nFind position of that \\(M\\) in sampling distribution and find probability\n\n\\(z = \\dfrac{M-\\mu_M}{\\sigma_M} = \\dfrac{159 - 284}{50 / \\sqrt{5}} = -5.59\\)\n\npnorm(-5.59)\n\n[1] 1.135348e-08"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#making-inferences-spidermen",
    "href": "slides/09_hypothesis-testing.html#making-inferences-spidermen",
    "title": "PSYC BC1101",
    "section": "Making inferences: spidermen",
    "text": "Making inferences: spidermen\n\nFor sample size \\(n = 5\\), approximately 0.00000001 of sample means are this (or more) extreme\n\nGiven how unlikely the mean is, maybe spidermen aren’t from this population"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#inferential-diagram",
    "href": "slides/09_hypothesis-testing.html#inferential-diagram",
    "title": "PSYC BC1101",
    "section": "Inferential diagram",
    "text": "Inferential diagram\n\n\n\n\n\n\nTreatment\n\n\nKnown\noriginal\npopulation\n\n\nUnknown\ntreated\npopulation\n\n\nSample\n\n\nTreated\nsample"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#null-hypothesis-significance-testing",
    "href": "slides/09_hypothesis-testing.html#null-hypothesis-significance-testing",
    "title": "PSYC BC1101",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\n\nStep 1: State hypotheses\n\n“Null” and “alternative”\n\nStep 2: Set decision criteria\n\n\\(\\alpha\\) and critical region(s)\n\nStep 3: Collect & analyze data\n\nCalculate required statistics\n\nStep 4: Make decision\n\nCompare outcome with predicted probabilities\nAccept or reject the null hypothesis"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#state-hypotheses",
    "href": "slides/09_hypothesis-testing.html#state-hypotheses",
    "title": "PSYC BC1101",
    "section": "1. State hypotheses",
    "text": "1. State hypotheses\n\nNull hypothesis: \\(H_0\\)\n\nStates that “treatment” has no effect\nTreated population is indistinguishable from original population\nNo change, no difference, or no relationship\n\nAlternative hypothesis: \\(H_1\\)\n\nStates that treated population differs from nontreated population\nThere is a change, a difference, or there is a relationship in the general population\n\nLogical complements\n\nCan’t both be true\nEnsures falsifiability"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#state-hypotheses-1",
    "href": "slides/09_hypothesis-testing.html#state-hypotheses-1",
    "title": "PSYC BC1101",
    "section": "1. State hypotheses",
    "text": "1. State hypotheses\n\n\n\nClaim: This pill makes you smarter\n\n\\(H_0\\): The pill doesn’t effect intelligence\n\\(H_1\\): The pill affects intelligence\n\n\n\n💊🧠\n\n\n\n\nClaim: Standing like superman makes you feel more confident\n\n\\(H_0\\): Posture does not affect confidence\n\\(H_1\\): Posture does affect confidence\n\n\n\n🦸‍♀ 😎️\n\n\n\n\nClaim: The more education people complete, the more they earn\n\n\\(H_0\\): Education is not associated with income\n\\(H_1\\): There is a relationship between education and income\n\n\n\n🎓🤑"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#decision-criterion",
    "href": "slides/09_hypothesis-testing.html#decision-criterion",
    "title": "PSYC BC1101",
    "section": "2. Decision criterion",
    "text": "2. Decision criterion\n\nIf the null hypothesis is true, what sample statistics are likely/unlikely?\n\nCentral Limit Theorem shows what samples are likely\nIf we get a very unlikely sample, we may reject the null\nSpecific sampling distribution depends on what test is being performed\n\nAlpha level & p-value\n\n\\(\\alpha\\) (alpha) is the probability value used to define “very unlikely” outcomes\np-value is the precise probability of statistics as extreme or more than observed sample statistic, assuming the null hypothesis is correct\nTypical alpha used by psychologists is \\(\\alpha = .05\\)\n\\(p &lt; .05\\); “Statistically significant”"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#decision-criterion-1",
    "href": "slides/09_hypothesis-testing.html#decision-criterion-1",
    "title": "PSYC BC1101",
    "section": "2. Decision criterion",
    "text": "2. Decision criterion\n\nDivide distribution of sample means into two parts\n\nOutcomes likely if \\(H_0\\) is true\nOutcomes unlikely if \\(H_0\\) is true\nBoundaries for critical region(s) determined by alpha"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#decision-criterion-2",
    "href": "slides/09_hypothesis-testing.html#decision-criterion-2",
    "title": "PSYC BC1101",
    "section": "2. Decision criterion",
    "text": "2. Decision criterion\n\n\n\nDirectional tests\n\nResearcher has a specific prediction about the direction of the treatment\nSpecifies (in advance) looking for increase or decrease\n\n\n\n\n\n\n\n\n\nNondirectional tests\n\nLooking for a difference in either direction"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#data-collection",
    "href": "slides/09_hypothesis-testing.html#data-collection",
    "title": "PSYC BC1101",
    "section": "3. Data collection",
    "text": "3. Data collection\n\nRandomly sample population of interest\n\nCompute a sample statistic to show the exact position (probability) of the sample in the distribution of sample means\nExact form of test statistic depends on research design\n\\(z\\)-test; \\(t\\)-test; ANOVA; correlation & regression statistics etc etc etc…"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#make-decision",
    "href": "slides/09_hypothesis-testing.html#make-decision",
    "title": "PSYC BC1101",
    "section": "4. Make decision",
    "text": "4. Make decision\n\nTwo possible outcomes:\n\nIf the sample statistic is not located in critical region(s)\n\nFail to reject null\nMeaning there does not seem to be an effect\n\nSample statistic is located in critical region(s)\n\n\\(p &lt; \\alpha\\)\nReject null\nMeaning there does seem to be an effect"
  },
  {
    "objectID": "slides/09_hypothesis-testing.html#spiderman-z-test",
    "href": "slides/09_hypothesis-testing.html#spiderman-z-test",
    "title": "PSYC BC1101",
    "section": "Spiderman \\(z\\)-test",
    "text": "Spiderman \\(z\\)-test\n\nFormal Spidermen z-test\n\n1: State Hypotheses\n\n\\(H_0\\): Radioactive spiderbites do not alter reaction times\n\\(H_1\\): Radioactive spiderbites alter reaction times\n\n2: Decision criteria\n\n\\(\\alpha = .05\\) two-tailed; Critical regions are -1.96 and 1.96\n\n3: Collect data; compute statistics & probabilities\n\n\\(\\mu = 284\\); \\(\\sigma = 50\\); so if \\(n = 5\\), \\(\\sigma_M = 50/\\sqrt{5} = 22.36\\)\n\\(M = 159\\); \\(z = (159 - 284) / 22.36 = -5.59\\)\n\n4: Decision\n\nObserved sample mean is in the critical region\n\\(p &lt; .05\\)\nReject the null"
  },
  {
    "objectID": "slides/06_z-scores.html#z-score-calculation",
    "href": "slides/06_z-scores.html#z-score-calculation",
    "title": "PSYC BC1101",
    "section": "\\(z\\)-score calculation",
    "text": "\\(z\\)-score calculation\n\n\\(z\\)-score formula:\n\n\n\\(z = \\dfrac{X - \\mu}\\sigma\\)     or…     \\(z = \\dfrac{X - M}s\\)\n\n\nNumerator: deviation score\nDenominator: standard deviation\n\\(z\\) expresses deviation in SD units"
  },
  {
    "objectID": "slides/06_z-scores.html#z-score-description",
    "href": "slides/06_z-scores.html#z-score-description",
    "title": "PSYC BC1101",
    "section": "\\(z\\)-score description",
    "text": "\\(z\\)-score description\n\n\n\n\\(z\\)-score describes exact location of any score in a distribution\nTwo pieces of information:\n\nSign\n\nPositive or negative\nIndicates whether score is located above or below the mean\n\nMagnitude\n\nIndicates distance between score and mean in standard deviation units\n\\(z = 0\\) is equal to the mean"
  },
  {
    "objectID": "slides/06_z-scores.html#example-test-scores",
    "href": "slides/06_z-scores.html#example-test-scores",
    "title": "PSYC BC1101",
    "section": "Example: test scores",
    "text": "Example: test scores\n\nHow well did you do on a test\nIs your score good, bad, just ok?\n\n\n\n\n\n\nTest\nScore\nM\nSD\n\n\n\n\ngeniustest.com\n80\n70\n5\n\n\nmensa.lu\n40\n20\n10\n\n\n\n\n\n\n\\(z\\)-score can describe location of a score in any distribution\n\nMakes scores from different distributions comparable"
  },
  {
    "objectID": "slides/06_z-scores.html#example-test-scores-1",
    "href": "slides/06_z-scores.html#example-test-scores-1",
    "title": "PSYC BC1101",
    "section": "Example: test scores",
    "text": "Example: test scores\n\nComparing scores from different distributions\n\nHow many SDs is a score above/below the mean?\n\n\n\n\n\\(z = \\dfrac{80-70}{5} = \\dfrac{10}{5} = 2\\)\n\n\n\\(z = \\dfrac{40-20}{10} = \\dfrac{20}{10} = 2\\)"
  },
  {
    "objectID": "slides/06_z-scores.html#determining-raw-score-from-z-score",
    "href": "slides/06_z-scores.html#determining-raw-score-from-z-score",
    "title": "PSYC BC1101",
    "section": "Determining raw score from \\(z\\)-score",
    "text": "Determining raw score from \\(z\\)-score\n\n\\(z = \\dfrac{X - \\mu}\\sigma\\)     so…     \\(X = \\mu + z\\sigma\\)\n\n\nAlgebraically solve for \\(X\\)\nRaw score \\(X\\) equals population mean plus \\(z\\) multiplied by standard deviation"
  },
  {
    "objectID": "slides/06_z-scores.html#determining-raw-score-from-z-score-1",
    "href": "slides/06_z-scores.html#determining-raw-score-from-z-score-1",
    "title": "PSYC BC1101",
    "section": "Determining raw score from \\(z\\)-score",
    "text": "Determining raw score from \\(z\\)-score\n\n\n\\[\\begin{align}\nX & = \\mu + z\\sigma \\\\\n& = 70 + 2*5 \\\\\n& = 70 + 10 \\\\\n& = 80 \\end{align}\\]\n\n\n\\[\\begin{align}\nX & = \\mu + z\\sigma \\\\\n& = 20 + 2*10 \\\\\n& = 20 + 20 \\\\\n& = 40 \\end{align}\\]"
  },
  {
    "objectID": "slides/06_z-scores.html#z-distribution",
    "href": "slides/06_z-scores.html#z-distribution",
    "title": "PSYC BC1101",
    "section": "\\(z\\) distribution",
    "text": "\\(z\\) distribution\n\nEvery \\(X\\) value can be transformed to a \\(z\\)-score\n\n\\(z\\)-score distribution is called a standardized distribution\n\nCharacteristics of \\(z\\)-score transformation\n\nSame shape as original distribution\nMean of \\(z\\)-score distribution is always \\(0\\)\n\nBecause the mean is the balance point; \\(\\Sigma (X - \\mu)\\) always equals \\(0\\)\n\nStandard deviation is always \\(1.00\\)\n\nBecause \\(SD\\) is the denominator"
  },
  {
    "objectID": "slides/06_z-scores.html#example",
    "href": "slides/06_z-scores.html#example",
    "title": "PSYC BC1101",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides/06_z-scores.html#other-standardized-distributions",
    "href": "slides/06_z-scores.html#other-standardized-distributions",
    "title": "PSYC BC1101",
    "section": "Other standardized distributions",
    "text": "Other standardized distributions\n\nStandardized: Predetermined mean & SD\n\n\\(z\\) distribution has \\(\\mu = 0\\) and \\(\\sigma = 1\\)\nSAT has \\(\\mu = 500\\) and \\(\\sigma = 100\\)\nIQ has \\(\\mu=100\\) and \\(\\sigma=15\\) points\n\nStandardizing a distribution has two steps\n\nOriginal raw scores transformed to \\(z\\)-scores\nThe \\(z\\)-scores are transformed to new \\(X\\) values so that the specific predetermined \\(\\mu\\) and \\(\\sigma\\) are attained\n2a. Multiply to set SD\n2b. Add or subtract a constant to set the mean"
  },
  {
    "objectID": "slides/06_z-scores.html#standardizing-scores",
    "href": "slides/06_z-scores.html#standardizing-scores",
    "title": "PSYC BC1101",
    "section": "Standardizing scores",
    "text": "Standardizing scores\n\n\n\n\nOriginal\n\n\n\\(z\\)-scores\n\n\n2a. Set SD\n\n2b. Set \\(M\\)\n\n\n\ngeniustest\n\n\n\n\n\n\n\n\n\n\n\nmensa.lu"
  },
  {
    "objectID": "slides/06_z-scores.html#z-scores-and-inferential-stats",
    "href": "slides/06_z-scores.html#z-scores-and-inferential-stats",
    "title": "PSYC BC1101",
    "section": "\\(z\\)-scores and inferential stats",
    "text": "\\(z\\)-scores and inferential stats"
  },
  {
    "objectID": "slides/06_z-scores.html#z-scores-and-inferential-stats-1",
    "href": "slides/06_z-scores.html#z-scores-and-inferential-stats-1",
    "title": "PSYC BC1101",
    "section": "\\(z\\)-scores and inferential stats",
    "text": "\\(z\\)-scores and inferential stats\n\n\n\n\nhttps://humanbenchmark.com/tests/reactiontime\n\nDo 5 reaction time trials\n(click when the color changes)\nNote your reaction time in \\(ms\\)\nThen click play to resume lecture when you’re done"
  },
  {
    "objectID": "slides/06_z-scores.html#peter-parker",
    "href": "slides/06_z-scores.html#peter-parker",
    "title": "PSYC BC1101",
    "section": "Peter Parker",
    "text": "Peter Parker\n\n\n\n\nPeter Parker \\(RT = 159ms\\)\n\nImpressive?\nDepends on population characteristics\n\n\n\n\n\\(\\begin{align} z & = \\dfrac{X - \\mu}{\\sigma} \\\\\n& = \\dfrac{159 - 284}{50} = -2.5 \\end{align}\\)"
  },
  {
    "objectID": "slides/06_z-scores.html#cbt-for-ocd",
    "href": "slides/06_z-scores.html#cbt-for-ocd",
    "title": "PSYC BC1101",
    "section": "CBT for OCD",
    "text": "CBT for OCD\n\nEfficacy of CBT for OCD1\n\nPre-treatment population \\(M = 30.25; SD = 14.89\\)\nSuppose a treated individual scores \\(X = 15.49\\)\n\n\n\n\n\n\n\\(\\begin{align} z & = \\dfrac{X - M}{s} \\\\\n& = \\dfrac{15.49 - 30.25}{14.89} \\\\\n& = -0.99 \\end{align}\\)\n\nAbramowitz et al. (2010)"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#type-1-error",
    "href": "slides/10_hypothesis-testing-pt-2.html#type-1-error",
    "title": "PSYC BC1101",
    "section": "Type 1 error",
    "text": "Type 1 error\n\nIf \\(H_0\\) is true…"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#inferential-errors-1",
    "href": "slides/10_hypothesis-testing-pt-2.html#inferential-errors-1",
    "title": "PSYC BC1101",
    "section": "Inferential errors",
    "text": "Inferential errors\n\n\n\nBoy who cried wolf\n\nVillagers make Type 1 error (false positive)\nType 2 error (false negative)\nIn that order"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#quantifying-effect-size",
    "href": "slides/10_hypothesis-testing-pt-2.html#quantifying-effect-size",
    "title": "PSYC BC1101",
    "section": "Quantifying effect size",
    "text": "Quantifying effect size\n\nOne measure: Cohen’s \\(d\\)\n\nQuantifies the absolute magnitude of a treatment effect, independent of sample size\nMeasures effect size in terms of standard deviation\n\\(d = 1.00\\): treatment changed \\(\\mu\\) by 1 SD\n\n\n\\[\\text{Cohen's } d = \\dfrac{\\text{mean difference}}{\\text{standard deviation}}\n= \\dfrac{\\mu_{treatment} - \\mu_{no \\ treatment}}{\\sigma}\\]\nFor \\(z\\)-tests:\n\\[\\text{Estimated Cohen's }d = \\dfrac{\\text{mean difference}}{\\text{standard deviation}}\n= \\dfrac{M - \\mu}{\\sigma}\\]"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#interpreting-cohens-d",
    "href": "slides/10_hypothesis-testing-pt-2.html#interpreting-cohens-d",
    "title": "PSYC BC1101",
    "section": "Interpreting Cohen’s \\(d\\)",
    "text": "Interpreting Cohen’s \\(d\\)\n\n\n\nCohen’s rules of thumb\n\n\n\n\n\n\n\\(d\\)\nInterpretation\n\n\n\n\n0.2\nSmall\n\n\n0.5\nMedium\n\n\n0.8\nLarge"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#effect-size-sample-size",
    "href": "slides/10_hypothesis-testing-pt-2.html#effect-size-sample-size",
    "title": "PSYC BC1101",
    "section": "Effect size & sample size",
    "text": "Effect size & sample size\n\nSAT scores: \\(\\mu = 500; \\sigma = 100\\)\n\nAdminister treatment (banana); \\(M = 501\\)\nSignificant? \\((\\alpha = .05\\) two-tailed; critical values \\(z = \\pm 1.96)\\)\nSubstantial? (effect size)\n\n\n\n\nWith 50 participants…\n\\[z = \\dfrac{501 - 500}{100 / \\sqrt{50}} = 0.06 \\\\\nd = \\dfrac{501 - 500}{100} = 0.01\\]\n\nWith 50,000 participants…\n\\[z = \\dfrac{501 - 500}{100 / \\sqrt{50000}} = 2.22\\\\\nd = \\dfrac{501 - 500}{100} = 0.01\\]"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#power-interactive",
    "href": "slides/10_hypothesis-testing-pt-2.html#power-interactive",
    "title": "PSYC BC1101",
    "section": "Power interactive",
    "text": "Power interactive\n\n\nPopulation characteristics\nμ =  \nσ =  \n\n\nExperiment parameters\nd = \nn = \n\\(\\alpha\\) = Two-tailed\n\nσM = \n\n\n\nDiagram options\nShow \\(H_1\\)\n\\(X\\)-axis:\nRaw scores \\(H_0\\) \\(z\\)-scores \\(H_1\\) \\(z\\)-scores\n\n\n\n\n\n\n\\(\\beta =\\) \nPower:"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#influences",
    "href": "slides/10_hypothesis-testing-pt-2.html#influences",
    "title": "PSYC BC1101",
    "section": "Influences",
    "text": "Influences\n\nFactors that influence power\n\nSee: http://rpsychologist.com/d3/NHST/\n\nEffect size\n\nLarger effect size; greater power\n\nSample size\n\nLarger sample size; greater power\n\nAlpha level\n\nLowering alpha (making the test more stringent) reduces power\n\nDirectional hypothesis\n\nUsing a one-tailed (directional) test increases power (relative to a two-tailed test)"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#using-statistical-power",
    "href": "slides/10_hypothesis-testing-pt-2.html#using-statistical-power",
    "title": "PSYC BC1101",
    "section": "Using statistical power",
    "text": "Using statistical power\n\nPower should be estimated before starting study\n\nUsing known quantities\nOr, more often, making assumptions about factors that influence power\n\nDetermining whether a research study is likely to be successful\n\nSpecify effect size, \\(n\\), \\(\\alpha\\); calculate power\n\nFiguring out how many participants you need\n\nSpecify desired power (e.g. .8), expected effect size, \\(\\alpha\\)\nCalculate required sample size"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#power-sample-sizes",
    "href": "slides/10_hypothesis-testing-pt-2.html#power-sample-sizes",
    "title": "PSYC BC1101",
    "section": "Power & sample sizes",
    "text": "Power & sample sizes\n\n\n\n\n\nGrouping variable\nDependent Variable\n\\(d\\)\nRequired \\(n\\)\n\n\n\n\nGender\nHeight\n1.85\n6\n\n\nLiberal / Conservative\nHow important is social equality?\n0.69\n34\n\n\nDo you like eggs? [yes / no]\nHow often do you eat egg salad?\n0.58\n48\n\n\nAre you a smoker? [yes / no]\nWhat is the likelihood of a smoker dying from a smoking-related illness?\n0.33\n144\n\n\nDo you prefer science or art?\nHow many planets can you name correctly?\n0.07\n3669\n\n\n\n\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2013, January). Life after p-hacking. In Meeting of the society for personality and social psychology, New Orleans, LA (pp. 17-19). http://dx.doi.org/10.2139/ssrn.2205186"
  },
  {
    "objectID": "slides/10_hypothesis-testing-pt-2.html#low-power",
    "href": "slides/10_hypothesis-testing-pt-2.html#low-power",
    "title": "PSYC BC1101",
    "section": "Low power",
    "text": "Low power\n\n\nRunning a study with low statistical power is like setting out to look for distant galaxies with a pair of binoculars: even if what you’re looking for is definitely out there, you have essentially no chance of seeing it.\n\n\nStuart Ritchie, Science Fictions"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Jan 20\n\n\n\n  \n    \n\n    \n\n    \n      \nTextbook\nLectures\nRecitation\nExams\nOther stuff\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#course-overview",
    "href": "lectures.html#course-overview",
    "title": "Lectures",
    "section": "",
    "text": "Jan 20\n\n\n\n  \n    \n\n    \n\n    \n      \nTextbook\nLectures\nRecitation\nExams\nOther stuff\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#variables",
    "href": "lectures.html#variables",
    "title": "Lectures",
    "section": "2. Variables",
    "text": "2. Variables\n\n\nJan 22\n\n\n\n  \n    \n\n    \n\n    \n      \nStatistics: Why? How? What?\nMeasuring things\nPopulations & samples\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#frequency",
    "href": "lectures.html#frequency",
    "title": "Lectures",
    "section": "3. Frequency",
    "text": "3. Frequency\n\n\nJan 27\n\n\n\n  \n    \n\n    \n\n    \n      \nFrequency\nFrequency tables\nFrequency graphs\nLearning checks\n\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#central-tendency",
    "href": "lectures.html#central-tendency",
    "title": "Lectures",
    "section": "4. Central Tendency",
    "text": "4. Central Tendency\n\n\nJan 29\n\n\n\n  \n    \n\n    \n\n    \n      \nMode\nMedian\nMean\nDistributions\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#variability",
    "href": "lectures.html#variability",
    "title": "Lectures",
    "section": "5. Variability",
    "text": "5. Variability\n\n\nFeb 03\n\n\n\n  \n    \n\n    \n\n    \n      \nVariability\nRange\nSum of squares, variance, SD\nDegrees of freedom\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#z-scores",
    "href": "lectures.html#z-scores",
    "title": "Lectures",
    "section": "6. \\(z\\)-Scores",
    "text": "6. \\(z\\)-Scores\n\n\nFeb 05\n\n\n\n  \n    \n\n    \n\n    \n      \n\\(z\\)-scores\nStandardized distributions\n\\(z\\)-scores & making inferences\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#probability",
    "href": "lectures.html#probability",
    "title": "Lectures",
    "section": "7. Probability",
    "text": "7. Probability\n\n\nFeb 10\n\n\n\n  \n    \n\n    \n\n    \n      \nProbability basics\nSampling\nProbability and distributions\nProbability and \\(z\\)-scores\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#sampling",
    "href": "lectures.html#sampling",
    "title": "Lectures",
    "section": "8. Sampling",
    "text": "8. Sampling\n\n\nFeb 12\n\n\n\n  \n    \n\n    \n\n    \n      \nSampling error\nDistribution of sample means\nCentral Limit Theorem\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#hypothesis-testing",
    "href": "lectures.html#hypothesis-testing",
    "title": "Lectures",
    "section": "9. Hypothesis testing",
    "text": "9. Hypothesis testing\n\n\nFeb 24\n\n\n\n  \n    \n\n    \n\n    \n      \nRecap\nMaking inferences\nHypothesis testing\n\\(z\\)-test\nLearning check\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#hypothesis-testing-pt.-2",
    "href": "lectures.html#hypothesis-testing-pt.-2",
    "title": "Lectures",
    "section": "10. Hypothesis testing pt. 2",
    "text": "10. Hypothesis testing pt. 2\n\n\nFeb 26\n\n\n\n  \n    \n\n    \n\n    \n      \nInferential errors\nEffect size\nStatistical power\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#the-t-test",
    "href": "lectures.html#the-t-test",
    "title": "Lectures",
    "section": "11. The \\(t\\) test",
    "text": "11. The \\(t\\) test\n\n\nMar 03\n\n\n\n  \n    \n\n    \n\n    \n      \n\\(t\\) vs. \\(z\\)\nThe \\(t\\) distribution\nThe \\(t\\)-test\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#the-t-test-part-2",
    "href": "lectures.html#the-t-test-part-2",
    "title": "Lectures",
    "section": "12. The \\(t\\) test part 2",
    "text": "12. The \\(t\\) test part 2\n\n\nMar 05\n\n\n\n  \n    \n\n    \n\n    \n      \nResearch designs\nAssumptions\nEffect size\nConfidence intervals\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#independent-samples-t-test",
    "href": "lectures.html#independent-samples-t-test",
    "title": "Lectures",
    "section": "13. Independent-samples \\(t\\) test",
    "text": "13. Independent-samples \\(t\\) test\n\n\nMar 10\n\n\n\n  \n    \n\n    \n\n    \n      \nResearch design\nCalculation\nHypothesis test\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#related-samples-t-test",
    "href": "lectures.html#related-samples-t-test",
    "title": "Lectures",
    "section": "14. Related-samples \\(t\\) test",
    "text": "14. Related-samples \\(t\\) test\n\n\nMar 12\n\n\n\n  \n    \n\n    \n\n    \n      \nResearch design\nEquations\nHypothesis test\nAssumptions\nConfidence interval\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#anova",
    "href": "lectures.html#anova",
    "title": "Lectures",
    "section": "15. ANOVA",
    "text": "15. ANOVA\n\n\nMar 31\n\n\n\n  \n    \n\n    \n\n    \n      \nIntro to ANOVA\nUses of ANOVA\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#anova-pt.-2",
    "href": "lectures.html#anova-pt.-2",
    "title": "Lectures",
    "section": "16. ANOVA pt. 2",
    "text": "16. ANOVA pt. 2\n\n\nApr 02\n\n\n\n  \n    \n\n    \n\n    \n      \nANOVA terminology\nCalculating ANOVA\nHypothesis test\nPost-hoc tests\nAssumptions\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#related-samples-anova",
    "href": "lectures.html#related-samples-anova",
    "title": "Lectures",
    "section": "17. Related-samples ANOVA",
    "text": "17. Related-samples ANOVA\n\n\nApr 07\n\n\n\n  \n    \n\n    \n\n    \n      \nLogic\nCalculations\nPost-hoc tests\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#factorial-anova",
    "href": "lectures.html#factorial-anova",
    "title": "Lectures",
    "section": "18. Factorial ANOVA",
    "text": "18. Factorial ANOVA\n\n\nApr 09\n\n\n\n  \n    \n\n    \n\n    \n      \nLogic\nCalculations\nHypothesis test\nInterpretation\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#correlation",
    "href": "lectures.html#correlation",
    "title": "Lectures",
    "section": "19. Correlation",
    "text": "19. Correlation\n\n\nApr 14\n\n\n\n  \n    \n\n    \n\n    \n      \nCorrelational research designs\nCorrelation statistic\nHypothesis test\nCorrelation & effect size\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "lectures.html#regression",
    "href": "lectures.html#regression",
    "title": "Lectures",
    "section": "20. Regression",
    "text": "20. Regression\n\n\nApr 16\n\n\n\n  \n    \n\n    \n\n    \n      \nPurpose\nEquations\nHypothesis test\nLearning checks\n\n    \n  \n\n  \nSlidesPanopto"
  },
  {
    "objectID": "recitation/problem-set-7-instructions.html",
    "href": "recitation/problem-set-7-instructions.html",
    "title": "Problem Set 7",
    "section": "",
    "text": "When conducting a single-sample \\(t\\)-test using the t.test() function, you used the argument x = to specify a single column of a data.frame, and mu = to specify the known population mean against which the sample data was to be compared. To conduct an independent-samples \\(t\\)-test, you need to specify both x and y. These refer to your two samples, each contained in a separate column of a data.frame. By default, mu = 0, which reflects the null hypothesis of no difference between the group means. Since it’s the default, you don’t need to set it explicitly.\nYou also need to use the argument var.equal = TRUE to tell R that we are assuming homogeneity of variances and we don’t want to apply the correction discussed in the lecture. E.g. t.test(x = sample1, y = sample2, var.equal = TRUE). By default, R assumes the assumption is not met (i.e., var.equal = FALSE). This is sensible, and if you run the function this way it’s fine. You’ll just get slightly different answers from someone who specifies var.equal = TRUE.\nRemember you can run a line of code that says ?t.test to get help using the function.\nTo find effect size (specifically Cohen’s \\(d\\)), you can find the answer mathematically using the equations discussed in the lecture. Or you can use the cohens_d() function from the package effectsize.\n\n\n\nIf your data is in the same structure we’ve seen in previous questions for independent-samples \\(t\\)-tests - i.e., a data.frame containing 2 columns for 2 samples of data - switching from an independent-samples \\(t\\)-test to a related-samples \\(t\\)-test is simple. The only difference is that we specify the argument paired = TRUE. That’s all there is to it: we’re just telling R that each row came from a single participant, rather than from different people (as with an independent-samples design). This is a good illustration of how the statistics that we can run on a set of data aren’t an inherent feature of the data themselves, but depend on the conceptual nature of the data. R doesn’t know about that, so it is our job to tell it how to treat the data.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "recitation/problem-set-7-instructions.html#instructions",
    "href": "recitation/problem-set-7-instructions.html#instructions",
    "title": "Problem Set 7",
    "section": "",
    "text": "When conducting a single-sample \\(t\\)-test using the t.test() function, you used the argument x = to specify a single column of a data.frame, and mu = to specify the known population mean against which the sample data was to be compared. To conduct an independent-samples \\(t\\)-test, you need to specify both x and y. These refer to your two samples, each contained in a separate column of a data.frame. By default, mu = 0, which reflects the null hypothesis of no difference between the group means. Since it’s the default, you don’t need to set it explicitly.\nYou also need to use the argument var.equal = TRUE to tell R that we are assuming homogeneity of variances and we don’t want to apply the correction discussed in the lecture. E.g. t.test(x = sample1, y = sample2, var.equal = TRUE). By default, R assumes the assumption is not met (i.e., var.equal = FALSE). This is sensible, and if you run the function this way it’s fine. You’ll just get slightly different answers from someone who specifies var.equal = TRUE.\nRemember you can run a line of code that says ?t.test to get help using the function.\nTo find effect size (specifically Cohen’s \\(d\\)), you can find the answer mathematically using the equations discussed in the lecture. Or you can use the cohens_d() function from the package effectsize.\n\n\n\nIf your data is in the same structure we’ve seen in previous questions for independent-samples \\(t\\)-tests - i.e., a data.frame containing 2 columns for 2 samples of data - switching from an independent-samples \\(t\\)-test to a related-samples \\(t\\)-test is simple. The only difference is that we specify the argument paired = TRUE. That’s all there is to it: we’re just telling R that each row came from a single participant, rather than from different people (as with an independent-samples design). This is a good illustration of how the statistics that we can run on a set of data aren’t an inherent feature of the data themselves, but depend on the conceptual nature of the data. R doesn’t know about that, so it is our job to tell it how to treat the data.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "recitation/problem-set-7-instructions.html#instructions-1",
    "href": "recitation/problem-set-7-instructions.html#instructions-1",
    "title": "Problem Set 7",
    "section": "Instructions",
    "text": "Instructions\n\nUsing t.test() with long-format data\nHere you’ll conduct and report a complete analysis of some more real(ish) data. (Actual raw data from Loftus and Palmer’s (1974) study is not available, but I’ve simulated data with comparable characteristics.)\nTo make it easier to describe and visualize the data, you’ll turn the data from “wide” to “long” format (using pivot_longer() as we’ve done in past problem sets).\nWhen the data is in long format, the t.test() function works a little differently. Rather than specifying x and y, two columns in a data.frame as you did in Part 1, you specify x as a ‘formula’. Generically, this formula is in the form DV ~ IV, meaning you want to use a \\(t\\) test to look for differences on the dependent variable by groups of the independent variable. Of course, in practice those terms will be names of columns in a data.frame, so the function also requires a data = argument to specify the data.frame containing those columns.\nSo suppose you have a long-format data.frame named df_long with two columns, one named group containing the categorical group each observation comes from and another named score containing the numeric scores on the dependent variable:\n\nlibrary(tidyr)\n\ndf_wide &lt;- data.frame(group_a = c(3, 5, 2),\n                      group_b = c(4, 6, 4))\n\ndf_long &lt;- df_wide |&gt; \n  pivot_longer(everything(),\n               names_to = \"group\",\n               values_to = \"score\")\n\nt.test(score ~ group, data = df_long)\n\n\n    Welch Two Sample t-test\n\ndata:  score by group\nt = -1.206, df = 3.7231, p-value = 0.2988\nalternative hypothesis: true difference in means between group group_a and group group_b is not equal to 0\n95 percent confidence interval:\n -4.494980  1.828313\nsample estimates:\nmean in group group_a mean in group group_b \n             3.333333              4.666667 \n\n\nNote that t.test(x = df_wide$group_a, y = df_wide$group_b) would give exactly the same answer. However, this ‘formula’ approach with long-format data becomes extremely useful when analyzing data with more groups, as in ANOVA designs 😉\nRemember, the t.test() function doesn’t include effect size, so that has to be computed separately. Luckily, the effectsize::cohens_d() function allows for the same kind of formula ast.test():\n\neffectsize::cohens_d(score ~ group, data = df_long)\n\nWarning: 'y' is numeric but has only 2 unique values.\n  If this is a grouping variable, convert it to a factor.\n\n\nCohen's d |        95% CI\n-------------------------\n-0.98     | [-2.67, 0.80]\n\n- Estimated using pooled SD.\n\n\n\n\nConfidence intervals for independent-samples\nWhen you use the t.test() function, a confidence interval is included in its output. That confidence interval pertains to the mean difference, i.e. the margin of error around the difference between group means. I won’t ask you to calculate it manually here, but if you’re unsure what it quantifies it would be worth thinking about how it is computed and seeing if you can replicate it by implementing the relevant equation like you did last time for a single-sample design. (As a hint, it necessitates finding the pooled variance and estimated standard error of the mean difference.)\nHowever, as part of describing and visualizing the data I will ask you to compute two different confidence intervals: one for each group of scores (as you have done previously). Note that those confidence intervals quantify something different from the one produced by the t.test() function in this case: the margin of error associated with each individual group point estimate.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 7"
    ]
  },
  {
    "objectID": "recitation/problem-set-3-instructions.html",
    "href": "recitation/problem-set-3-instructions.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "Suppose we have a set of values: 1, 2, 3, 3, 4, 5, 10\nThere are several ways you could find the mean. One way would be to add the scores and divide by \\(N\\), like you would on paper.\n\n(1 + 2 + 3 + 3 + 4 + 5 + 10) / 7\n\n[1] 4\n\n\nOf course, you don’t want to have to type all those numbers and figure out \\(N\\) yourself (especially if you were dealing with a much larger set of numbers). Better to have the scores as a named object:\n\nscores &lt;- c(1, 2, 3, 3, 4, 5, 10)\n\nNow life is much easier. The same mathematical approach can be implemented with the sum() and length() functions.\n\nsum(scores) / length(scores) # this is equivalent to Sigma X divided by N\n\n[1] 4\n\n\nEven better, R has built-in functions to easily find the mean and median of a set of scores.\n\nmean(scores)\n\n[1] 4\n\nmedian(scores)\n\n[1] 3\n\n\nYou’d think there’d be one for finding the mode as well…\n\nmode(scores)\n\n[1] \"numeric\"\n\n\nBut that actually does something different.\nSince the mode is a common statistic, though, somebody already wrote a function to compute it. It’s in a package called modeest. The package contains the function mfv1(), for “most frequent value”, which we can now use to get the mode. Remember, since it’s an external package, we need to activate it with the library() function.\n\nlibrary(modeest)\nmfv1(scores)\n\n[1] 3\n\n\nOr equivalently, you can use the double-colon operator to refer to a package::its_function() without a separate library() call:\n\nmodeest::mfv1(scores)\n\n[1] 3\n\n\n\n\n\nThere are functions to find the minimum and maximum value in a set of scores. The range is the difference between those:\n\nmin(scores)\n\n[1] 1\n\nmax(scores)\n\n[1] 10\n\nmax(scores) - min(scores) # range\n\n[1] 9\n\n\nThere is a built-in range() function. It returns the actual lowest and highest values as a collection. To find the difference between them, you could nest the range inside the diff() function.\n\nrange(scores)\n\n[1]  1 10\n\ndiff(range(scores))\n\n[1] 9\n\n\nR has a built-in functions for sample SD.\n\nsd(scores)\n\n[1] 2.94392\n\n\nThere’s also a var() function to compute the variance. Remember that standard deviation is the square-root of variance, so if we square the SD it should give the same answer as var()…\n\nvar(scores)\n\n[1] 8.666667\n\nsd(scores)^2\n\n[1] 8.666667\n\n\nNeato!\nNote that these functions return sample variance & SD. There are no built-in functions for population variance & SD so if you wanted to find them you would have to use the mathematical procedure outlined in the lecture.\n\n\n\nThe above instructions show how mathematical operations and functions can be used to compute summary statistics for a set of numbers. Most often, however, the set of numbers we’re dealing with is a column in a data.frame.\n\ndf &lt;- data.frame(scores = c(1, 2, 3, 3, 4, 5, 10))\n\nYou can use the same approaches as above, just remember the $ notation to refer to a data.frame column:\n\nmean(df$scores)\n\n[1] 4\n\nsd(df$scores)\n\n[1] 2.94392\n\n\nHowever, dplyr has a powerful summarize() function that fits in nicely with the pipe operator.\n\nlibrary(dplyr)\n\ndf |&gt; \n  summarize(mean = mean(scores),\n            median = median(scores),\n            mode = modeest::mfv1(scores),\n            min = min(scores),\n            max = max(scores),\n            range = max - min,\n            sd = sd(scores))\n\n  mean median mode min max range      sd\n1    4      3    3   1  10     9 2.94392\n\n\nIn a single pipeline we produce all the summary statistics as a data.frame! Note that because I pipe df into the summarize() function, scores refers specifically to that column in the df data.frame object rather than the separate scores object I created previously. Note also that I defined the min and max summary columns, and I was able to refer to those new columns within the same single mutate() function to compute the range.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "recitation/problem-set-3-instructions.html#instructions",
    "href": "recitation/problem-set-3-instructions.html#instructions",
    "title": "Problem Set 3",
    "section": "",
    "text": "Suppose we have a set of values: 1,2,3,3,4,5,10\nThere are several ways you could find the mean. One way would be to add the scores and divide by \\(N\\), like you would on paper.\n\n(1 + 2 + 3 + 3 + 4 + 5 + 10) / 7\n\n[1] 4\n\n\nOf course, you don’t want to have to type all those numbers and figure out N yourself (especially if you were dealing with a much larger set of numbers). Better to have the scores as a named object:\n\nscores &lt;- c(1,2,3,3,4,5,10)\n\nNow life is much easier. The same mathematical approach can be implemented with the sum() and length() functions.\n\nsum(scores)/length(scores) # this is equivalent to Sigma X divided by N\n\n[1] 4\n\n\nEven better, R has built-in functions to easily find the mean and median of a set of scores.\n\nmean(scores)\n\n[1] 4\n\nmedian(scores)\n\n[1] 3\n\n\nYou’d think there’d be one for finding the mode as well…\n\nmode(scores)\n\n[1] \"numeric\"\n\n\nBut that actually does something different.\nSince the mode is a common statistic, though, somebody already wrote a function to compute it. It’s in a package called modeest. The package contains the function mfv(), for “most frequent value”, which we can now use to get the mode. Remember, since it’s an external package, we need to activate it with the library() function.\n\nlibrary(modeest)\nmfv(scores)\n\n[1] 3\n\n\nOr equivalently, you can use the double-colon operator to refer to a package::its_function() without a separate library() call:\n\nmodeest::mfv(scores)\n\n[1] 3\n\n\n\n\n\nThere are functions to find the minimum and maximum value in a set of scores. The range is the difference between those:\n\nmin(scores)\n\n[1] 1\n\nmax(scores)\n\n[1] 10\n\nmax(scores) - min(scores) # range\n\n[1] 9\n\n\nR has a built-in functions for sample SD.\n\nsd(scores)\n\n[1] 2.94392\n\n\nThere’s also a var() function to compute the variance. Remember that standard deviation is the square-root of variance, so if we square the SD it should give the same answer as var()…\n\nvar(scores)\n\n[1] 8.666667\n\nsd(scores)^2\n\n[1] 8.666667\n\n\nNeato!\nNote that these functions return sample variance & SD. There are no built-in functions for population variance & SD so if you wanted to find them you would have to use the mathematical procedure outlined in the lecture.\n\n\n\nThe above instructions show how mathematical operations and functions can be used to compute summary statistics for a set of numbers. Most often, however, the set of numbers we’re dealing with is a column in a data.frame.\n\ndf &lt;- data.frame(scores = c(1,2,3,3,4,5,10))\n\nYou can use the same approaches as above, just remember the $ notation to refer to a data.frame column:\n\nmean(df$scores)\n\n[1] 4\n\nsd(df$scores)\n\n[1] 2.94392\n\n\nHowever, dplyr has a powerful summarize() function that fits in nicely with the pipe operator.\n\nlibrary(dplyr)\n\ndf |&gt; \n  summarize(mean = mean(scores),\n            median = median(scores),\n            mode = modeest::mfv(scores),\n            min = min(scores),\n            max = max(scores),\n            range = max - min,\n            sd = sd(scores))\n\n  mean median mode min max range      sd\n1    4      3    3   1  10     9 2.94392\n\n\nIn a single pipeline we produce all the summary statistics as a data.frame! Note that because I pipe df into the summarize() function, scores refers specifically to that column in the df data.frame object rather than the separate scores object I created previously. Note also that I defined the min and max summary columns, and I was able to refer to those new columns within the same single mutate() function to compute the range.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "recitation/problem-set-9-instructions.html",
    "href": "recitation/problem-set-9-instructions.html",
    "title": "Problem Set 9",
    "section": "",
    "text": "When analyzing data from a 2x2 factorial design, the steps are:\n\nProduce descriptive statistics (marginal and cell means, SDs, CIs.\nMake a graph showing those descriptives.\nCompute the ANOVA model\nIf any element of the model is ‘significant’, compute effect size and run post-hoc tests.\n\n\n\nThere are several ways to get descriptive statistics for 2x2 ANOVA designs. One is to use the summarize function from the dplyr package, which you’re hopefully feeling familiar with by now. The only difference here for a 2x2 design is that you have 2 grouping variables. Happily, we can specify them both by collecting them together (using c()) as the function’s .by argument.\n\nlibrary(dplyr)\n\ncell_means &lt;- my_dataframe |&gt; \n  summarize(mean = mean(dv_column),\n            sd = sd(dv_column),\n            ci = ci(dv_column),\n            .by = c(iv_one_column, iv_two_column))\n\nWith both grouping variables we get all 4 ‘cell means’. To find the ‘marginal means’ for each IV, you would do the same thing but with just one grouping variable at a time.\n\nmarginal_means_iv_one &lt;- summarize(my_dataframe, \n                                   mean = mean(dv_column),\n                                   sd = sd(dv_column),\n                                   ci = ci(dv_column),\n                                   .by = iv_one_column)\n\nmarginal_means_iv_two &lt;- summarize(my_dataframe, \n                                   mean = mean(dv_column),\n                                   sd = sd(dv_column),\n                                   ci = ci(dv_column),\n                                   .by = iv_two_column)\n\nAlso notice that I’m using a ci() function to compute the 95% confidence interval. Remember, it’s not built into R; it’s one that I made back in Problem Set 6! You’ll need to copy and paste that code into your problem set here before trying to use it.\n\nci &lt;- function(x) {\n  # code to compute CI goes here!\n}\n\ncell_means &lt;- my_dataframe |&gt; \n  summarize(...,\n            ci = ci(dv_column),\n            ...)\n\n\n\n\nA nice thing about getting summary statistics using summarize() is that it returns the summary data as a data.frame, so if you assigned that output to a name (as I did above!) you can make use of it, for example to visualize the summary statistics as a 2x2 plot.\n\nlibrary(ggplot2)\n\n# bar graph\ncell_means |&gt; \n  ggplot(aes(x = iv_one_column, \n             y = summary_variable, \n             fill = iv_two_column)) +\n  geom_col(position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean - ci, ymax = mean + ci), \n                position = position_dodge(width = 0.9), width = 0.1)\n\n# line graph\ncell_means |&gt; \n  ggplot(aes(x = iv_one_column, \n             y = summary_variable, \n             color = iv_two_column, \n             group = iv_two_column)) +\n  geom_point() +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean - ci, ymax = mean + ci), width = 0.1)\n\nHere as the aesthetics we assign one IV to the x-axis, the second IV as both color and group, and the DV is assigned to the y-axis.\n\n\n\nTo analyze a two-factor design, use aov() and summary() just like with a one-way design. The difference is that in the formula argument you will specify both your IVs, separated by an asterisk, meaning that you are interested in the effect of both variables and the interaction between them.\n\nmodel &lt;- aov(formula = dv_column ~ iv_one_column * iv_two_column, \n             data = my.data)\nsummary(model)\n\n\n\n\nThe etaSquared() function from the lsr package is the easiest way to obtain effect size for a 2x2 ANOVA.\n\neffectsize::eta_squared(model)\n\n\n\n\nPost-hoc tests work the same way for a two-factor design as a single-factor design. Just feed your aov() analysis into the TukeyHSD() function (as long as it’s a between-participants design). The output shows you every possible comparison of conditions in the data, and states which are significantly different from one another.\n\nTukeyHSD(model)",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 9"
    ]
  },
  {
    "objectID": "recitation/problem-set-9-instructions.html#instructions",
    "href": "recitation/problem-set-9-instructions.html#instructions",
    "title": "Problem Set 9",
    "section": "",
    "text": "When analyzing data from a 2x2 factorial design, the steps are:\n\nProduce descriptive statistics (marginal and cell means, SDs, CIs.\nMake a graph showing those descriptives.\nCompute the ANOVA model\nIf any element of the model is ‘significant’, compute effect size and run post-hoc tests.\n\n\n\nThere are several ways to get descriptive statistics for 2x2 ANOVA designs. One is to use the summarize function from the dplyr package, which you’re hopefully feeling familiar with by now. The only difference here for a 2x2 design is that you have 2 grouping variables. Happily, we can specify them both by collecting them together (using c()) as the function’s .by argument.\n\nlibrary(dplyr)\n\ncell_means &lt;- my_dataframe |&gt; \n  summarize(mean = mean(dv_column),\n            sd = sd(dv_column),\n            ci = ci(dv_column),\n            .by = c(iv_one_column, iv_two_column))\n\nWith both grouping variables we get all 4 ‘cell means’. To find the ‘marginal means’ for each IV, you would do the same thing but with just one grouping variable at a time.\n\nmarginal_means_iv_one &lt;- summarize(my_dataframe, \n                                   mean = mean(dv_column),\n                                   sd = sd(dv_column),\n                                   ci = ci(dv_column),\n                                   .by = iv_one_column)\n\nmarginal_means_iv_two &lt;- summarize(my_dataframe, \n                                   mean = mean(dv_column),\n                                   sd = sd(dv_column),\n                                   ci = ci(dv_column),\n                                   .by = iv_two_column)\n\nAlso notice that I’m using a ci() function to compute the 95% confidence interval. Remember, it’s not built into R; it’s one that I made back in Problem Set 6! You’ll need to copy and paste that code into your problem set here before trying to use it.\n\nci &lt;- function(x) {\n  # code to compute CI goes here!\n}\n\ncell_means &lt;- my_dataframe |&gt; \n  summarize(...,\n            ci = ci(dv_column),\n            ...)\n\n\n\n\nA nice thing about getting summary statistics using summarize() is that it returns the summary data as a data.frame, so if you assigned that output to a name (as I did above!) you can make use of it, for example to visualize the summary statistics as a 2x2 plot.\n\nlibrary(ggplot2)\n\n# bar graph\ncell_means |&gt; \n  ggplot(aes(x = iv_one_column, \n             y = summary_variable, \n             fill = iv_two_column)) +\n  geom_col(position = position_dodge()) +\n  geom_errorbar(aes(ymin = mean - ci, ymax = mean + ci), \n                position = position_dodge(width = 0.9), width = 0.1)\n\n# line graph\ncell_means |&gt; \n  ggplot(aes(x = iv_one_column, \n             y = summary_variable, \n             color = iv_two_column, \n             group = iv_two_column)) +\n  geom_point() +\n  geom_line() +\n  geom_errorbar(aes(ymin = mean - ci, ymax = mean + ci), width = 0.1)\n\nHere as the aesthetics we assign one IV to the x-axis, the second IV as both color and group, and the DV is assigned to the y-axis.\n\n\n\nTo analyze a two-factor design, use aov() and summary() just like with a one-way design. The difference is that in the formula argument you will specify both your IVs, separated by an asterisk, meaning that you are interested in the effect of both variables and the interaction between them.\n\nmodel &lt;- aov(formula = dv_column ~ iv_one_column * iv_two_column, \n             data = my.data)\nsummary(model)\n\n\n\n\nThe etaSquared() function from the lsr package is the easiest way to obtain effect size for a 2x2 ANOVA.\n\neffectsize::eta_squared(model)\n\n\n\n\nPost-hoc tests work the same way for a two-factor design as a single-factor design. Just feed your aov() analysis into the TukeyHSD() function (as long as it’s a between-participants design). The output shows you every possible comparison of conditions in the data, and states which are significantly different from one another.\n\nTukeyHSD(model)",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 9"
    ]
  },
  {
    "objectID": "recitation/problem-set-5-instructions.html",
    "href": "recitation/problem-set-5-instructions.html",
    "title": "Problem Set 5",
    "section": "",
    "text": "Cohen’s \\(d\\) is simple to calculate: It is the mean difference divided by standard deviation.\nSo if you have a population with mean 100 and standard deviation 15, and you administer a treatment which produces a sample mean of 106, Cohen’s \\(d\\) would be:\n\n(106 - 100) / 15\n\n[1] 0.4\n\n\n\n\n\nSuppose we’re measuring reaction times in milliseconds. A sample \\(n = 40\\) is selected from a normally-distributed population with \\(\\mu = 75\\) and \\(\\sigma = 12\\), and a treatment is administered to the sample. The treatment is expected to increase scores by an average of \\(4\\) msec. We plan to use a two-tailed hypothesis test using \\(\\alpha = .05\\).\nStep 1: Calculate the standard error of the mean\n\nstd_err &lt;- 12/sqrt(40)\n\nStep 2: multiply the critical \\(z\\)-scores by the standard error of the mean and add to the population mean to find the range of means that would produce a Type II error. We can put two values into the qnorm() function using the c() function. This way, we will get both of the means we’re looking for using a single line of code.\n\nrange &lt;- 75 + qnorm(c(.025, .975)) * std_err \n\nrange\n\n[1] 71.28123 78.71877\n\n\n…which tells us that any mean between 71.28 and 78.72 would lead us to fail to reject the null hypothesis\nstep 3: Calculate beta. Assuming that the treatment worked and thus the new mean is 79, determine what proportion of sample means fall within that range.\nOne way would be to subtract the lower value from the higher value\n\nbeta &lt;- pnorm(78.72, mean = 79, sd = std_err) - pnorm(71.28, mean = 79, sd = std_err) \n\nbeta\n\n[1] 0.4413163\n\n\nA slightly easier way would be to use the diff() function. If you feed two numbers into the diff function, it will figure out the difference between them. You should get approximately the same answer. (Since the first method involved rounding the numbers along the way, there will be a slight difference.)\n\nareas &lt;- pnorm(range, mean=79, sd=std_err) \n\nbeta &lt;- diff(areas) \n\nbeta\n\n[1] 0.4410604\n\n\nStep 4: Power = 1 - beta\n\n1 - beta\n\n[1] 0.5589396\n\n\nWith a 1-tailed test, you don’t have to deal with the range between 2 means. You should just find the single critical mean, and figure out the probability of obtaining a mean less than that value.\n\nstd_err &lt;- 12/sqrt(40) \n\nm_critical &lt;- 75 + qnorm(.95) * std_err \n\nbeta &lt;- pnorm(m_critical, mean=79, sd=std_err) \n\n1 - beta\n\n[1] 0.6784366\n\n\n\n\n\n\n\n\nTipBonus\n\n\n\n\n\nThe package pwr can do power calculations for you. It’s worth getting to grips with, but if you use it, do the calculations manually as well at least the first couple of times to make sure you’re getting the right answers.\n\nlibrary(pwr) \n\npwr.norm.test(d=(79-75)/12, # d is the (known or anticipated) effect size; (M - mu)/SD \n              n=40, # n is the sample size \n              sig.level = .05, # sig.level is alpha \n              alternative = \"two.sided\") # this is the alternative hypothesis (greater, less, or two.sided)\n\n\n     Mean power calculation for normal distribution with known variance \n\n              d = 0.3333333\n              n = 40\n      sig.level = 0.05\n          power = 0.5589396\n    alternative = two.sided\n\npwr.norm.test(d = (79-75)/12, n = 40, sig.level = .05, alternative = \"greater\") # one-tailed power\n\n\n     Mean power calculation for normal distribution with known variance \n\n              d = 0.3333333\n              n = 40\n      sig.level = 0.05\n          power = 0.6784366\n    alternative = greater",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "recitation/problem-set-5-instructions.html#instructions",
    "href": "recitation/problem-set-5-instructions.html#instructions",
    "title": "Problem Set 5",
    "section": "",
    "text": "Cohen’s \\(d\\) is simple to calculate: It is the mean difference divided by standard deviation.\nSo if you have a population with mean 100 and standard deviation 15, and you administer a treatment which produces a sample mean of 106, Cohen’s \\(d\\) would be:\n\n(106 - 100) / 15\n\n[1] 0.4\n\n\n\n\n\nSuppose we’re measuring reaction times in milliseconds. A sample \\(n = 40\\) is selected from a normally-distributed population with \\(\\mu = 75\\) and \\(\\sigma = 12\\), and a treatment is administered to the sample. The treatment is expected to increase scores by an average of \\(4\\) msec. We plan to use a two-tailed hypothesis test using \\(\\alpha = .05\\).\nStep 1: Calculate the standard error of the mean\n\nstd_err &lt;- 12/sqrt(40)\n\nStep 2: multiply the critical \\(z\\)-scores by the standard error of the mean and add to the population mean to find the range of means that would produce a Type II error. We can put two values into the qnorm() function using the c() function. This way, we will get both of the means we’re looking for using a single line of code.\n\nrange &lt;- 75 + qnorm(c(.025, .975)) * std_err \n\nrange\n\n[1] 71.28123 78.71877\n\n\n…which tells us that any mean between 71.28 and 78.72 would lead us to fail to reject the null hypothesis\nstep 3: Calculate beta. Assuming that the treatment worked and thus the new mean is 79, determine what proportion of sample means fall within that range.\nOne way would be to subtract the lower value from the higher value\n\nbeta &lt;- pnorm(78.72, mean = 79, sd = std_err) - pnorm(71.28, mean = 79, sd = std_err) \n\nbeta\n\n[1] 0.4413163\n\n\nA slightly easier way would be to use the diff() function. If you feed two numbers into the diff function, it will figure out the difference between them. You should get approximately the same answer. (Since the first method involved rounding the numbers along the way, there will be a slight difference.)\n\nareas &lt;- pnorm(range, mean=79, sd=std_err) \n\nbeta &lt;- diff(areas) \n\nbeta\n\n[1] 0.4410604\n\n\nStep 4: Power = 1 - beta\n\n1 - beta\n\n[1] 0.5589396\n\n\nWith a 1-tailed test, you don’t have to deal with the range between 2 means. You should just find the single critical mean, and figure out the probability of obtaining a mean less than that value.\n\nstd_err &lt;- 12/sqrt(40) \n\nm_critical &lt;- 75 + qnorm(.95) * std_err \n\nbeta &lt;- pnorm(m_critical, mean=79, sd=std_err) \n\n1 - beta\n\n[1] 0.6784366\n\n\n\n\n\n\n\n\nTipBonus\n\n\n\n\n\nThe package pwr can do power calculations for you. It’s worth getting to grips with, but if you use it, do the calculations manually as well at least the first couple of times to make sure you’re getting the right answers.\n\nlibrary(pwr) \n\npwr.norm.test(d=(79-75)/12, # d is the (known or anticipated) effect size; (M - mu)/SD \n              n=40, # n is the sample size \n              sig.level = .05, # sig.level is alpha \n              alternative = \"two.sided\") # this is the alternative hypothesis (greater, less, or two.sided)\n\n\n     Mean power calculation for normal distribution with known variance \n\n              d = 0.3333333\n              n = 40\n      sig.level = 0.05\n          power = 0.5589396\n    alternative = two.sided\n\npwr.norm.test(d = (79-75)/12, n = 40, sig.level = .05, alternative = \"greater\") # one-tailed power\n\n\n     Mean power calculation for normal distribution with known variance \n\n              d = 0.3333333\n              n = 40\n      sig.level = 0.05\n          power = 0.6784366\n    alternative = greater",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "recitation/problem-set-5-instructions.html#instructions-1",
    "href": "recitation/problem-set-5-instructions.html#instructions-1",
    "title": "Problem Set 5",
    "section": "Instructions",
    "text": "Instructions\nAs in the lecture, you can think about hypothesis testing as a series of steps to follow\nStep 1: State your hypotheses. R can’t do this for you - you just have to think about the question and formulate your null (\\(H_0\\)) and alternative (\\(H_1\\)) hypotheses.\nStep 2: Set decision criteria. The questions below will tell you what alpha to use. Suppose we’re using \\(\\alpha = .05\\), and a two-tailed test. For a \\(z\\)-test, find the critical values using qnorm().\nFor a one-tailed test, just put in the proportion you want to be your critical region:\n\nqnorm(.05) # If you expect a decrease\n\n[1] -1.644854\n\nqnorm(.95) # If you expect an increase\n\n[1] 1.644854\n\nqnorm(.05, lower.tail = FALSE) # Also works for an increase\n\n[1] 1.644854\n\n\nFor a two-tailed test we have two critical regions, each containing alpha divided by 2. We can feed both into qnorm() at the same time using c(), and it will give us two answers.\n\nqnorm(c(0.025, 0.975))\n\n[1] -1.959964  1.959964\n\n\nStep 3: Calculate your test statistic. Do this by plugging the appropriate values into the relevant equation. (For other tests, there may be R functions you can use, but there’s no built-in function for a \\(z\\)-test)\nStep 4: Decide. Compare your calculated test statistic with the critical values to determine whether your sample is within the critical region(s), and thus whether to reject the null.\n\n\n\n\n\n\nTipBonus\n\n\n\n\n\nThere is no built-in function for doing \\(z\\)-tests using real data, because researchers don’t need to do it very often. For a fun challenge you could try creating your own function.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 5"
    ]
  },
  {
    "objectID": "recitation/problem-set-10-instructions.html",
    "href": "recitation/problem-set-10-instructions.html",
    "title": "Problem Set 10",
    "section": "",
    "text": "R has two basic functions for calculating correlations: cor(), and cor.test() . They each require two arguments, named x and y, referring to the two variables (i.e. columns of data) we want to compute the correlation between.\n\n# first read in the relevant data; for this example it's a spreadsheet with two\n# columns, named x and y\ndf &lt;- read.csv(\"data/10_example.csv\")\n\ncor(df$x, df$y)\n\n[1] -0.09394529\n\ncor.test(df$x, df$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$x and df$y\nt = -0.2669, df = 8, p-value = 0.7963\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.6831623  0.5693588\nsample estimates:\n        cor \n-0.09394529 \n\n\nAs you will see, cor() simply returns the value of Pearson’s \\(r\\). cor.test() returns all the information you need to complete a significance test, i.e. the correlation coefficient, degrees of freedom, and \\(p\\)-value as well as some stuff you can ignore for our purposes, such as \\(t\\) and a confidence interval.\n\n\n\nThe lm() function computes a ‘linear model’, i.e. the kind of linear regression model that we have learned about in the lectures. Rather than accepting x and y arguments, the regression function requires a formula, just like you’ve used with t.test() and aov() in previous problem sets. The idea is the same here: we articulate a formula in the form DV ~ IV. In this context, the DV is the outcome variable, i.e. the one we want to predict. The IV is the predictor. And remember, I’m using DV and IV generically here: when you write your code you will replace them with the names of columns in your data.frame.\nThere is a second step though. Since lm() doesn’t give us all the information about the regression model that we usually need to report, we assign the model (the output of the lm() function) to a name (model, in my case), and then we feed that into the summary() function.\n\nmodel &lt;- lm(y ~ x, data = df)\n\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9279 -1.1678 -0.3557  1.1487  3.2374 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  5.94165    2.02778   2.930    0.019 *\nx           -0.09411    0.35262  -0.267    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.171 on 8 degrees of freedom\nMultiple R-squared:  0.008826,  Adjusted R-squared:  -0.1151 \nF-statistic: 0.07123 on 1 and 8 DF,  p-value: 0.7963\n\n\n\n\n\nTo visualize bivariate correlations, you can use ggplot, specifying your x and y variables as aesthetics, and adding a geom_point() layer for the dots.\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo make this a visualization of the regression model as well, we can simply add another layer to show the best fit line. This is computed by geom_smooth. We just need to tell it the method we want it to use to compute the line. For our purposes, method = \"lm\" meaning “linear model”.\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 10"
    ]
  },
  {
    "objectID": "recitation/problem-set-10-instructions.html#instructions",
    "href": "recitation/problem-set-10-instructions.html#instructions",
    "title": "Problem Set 10",
    "section": "",
    "text": "R has two basic functions for calculating correlations: cor(), and cor.test() . They each require two arguments, named x and y, referring to the two variables (i.e. columns of data) we want to compute the correlation between.\n\n# first read in the relevant data; for this example it's a spreadsheet with two\n# columns, named x and y\ndf &lt;- read.csv(\"data/10_example.csv\")\n\ncor(df$x, df$y)\n\n[1] -0.09394529\n\ncor.test(df$x, df$y)\n\n\n    Pearson's product-moment correlation\n\ndata:  df$x and df$y\nt = -0.2669, df = 8, p-value = 0.7963\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.6831623  0.5693588\nsample estimates:\n        cor \n-0.09394529 \n\n\nAs you will see, cor() simply returns the value of Pearson’s \\(r\\). cor.test() returns all the information you need to complete a significance test, i.e. the correlation coefficient, degrees of freedom, and \\(p\\)-value as well as some stuff you can ignore for our purposes, such as \\(t\\) and a confidence interval.\n\n\n\nThe lm() function computes a ‘linear model’, i.e. the kind of linear regression model that we have learned about in the lectures. Rather than accepting x and y arguments, the regression function requires a formula, just like you’ve used with t.test() and aov() in previous problem sets. The idea is the same here: we articulate a formula in the form DV ~ IV. In this context, the DV is the outcome variable, i.e. the one we want to predict. The IV is the predictor. And remember, I’m using DV and IV generically here: when you write your code you will replace them with the names of columns in your data.frame.\nThere is a second step though. Since lm() doesn’t give us all the information about the regression model that we usually need to report, we assign the model (the output of the lm() function) to a name (model, in my case), and then we feed that into the summary() function.\n\nmodel &lt;- lm(y ~ x, data = df)\n\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9279 -1.1678 -0.3557  1.1487  3.2374 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  5.94165    2.02778   2.930    0.019 *\nx           -0.09411    0.35262  -0.267    0.796  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.171 on 8 degrees of freedom\nMultiple R-squared:  0.008826,  Adjusted R-squared:  -0.1151 \nF-statistic: 0.07123 on 1 and 8 DF,  p-value: 0.7963\n\n\n\n\n\nTo visualize bivariate correlations, you can use ggplot, specifying your x and y variables as aesthetics, and adding a geom_point() layer for the dots.\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo make this a visualization of the regression model as well, we can simply add another layer to show the best fit line. This is computed by geom_smooth. We just need to tell it the method we want it to use to compute the line. For our purposes, method = \"lm\" meaning “linear model”.\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 10"
    ]
  },
  {
    "objectID": "recitation/problem-set-10-instructions.html#instructions-1",
    "href": "recitation/problem-set-10-instructions.html#instructions-1",
    "title": "Problem Set 10",
    "section": "Instructions",
    "text": "Instructions\nFor this question you will work with some real data from the World Happiness Report.\nLook at the data after you read it into R to get an idea of what variables it contains. The column names generally give a clear indication of what the scores are, but some may be unclear or unfamiliar. There’s a full explanation at in the WHR appendix here.\nYou’ll be using cor() and/or cor.test() to examine correlations. The only new thing to be aware of is that, if you try to correlate two variables and there is even a single missing data point (indicated by NA), you will get an answer of NA. To avoid this, you can specify use = \"pairwise.complete.obs as an argument within either function. That will cause R to ignore cases where one data point is missing, and use however many have complete data for both variables.\n\n\n\n\n\n\nTipOr…\n\n\n\n\n\nSince you’re coming to the end of this journey with R, and you’re becoming an expert at doing so many things with it, I want to give you the chance to spread your wings and fly. If you want to put your skills to the test and push your limits, please ignore the following questions and just do something cool with the world happiness data. You might produce a visual ranking the happiness of every country, make a map showing happiness around the world (yes, R can do maps!), graph changes over time (10_world_happiness_all_years.csv has data for multiple years whereas 10_world_happiness_2023.csv is just that one year), or something else that I haven’t even thought of. Maybe chatGPT can help get you started with ideas and code. Prof B would love to help as well. This is your chance to shine!\nOr you can just ignore this and play it safe with the questions below. Your choice; no judgement 😊",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 10"
    ]
  },
  {
    "objectID": "recitation/problem-set-1-instructions.html#code-chunks",
    "href": "recitation/problem-set-1-instructions.html#code-chunks",
    "title": "Problem Set 1",
    "section": "Code chunks",
    "text": "Code chunks\nThis is where we can start using R code to find answers to questions. Using the quarto document format, you include code “chunks” where you will write your code. The code will get executed when you Render the document (or you can execute it interactively to see output right away).\nThe easiest way to insert a code chunk is to put your cursor on a new line and type a forward-slash on your keyboard (/).\nYou should see a pop up showing all the stuff you can put into a document. The first option is an “R code chunk” which is what you need, so you can click it or just press return on the keyboard to insert the chunk. If you prefer, you can also click Insert &gt; Executable Cell &gt; R from the menu at the top of the editor.\nHere’s what a code chunk looks like in the editor:\n{r}\n# Code goes here! This is just a comment. \n\n# Comments are preceded by the # symbol\nNotice the {r} at the top: that means the chunk contains R code.\nYou should thoroughly comment and/or annotate your problem sets with regular text like this, to explain to any reader (including your future self) what your code is doing. (You will forget!)",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "recitation/problem-set-1-instructions.html#using-r-like-a-calculator",
    "href": "recitation/problem-set-1-instructions.html#using-r-like-a-calculator",
    "title": "Problem Set 1",
    "section": "Using R like a calculator",
    "text": "Using R like a calculator\nFor the questions in part 2, you could easily do the sums by hand–and I recommend checking your answers that way. But it is important to learn how to do these kinds of things in R as well. Usually there’s more than one way to achieve something in R.\nIf you want to add up a set of scores, you can use R just like a calculator:\n\n1 + 2 + 3 + 4\n\n[1] 10\n\n\nWhen you “Render”, the code will be executed and you’ll see the answer right after the code, like in these instructions. You can also run the code yourself to see the answer right away. Put your cursor on the line of code and press Ctrl/Command + Enter on your keyboard to run the code. Note that you DON’T type 1 + 2 + 3 + 4 = 10. In fact, that’ll confuse R and you’ll get an error. (Try it)",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "recitation/problem-set-1-instructions.html#assignment-and-functions",
    "href": "recitation/problem-set-1-instructions.html#assignment-and-functions",
    "title": "Problem Set 1",
    "section": "Assignment and functions",
    "text": "Assignment and functions\nRather than just typing the sum out like that, a better way is to assign the scores a name. The &lt;- arrow operator is used to assign names. The name can be anything you want except that it cannot include spaces and cannot start with a number.\n\nscores &lt;- c(1, 2, 3, 4)\n\nThe c() function collects items. If you run the previous line of code with Ctrl/Cmd + Enter, you’ll see something called “scores” appear over in the Global Environment pane on the top-right. It consists of the numbers 1, 2, 3, and 4 collected together as a set. You won’t actually see any output though, and nothing will show up in your Rendered document. If you want a named object to be shown, you can type the name as a line of code.\n\nscores\n\n[1] 1 2 3 4\n\n\nTo add the set of scores up, we can use the sum() function.\n\nsum(scores)\n\n[1] 10\n\n\nThis might not seem like a time-saver with just 4 numbers, but imagine if you had a dataset of 1000 numbers…\nYou can do other things, like squaring each score, or squaring the summed scores. Note that R follows the usual order of mathematical operations.\n\nsum(scores^2) \n\n[1] 30\n\nsum(scores)^2 \n\n[1] 100\n\n\nNow you should be ready to answer the questions by typing your own code.\n\n\n\n\n\n\nTipFormatting equations in-text\n\n\n\n\n\nNotice how mathematical symbols are created in the question text: Click on the symbols and you’ll see that, behind the scenes, equations are enclosed in dollar signs. E.g. typing $\\Sigma X$ produces \\(\\Sigma X\\).\nThere may be times that you’ll want to type symbols or equations like these in your answers to questions, so it’s worth knowing how to do so.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html",
    "href": "recitation/problem-set-2-instructions.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "Say we have some data in a .csv file. The first step is to read it into R as a data.frame object, like you practiced in the first problem set.\n\nmy_data &lt;- read.csv(\"data/2_example.csv\")\n\nMake sure to take a look at the resulting data.frame. Here it’s a spreadsheet with just one column named “scores”–that’s the set of numbers I want to compute frequencies for.\n\n\n\nThere are many ways of getting producing a frequency table. There’s a built-in table() function, which takes a vector of values and returns a “table” object with the individual scores on one line, and their frequencies on the line below.\n\nfrequencies &lt;- table(my_data$scores)\nfrequencies\n\n\n3 4 5 6 7 8 9 \n1 2 3 4 5 3 2 \n\n\nYou can compute other things you might want to compute, like the total number of scores, \\(N\\), and proportions.\n\nsum(frequencies) # add up frequencies to get N\n\n[1] 20\n\nfrequencies / sum(frequencies) # divide each frequency by N to get proportions\n\n\n   3    4    5    6    7    8    9 \n0.05 0.10 0.15 0.20 0.25 0.15 0.10 \n\n\n\n\n\nThat’s nice enough, but a more powerful way of doing this is to use the dplyr function count(). dplyr is a package that is not built in to R; people developed it to make doing common tasks, like counting frequencies, simpler. This is part of the power and flexibility of R–the ability for anyone to add to it. All we need to do to use the dplyr package is install it (which I already did in your posit.cloud project) and then activate it using the library() function. Then we’ll use dplyr‘s count() function as part of an analytic pipeline using the |&gt; pipe operator. The pipe operator takes the object on its left and ’pipes’ it into the next function as its first argument. Many dplyr functions are designed to be used this way.\n\nlibrary(dplyr)\n\nmy_data |&gt; \n  count(scores) # compute frequencies for the 'scores' column\n\n  scores n\n1      3 1\n2      4 2\n3      5 3\n4      6 4\n5      7 5\n6      8 3\n7      9 2\n\n\nThe value of doing it that way is that the count() function returns a data.frame, rather than the slightly more esoteric ‘table’ object returned by table(). And the nice thing about having the scores and frequencies as a data.frame is that we can easily add to the analytic pipeline to add other quantities we might be interested in. dplyr’s mutate() function allows you to perform computations and add columns to a data.frame.\n\nfrequency_table &lt;- my_data |&gt; \n  # count the scores to produce frequencies\n  count(scores) |&gt; \n  # now add additional columns\n  mutate(proportion = n / sum(n),\n         percent = proportion * 100,\n         cumulative_percent = cumsum(percent))\n\nfrequency_table\n\n  scores n proportion percent cumulative_percent\n1      3 1       0.05       5                  5\n2      4 2       0.10      10                 15\n3      5 3       0.15      15                 30\n4      6 4       0.20      20                 50\n5      7 5       0.25      25                 75\n6      8 3       0.15      15                 90\n7      9 2       0.10      10                100\n\n\nNotice that the data.frame initially produced by count() has two columns. The first is named whatever the column that you counted was named (so here it was scores) and the second column, containing the frequencies, is named n by default. That’s why to compute the proportions, I used n / sum(n): it’s referring to the n column of values. You can change that if you like by using the name argument inside the count function to specify the name of the resulting column. Just remember to use the correct name in subsequent operations inside mutate() if you do so.\n\n# count column will be named \"n\" by default\nmy_data |&gt; \n  count(scores)\n\n  scores n\n1      3 1\n2      4 2\n3      5 3\n4      6 4\n5      7 5\n6      8 3\n7      9 2\n\n# count column will be named \"frequency\"\nmy_data |&gt; \n  count(scores, name = \"frequency\") \n\n  scores frequency\n1      3         1\n2      4         2\n3      5         3\n4      6         4\n5      7         5\n6      8         3\n7      9         2\n\n\n\n\n\n\n\n\nTipFormatting tables\n\n\n\nYou might find that your table looks unsightly in your Rendered document because the numbers have so many decimal places. Usually just one or two decimals is sufficient precision. Maybe you would like to have different column names as well.\nBehind the scenes, data.frames are rendered using knitr::kable(), i.e. the kable() function from the knitr package (It’s called knitr because it “knits” the raw stuff in your editor into a lovely finished document, like turning yarn into a lovely sweater. I guess the name “kable” comes from mashing up “knitr table”. Isn’t this fun?).\nSo instead of just typing the name of your data.frame and letting it show up with default settings and the original column names, you can explicitly feed it into the knitr::kable() function and include the additional arguments digits and col.names. You can even use fancy-looking mathematical symbols for the column names by enclosing it within dollar signs!\n\nfrequency_table |&gt; \n  knitr::kable(digits = 2,\n               col.names = c(\"Score\", \n                             \"$f$\", \n                             \"$p$\", \n                             \"%\", \n                             \"cumulative %\"))\n\n\n\n\nScore\n\\(f\\)\n\\(p\\)\n%\ncumulative %\n\n\n\n\n3\n1\n0.05\n5\n5\n\n\n4\n2\n0.10\n10\n15\n\n\n5\n3\n0.15\n15\n30\n\n\n6\n4\n0.20\n20\n50\n\n\n7\n5\n0.25\n25\n75\n\n\n8\n3\n0.15\n15\n90\n\n\n9\n2\n0.10\n10\n100",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#part-1-frequency-tables",
    "href": "recitation/problem-set-2-instructions.html#part-1-frequency-tables",
    "title": "Problem Set 2",
    "section": "",
    "text": "Say we have some data in a .csv file. The first step is to read it into R as a data.frame object, like you practiced in the first problem set.\n\nmy_data &lt;- read.csv(\"data/2_example.csv\")\n\nMake sure to take a look at the resulting data.frame. Here it’s a spreadsheet with just one column named “scores”–that’s the set of numbers I want to compute frequencies for.\n\n\n\nThere are many ways of getting producing a frequency table. There’s a built-in table() function, which takes a vector of values and returns a “table” object with the individual scores on one line, and their frequencies on the line below.\n\nfrequencies &lt;- table(my_data$scores)\nfrequencies\n\n\n3 4 5 6 7 8 9 \n1 2 3 4 5 3 2 \n\n\nYou can compute other things you might want to compute, like the total number of scores, \\(N\\), and proportions.\n\nsum(frequencies) # add up frequencies to get N\n\n[1] 20\n\nfrequencies / sum(frequencies) # divide each frequency by N to get proportions\n\n\n   3    4    5    6    7    8    9 \n0.05 0.10 0.15 0.20 0.25 0.15 0.10 \n\n\n\n\n\nThat’s nice enough, but a more powerful way of doing this is to use the dplyr function count(). dplyr is a package that is not built in to R; people developed it to make doing common tasks, like counting frequencies, simpler. This is part of the power and flexibility of R–the ability for anyone to add to it. All we need to do to use the dplyr package is install it (which I already did in your posit.cloud project) and then activate it using the library() function. Then we’ll use dplyr‘s count() function as part of an analytic pipeline using the |&gt; pipe operator. The pipe operator takes the object on its left and ’pipes’ it into the next function as its first argument. Many dplyr functions are designed to be used this way.\n\nlibrary(dplyr)\n\nmy_data |&gt; \n  count(scores) # compute frequencies for the 'scores' column\n\n  scores n\n1      3 1\n2      4 2\n3      5 3\n4      6 4\n5      7 5\n6      8 3\n7      9 2\n\n\nThe value of doing it that way is that the count() function returns a data.frame, rather than the slightly more esoteric ‘table’ object returned by table(). And the nice thing about having the scores and frequencies as a data.frame is that we can easily add to the analytic pipeline to add other quantities we might be interested in. dplyr’s mutate() function allows you to perform computations and add columns to a data.frame.\n\nfrequency_table &lt;- my_data |&gt; \n  # count the scores to produce frequencies\n  count(scores) |&gt; \n  # now add additional columns\n  mutate(proportion = n / sum(n),\n         percent = proportion * 100,\n         cumulative_percent = cumsum(percent))\n\nfrequency_table\n\n  scores n proportion percent cumulative_percent\n1      3 1       0.05       5                  5\n2      4 2       0.10      10                 15\n3      5 3       0.15      15                 30\n4      6 4       0.20      20                 50\n5      7 5       0.25      25                 75\n6      8 3       0.15      15                 90\n7      9 2       0.10      10                100\n\n\nNotice that the data.frame initially produced by count() has two columns. The first is named whatever the column that you counted was named (so here it was scores) and the second column, containing the frequencies, is named n by default. That’s why to compute the proportions, I used n / sum(n): it’s referring to the n column of values. You can change that if you like by using the name argument inside the count function to specify the name of the resulting column. Just remember to use the correct name in subsequent operations inside mutate() if you do so.\n\n# count column will be named \"n\" by default\nmy_data |&gt; \n  count(scores)\n\n  scores n\n1      3 1\n2      4 2\n3      5 3\n4      6 4\n5      7 5\n6      8 3\n7      9 2\n\n# count column will be named \"frequency\"\nmy_data |&gt; \n  count(scores, name = \"frequency\") \n\n  scores frequency\n1      3         1\n2      4         2\n3      5         3\n4      6         4\n5      7         5\n6      8         3\n7      9         2\n\n\n\n\n\n\n\n\nTipFormatting tables\n\n\n\nYou might find that your table looks unsightly in your Rendered document because the numbers have so many decimal places. Usually just one or two decimals is sufficient precision. Maybe you would like to have different column names as well.\nBehind the scenes, data.frames are rendered using knitr::kable(), i.e. the kable() function from the knitr package (It’s called knitr because it “knits” the raw stuff in your editor into a lovely finished document, like turning yarn into a lovely sweater. I guess the name “kable” comes from mashing up “knitr table”. Isn’t this fun?).\nSo instead of just typing the name of your data.frame and letting it show up with default settings and the original column names, you can explicitly feed it into the knitr::kable() function and include the additional arguments digits and col.names. You can even use fancy-looking mathematical symbols for the column names by enclosing it within dollar signs!\n\nfrequency_table |&gt; \n  knitr::kable(digits = 2,\n               col.names = c(\"Score\", \n                             \"$f$\", \n                             \"$p$\", \n                             \"%\", \n                             \"cumulative %\"))\n\n\n\n\nScore\n\\(f\\)\n\\(p\\)\n%\ncumulative %\n\n\n\n\n3\n1\n0.05\n5\n5\n\n\n4\n2\n0.10\n10\n15\n\n\n5\n3\n0.15\n15\n30\n\n\n6\n4\n0.20\n20\n50\n\n\n7\n5\n0.25\n25\n75\n\n\n8\n3\n0.15\n15\n90\n\n\n9\n2\n0.10\n10\n100",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#reading-in-data",
    "href": "recitation/problem-set-2-instructions.html#reading-in-data",
    "title": "Problem Set 2",
    "section": "",
    "text": "Say we have some data in a .csv file. The first step is to read it into R as a data.frame object, like you practiced in the first problem set.\n\nmy_data &lt;- read.csv(\"data/2_example.csv\")\n\nMake sure to take a look at the resulting data.frame. Here it’s a spreadsheet with just one column named “scores”–that’s the set of numbers I want to compute frequencies for.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#using-the-built-in-table-function",
    "href": "recitation/problem-set-2-instructions.html#using-the-built-in-table-function",
    "title": "Problem Set 2",
    "section": "",
    "text": "There are many ways of getting producing a frequency table. There’s a built-in table() function, which takes a vector of values and returns a “table” object with the individual scores on one line, and their frequencies on the line below.\n\nfrequencies &lt;- table(my_data$scores)\nfrequencies\n\n\n3 4 5 6 7 8 9 \n1 2 3 4 5 3 2 \n\n\nYou can compute other things you might want to compute, like the total number of scores, \\(N\\), and proportions.\n\nsum(frequencies) # add up frequencies to get N\n\n[1] 20\n\nfrequencies / sum(frequencies) # divide each frequency by N to get proportions\n\n\n   3    4    5    6    7    8    9 \n0.05 0.10 0.15 0.20 0.25 0.15 0.10",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#using-dplyr-and-count",
    "href": "recitation/problem-set-2-instructions.html#using-dplyr-and-count",
    "title": "Problem Set 2",
    "section": "",
    "text": "That’s nice enough, but a more powerful way of doing this is to use the dplyr function count(). dplyr is a package that is not built in to R; people developed it to make doing common tasks, like counting frequencies, simpler. This is part of the power and flexibility of R–the ability for anyone to add to it. All we need to do to use the dplyr package is install it (which I already did in your posit.cloud project) and then activate it using the library() function. Then we’ll use dplyr‘s count() function as part of an analytic pipeline using the |&gt; pipe operator. The pipe operator takes the object on its left and ’pipes’ it into the next function as its first argument. Many dplyr functions are designed to be used this way.\n\nlibrary(dplyr)\n\nmy_data |&gt; \n  count(scores) # compute frequencies for the 'scores' column\n\n  scores n\n1      3 1\n2      4 2\n3      5 3\n4      6 4\n5      7 5\n6      8 3\n7      9 2\n\n\nThe value of doing it that way is that the count() function returns a data.frame, rather than the slightly more esoteric ‘table’ object returned by table(). And the nice thing about having the scores and frequencies as a data.frame is that we can easily add to the analytic pipeline to add other quantities we might be interested in. dplyr’s mutate() function allows you to perform computations and add columns to a data.frame.\n\nfrequency_table &lt;- my_data |&gt; \n  # count the scores to produce frequencies\n  count(scores) |&gt; \n  # now add additional columns\n  mutate(proportion = n / sum(n),\n         percent = proportion * 100,\n         cumulative_percent = cumsum(percent))\n\nfrequency_table\n\n  scores n proportion percent cumulative_percent\n1      3 1       0.05       5                  5\n2      4 2       0.10      10                 15\n3      5 3       0.15      15                 30\n4      6 4       0.20      20                 50\n5      7 5       0.25      25                 75\n6      8 3       0.15      15                 90\n7      9 2       0.10      10                100\n\n\nNotice that the data.frame initially produced by count() has two columns. The first is named whatever the column that you counted was named (so here it was scores) and the second column, containing the frequencies, is named n by default. That’s why to compute the proportions, I used n / sum(n): it’s referring to the n column of values. You can change that if you like by using the name argument inside the count function to specify the name of the resulting column. Just remember to use the correct name in subsequent operations inside mutate() if you do so.\n\n# count column will be named \"n\" by default\nmy_data |&gt; \n  count(scores)\n\n  scores n\n1      3 1\n2      4 2\n3      5 3\n4      6 4\n5      7 5\n6      8 3\n7      9 2\n\n# count column will be named \"frequency\"\nmy_data |&gt; \n  count(scores, name = \"frequency\") \n\n  scores frequency\n1      3         1\n2      4         2\n3      5         3\n4      6         4\n5      7         5\n6      8         3\n7      9         2\n\n\n\n\n\n\n\n\nTipFormatting tables\n\n\n\nYou might find that your table looks unsightly in your Rendered document because the numbers have so many decimal places. Usually just one or two decimals is sufficient precision. Maybe you would like to have different column names as well.\nBehind the scenes, data.frames are rendered using knitr::kable(), i.e. the kable() function from the knitr package (It’s called knitr because it “knits” the raw stuff in your editor into a lovely finished document, like turning yarn into a lovely sweater. I guess the name “kable” comes from mashing up “knitr table”. Isn’t this fun?).\nSo instead of just typing the name of your data.frame and letting it show up with default settings and the original column names, you can explicitly feed it into the knitr::kable() function and include the additional arguments digits and col.names. You can even use fancy-looking mathematical symbols for the column names by enclosing it within dollar signs!\n\nfrequency_table |&gt; \n  knitr::kable(digits = 2,\n               col.names = c(\"Score\", \n                             \"$f$\", \n                             \"$p$\", \n                             \"%\", \n                             \"cumulative %\"))\n\n\n\n\nScore\n\\(f\\)\n\\(p\\)\n%\ncumulative %\n\n\n\n\n3\n1\n0.05\n5\n5\n\n\n4\n2\n0.10\n10\n15\n\n\n5\n3\n0.15\n15\n30\n\n\n6\n4\n0.20\n20\n50\n\n\n7\n5\n0.25\n25\n75\n\n\n8\n3\n0.15\n15\n90\n\n\n9\n2\n0.10\n10\n100",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#ggplot",
    "href": "recitation/problem-set-2-instructions.html#ggplot",
    "title": "Problem Set 2",
    "section": "ggplot",
    "text": "ggplot\nAs usual, there are many ways of producing data visualization in R, but the most widely used and flexible is probably the ggplot2 package. The “gg” in “ggplot” refers to the “grammar of graphics”. This package isn’t built-in to R; someone else created it and made it freely available as an add-on. For packages like that, we have to tell R we want to use them using the library() function.\n\nlibrary(ggplot2) # activate the ggplot2 package\n\nmy_data &lt;- read.csv(\"data/2_example.csv\") # read some data, if you didn't already",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#bar-graph",
    "href": "recitation/problem-set-2-instructions.html#bar-graph",
    "title": "Problem Set 2",
    "section": "Bar graph",
    "text": "Bar graph\nThe following two lines create a simple bar graph. ggplot works by layering, using the + symbol. The first line specifies the name of the data.frame and the ‘aesthetics’: we want the scores (which are in a column conveniently named scores in the data.frame) on the x-axis. Then we add geom_bar() as the geometry layer. geom_bar is designed to take a set of scores and calculate the frequencies, which become the height of the bars on the \\(y\\)-axis (which is why we don’t need to explicitly specific a \\(y\\) aesthetic in the aes() function.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThat’s all there is to it: we now have a perfectly serviceable bar graph showing the frequency distribution.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#histogram",
    "href": "recitation/problem-set-2-instructions.html#histogram",
    "title": "Problem Set 2",
    "section": "Histogram",
    "text": "Histogram\nIf we instead wanted a histogram, there are a couple of ways to do it. Remember, the only visual difference between a bar graph and histogram is that a histogram has bars that are touching, while the bar graph has a bit of space between them. So we can repeat the previous code, but specify that the width of the bars should be 1.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar(width = 1)\n\n\n\n\n\n\n\n\nAnd that basically produces what we want. But there is also a dedicated histogram geom we can use:\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhen you run those lines of code you’ll notice a warning message in the console advising you to pick a new binwidth. That’s because the default is to have 30 bins, which often won’t be appropriate for your data. Here, because sleep duration was recorded ‘to the nearest whole hour’, i.e. the data is integer, we probably want a binwidth of 1 unit, i.e. we want a bar for each hour.\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nNow this is identical to the geom_bar(width = 1) version from before. Note that bar’s width and the binwidth are different things, though. The width is literally the width of the bars; binwidth is the width of the intervals into which scores are grouped. The usefulness of geom_histogram()is that it allows for more flexibility with the binwidth, which can be useful for continuous scores and datasets which cover a width range of scores, for which we might want a grouped frequency histogram with a wider binwidth.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#frequency-polygon",
    "href": "recitation/problem-set-2-instructions.html#frequency-polygon",
    "title": "Problem Set 2",
    "section": "Frequency polygon",
    "text": "Frequency polygon\nLastly, we make a frequency polygon in much the same way as a histogram, remembering to specify the appropriate binwidth.\n\nggplot(my_data, aes(x = scores)) + \n  geom_freqpoly(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipBonus: Making things look nice\n\n\n\n\n\nThe default options for the look of the plot are still pretty ugly though. If you’re so inclined, you can customize just about everything about the look and layout of a graph using ggplot. Some aspects are fairly straight forward, like changing the outline and fill of the bars within the geom_histogram() function. We add another layer, labs() to set the axis titles. Stating breaks within scale_x_continuous() determines the labels of the x axis. And then there are all the theme() elements we can change or remove.\n\nggplot(data = my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1, fill = \"grey\", color = \"black\") +\n  labs(x = \"Scores\", y = \"Frequency\") +\n  scale_x_continuous(breaks = 3:9) +\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_line(),\n        axis.line.y = element_line(),\n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nI don’t suggest you get into the weeds on this unless you find that kind of thing satisfying (like I do). But for psych majors it can be a useful exercise figuring out how to produce a graph formatted according to, say, APA conventions, by tweaking the theme options.\nThere’s a learning curve to ggplot, but once you get to grips with it, it is an incredibly powerful and flexible way of visualizing data.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-2-instructions.html#part-2-data-visualization",
    "href": "recitation/problem-set-2-instructions.html#part-2-data-visualization",
    "title": "Problem Set 2",
    "section": "Part 2: Data visualization",
    "text": "Part 2: Data visualization\n\nThe ggplot package\nAs usual, there are many ways of producing data visualization in R, but the most widely used and flexible is probably the ggplot2 package. The “gg” in “ggplot” refers to the “grammar of graphics”. This package isn’t built-in to R; someone else created it and made it freely available as an add-on. For packages like that, we have to tell R we want to use them using the library() function.\n\nlibrary(ggplot2) # activate the ggplot2 package\n\nmy_data &lt;- read.csv(\"data/2_example.csv\") # read some data, if you didn't already\n\n\n\nBar graph\nThe following two lines create a simple bar graph. ggplot works by layering, using the + symbol. The first line specifies the name of the data.frame and the ‘aesthetics’: we want the scores (which are in a column conveniently named scores in the data.frame) on the x-axis. Then we add geom_bar() as the geometry layer. geom_bar is designed to take a set of scores and calculate the frequencies, which become the height of the bars on the \\(y\\)-axis (which is why we don’t need to explicitly specific a \\(y\\) aesthetic in the aes() function.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThat’s all there is to it: we now have a perfectly serviceable bar graph showing the frequency distribution.\n\n\nHistogram\nIf we instead wanted a histogram, there are a couple of ways to do it. Remember, the only visual difference between a bar graph and histogram is that a histogram has bars that are touching, while the bar graph has a bit of space between them. So we can repeat the previous code, but specify that the width of the bars should be 1.\n\nggplot(my_data, aes(x = scores)) + \n  geom_bar(width = 1)\n\n\n\n\n\n\n\n\nAnd that basically produces what we want. But there is also a dedicated histogram geom we can use:\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWhen you run those lines of code you’ll notice a warning message in the console advising you to pick a new binwidth. That’s because the default is to have 30 bins, which often won’t be appropriate for your data. Here, because sleep duration was recorded ‘to the nearest whole hour’, i.e. the data is integer, we probably want a binwidth of 1 unit, i.e. we want a bar for each hour.\n\nggplot(my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nNow this is identical to the geom_bar(width = 1) version from before. Note that bar’s width and the binwidth are different things, though. The width is literally the width of the bars; binwidth is the width of the intervals into which scores are grouped. The usefulness of geom_histogram()is that it allows for more flexibility with the binwidth, which can be useful for continuous scores and datasets which cover a width range of scores, for which we might want a grouped frequency histogram with a wider binwidth.\n\n\nFrequency polygon\nLastly, we make a frequency polygon in much the same way as a histogram, remembering to specify the appropriate binwidth.\n\nggplot(my_data, aes(x = scores)) + \n  geom_freqpoly(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipBonus: Making things look nice\n\n\n\n\n\nThe default options for the look of the plot are still pretty ugly though. If you’re so inclined, you can customize just about everything about the look and layout of a graph using ggplot. Some aspects are fairly straight forward, like changing the outline and fill of the bars within the geom_histogram() function. We add another layer, labs() to set the axis titles. Stating breaks within scale_x_continuous() determines the labels of the x axis. And then there are all the theme() elements we can change or remove.\n\nggplot(data = my_data, aes(x = scores)) + \n  geom_histogram(binwidth = 1, fill = \"grey\", color = \"black\") +\n  labs(x = \"Scores\", y = \"Frequency\") +\n  scale_x_continuous(breaks = 3:9) +\n  theme(panel.grid = element_blank(),\n        axis.line.x = element_line(),\n        axis.line.y = element_line(),\n        panel.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\n\n\nI don’t suggest you get into the weeds on this unless you find that kind of thing satisfying (like I do). But for psych majors it can be a useful exercise figuring out how to produce a graph formatted according to, say, APA conventions, by tweaking the theme options.\nThere’s a learning curve to ggplot, but once you get to grips with it, it is an incredibly powerful and flexible way of visualizing data.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "recitation/problem-set-3-instructions.html#part-1-computing-descriptive-statistics",
    "href": "recitation/problem-set-3-instructions.html#part-1-computing-descriptive-statistics",
    "title": "Problem Set 3",
    "section": "",
    "text": "Suppose we have a set of values: 1, 2, 3, 3, 4, 5, 10\nThere are several ways you could find the mean. One way would be to add the scores and divide by \\(N\\), like you would on paper.\n\n(1 + 2 + 3 + 3 + 4 + 5 + 10) / 7\n\n[1] 4\n\n\nOf course, you don’t want to have to type all those numbers and figure out \\(N\\) yourself (especially if you were dealing with a much larger set of numbers). Better to have the scores as a named object:\n\nscores &lt;- c(1, 2, 3, 3, 4, 5, 10)\n\nNow life is much easier. The same mathematical approach can be implemented with the sum() and length() functions.\n\nsum(scores) / length(scores) # this is equivalent to Sigma X divided by N\n\n[1] 4\n\n\nEven better, R has built-in functions to easily find the mean and median of a set of scores.\n\nmean(scores)\n\n[1] 4\n\nmedian(scores)\n\n[1] 3\n\n\nYou’d think there’d be one for finding the mode as well…\n\nmode(scores)\n\n[1] \"numeric\"\n\n\nBut that actually does something different.\nSince the mode is a common statistic, though, somebody already wrote a function to compute it. It’s in a package called modeest. The package contains the function mfv1(), for “most frequent value”, which we can now use to get the mode. Remember, since it’s an external package, we need to activate it with the library() function.\n\nlibrary(modeest)\nmfv1(scores)\n\n[1] 3\n\n\nOr equivalently, you can use the double-colon operator to refer to a package::its_function() without a separate library() call:\n\nmodeest::mfv1(scores)\n\n[1] 3\n\n\n\n\n\nThere are functions to find the minimum and maximum value in a set of scores. The range is the difference between those:\n\nmin(scores)\n\n[1] 1\n\nmax(scores)\n\n[1] 10\n\nmax(scores) - min(scores) # range\n\n[1] 9\n\n\nThere is a built-in range() function. It returns the actual lowest and highest values as a collection. To find the difference between them, you could nest the range inside the diff() function.\n\nrange(scores)\n\n[1]  1 10\n\ndiff(range(scores))\n\n[1] 9\n\n\nR has a built-in functions for sample SD.\n\nsd(scores)\n\n[1] 2.94392\n\n\nThere’s also a var() function to compute the variance. Remember that standard deviation is the square-root of variance, so if we square the SD it should give the same answer as var()…\n\nvar(scores)\n\n[1] 8.666667\n\nsd(scores)^2\n\n[1] 8.666667\n\n\nNeato!\nNote that these functions return sample variance & SD. There are no built-in functions for population variance & SD so if you wanted to find them you would have to use the mathematical procedure outlined in the lecture.\n\n\n\nThe above instructions show how mathematical operations and functions can be used to compute summary statistics for a set of numbers. Most often, however, the set of numbers we’re dealing with is a column in a data.frame.\n\ndf &lt;- data.frame(scores = c(1, 2, 3, 3, 4, 5, 10))\n\nYou can use the same approaches as above, just remember the $ notation to refer to a data.frame column:\n\nmean(df$scores)\n\n[1] 4\n\nsd(df$scores)\n\n[1] 2.94392\n\n\nHowever, dplyr has a powerful summarize() function that fits in nicely with the pipe operator.\n\nlibrary(dplyr)\n\ndf |&gt; \n  summarize(mean = mean(scores),\n            median = median(scores),\n            mode = modeest::mfv1(scores),\n            min = min(scores),\n            max = max(scores),\n            range = max - min,\n            sd = sd(scores))\n\n  mean median mode min max range      sd\n1    4      3    3   1  10     9 2.94392\n\n\nIn a single pipeline we produce all the summary statistics as a data.frame! Note that because I pipe df into the summarize() function, scores refers specifically to that column in the df data.frame object rather than the separate scores object I created previously. Note also that I defined the min and max summary columns, and I was able to refer to those new columns within the same single mutate() function to compute the range.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 3"
    ]
  },
  {
    "objectID": "recitation/problem-set-4-instructions.html#finding-probabilities-for-z-scores",
    "href": "recitation/problem-set-4-instructions.html#finding-probabilities-for-z-scores",
    "title": "Problem Set 4",
    "section": "",
    "text": "The old-fashioned way to find the probability (the proportion of the distribution) associated with particular \\(z\\)-scores would be to look the score up in a \\(z\\)-score table (Unit Normal Table). R can do this for us much more easily, but it’s a good idea to check your first few answers against a table, e.g. https://www.westga.edu/academics/research/vrc/assets/docs/UnitNormalTable.pdf\nIf you have a \\(z\\)-score in mind and want to know the associated probability, use the pnorm() function. By default, pnorm() assumes we want the proportion of the distribution to the left of the \\(z\\)-score we specify.\n\npnorm(2) \n\n[1] 0.9772499\n\n\nSo .9772499, or ~98% of scores in a normal distribution are less than \\(z = 2\\). Here’s what that looks like:\n\n\n\n\n\n\n\n\n\nTo get the proportion to the right, we set the lower.tail argument to FALSE (i.e. we want the upper tail)\n\npnorm(2, lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nThat tells us that ~2% of scores are greater than \\(z = 2\\). Here’s what that looks like:\n\n\n\n\n\n\n\n\n\nThink about how R’s lower.tail distinction corresponds to the “body” vs. “tail” distinction. It’s not always necessarily the same.\nTo find the proportion of the normal distribution between two \\(z\\)-scores, one way would be to subtract p(lower score) from p(higher score)\n\npnorm(.25) - pnorm(-.25)\n\n[1] 0.1974127\n\n\n\n\n\n\n\n\n\n\n\nJust under .20, or 20% is between \\(z = -.25\\) and \\(z = .25\\). Note that the slightly different code below gives the same answer. Do you understand why? Either solution is perfectly acceptable.\n\npnorm(-.25, lower.tail = F) - pnorm(.25, lower.tail = F)\n\n[1] 0.1974127",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "recitation/problem-set-4-instructions.html#finding-z-scores-for-probabilities",
    "href": "recitation/problem-set-4-instructions.html#finding-z-scores-for-probabilities",
    "title": "Problem Set 4",
    "section": "Finding \\(z\\)-scores for probabilities",
    "text": "Finding \\(z\\)-scores for probabilities\nWhen you have a proportion in mind and want to find the associated \\(z\\)-score, qnorm() is the appropriate function.\n\nqnorm(.05)\n\n[1] -1.644854\n\n\nThat tells us that the cutoff for the lowest 5% of the distribution corresponds to \\(z = -1.64\\). qnorm() also accepts the lower.tail argument\n\nqnorm(.05, lower.tail = FALSE) # Now we get a positive value\n\n[1] 1.644854\n\n\nThe line below gives the same answer. Do you understand why?\n\nqnorm(.95, lower.tail = TRUE)\n\n[1] 1.644854",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "recitation/problem-set-4-instructions.html#non-standardized-distributions",
    "href": "recitation/problem-set-4-instructions.html#non-standardized-distributions",
    "title": "Problem Set 4",
    "section": "Non-standardized distributions",
    "text": "Non-standardized distributions\nThe last thing you need to know for the questions below is that both pnorm() and qnorm() have optional arguments for the mean and SD of the distribution. That is, if we have raw scores, we don’t have to convert to z-scores to use these functions, we can specify the appropriate values\n\npnorm(130, mean = 100, sd = 15)\n\n[1] 0.9772499\n\n\n\n\n\n\n\n\nTipSketching distributions\n\n\n\n\n\nWith the kinds of questions that follow, I find it massively useful to sketch a rough normal curve on a piece of paper and mark approximately where the \\(z\\)-scores would go, and shade in the corresponding probability region. My strong recommendation is that you do that on a piece of paper.\nIf are falling in love with R and ggplot in particular, however, you can draw distributions right here.\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(-2, 2), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nThe first stat_function() layer draws the red shaded region, and the second one draws the black line of the full normal curve. I picked c(-4, 4) for the xlims of the full curve because that’s wide enough to show the ends tailing off to 0. For the shaded region, c(-2, 2) was just an arbitrary choice.\nSay you wanted to shade the region below \\(z = 2\\). You could use:\n\nggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(-4, 2), # xlim goes up to 2\n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nThat gives you a good visual sense that most of the distribution is below that point, so when pnorm(2) gives you the answer, it should make sense.\n\npnorm(2)\n\n[1] 0.9772499\n\n\nOr say you want to see what the highest 20% of the distribution looks like.\n\nggplot() +\n  stat_function(fun = dnorm, geom = \"density\", \n                xlim = c(qnorm(0.2, lower.tail = FALSE), 4), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nOr a region of 40% in the middle of the distribution, separating 30% in each tail:\n\nggplot() +\n  stat_function(fun = dnorm, geom = \"density\", \n                xlim = c(qnorm(0.3), qnorm(0.3, lower.tail = FALSE)), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nYou could even turn this into a convenient function…\n\nplot_distribution &lt;- function(lower_z = -4, upper_z = 4) {\n  ggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(lower_z, upper_z), # xlim goes up to 2\n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n}\n\nplot_distribution(-2, 2)\n\n\n\n\n\n\n\n\nI’m not saying this will make your life easier: you still have to know your way around pnorm(), qnorm(), and the idea of chopping the distribution into different regions. But it can be satisfying to produce a visual reference corresponding to your numeric answer.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 4"
    ]
  },
  {
    "objectID": "recitation/problem-set-4-instructions.html#dealing-with-non-standardized-distributions",
    "href": "recitation/problem-set-4-instructions.html#dealing-with-non-standardized-distributions",
    "title": "Problem Set 4",
    "section": "Dealing with non-standardized distributions",
    "text": "Dealing with non-standardized distributions\nThe last thing you need to know for the questions below is that both pnorm() and qnorm() have optional arguments for the mean and SD of the distribution. That is, if we have raw scores, we don’t have to convert to \\(z\\)-scores to use these functions, we can just specify the appropriate values.\n\npnorm(130, mean = 100, sd = 15)\n\n[1] 0.9772499\n\n\n\n\n\n\n\n\nTipSketching distributions\n\n\n\n\n\nDo you find the graphs in the instructions above helpful? With the kinds of questions in this problem set, I find it massively useful to sketch a rough normal curve on a piece of paper and mark approximately where the \\(z\\)-scores would go, and shade in the corresponding probability region. My strong recommendation is that you do that on a piece of paper.\nIf are falling in love with R and ggplot in particular, however, you can draw distributions right in your problem set using ggplot.\n\nlibrary(ggplot2)\n\nggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(-2, 2), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nThe first stat_function() layer draws the red shaded region, and the second one draws the black line of the full normal curve. I picked c(-4, 4) for the xlims of the full curve because that’s wide enough to show the ends tailing off to 0. For the shaded region, c(-2, 2) was just an arbitrary choice.\nSay you wanted to shade the region below \\(z = 2\\). You could use:\n\nggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(-4, 2), # xlim goes up to 2\n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nThat gives you a good visual sense that most of the distribution is below that point, so when pnorm(2) gives you the answer, it should make sense.\n\npnorm(2)\n\n[1] 0.9772499\n\n\nOr say you want to see what the highest 20% of the distribution looks like.\n\nggplot() +\n  stat_function(fun = dnorm, geom = \"density\", \n                xlim = c(qnorm(0.2, lower.tail = FALSE), 4), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nOr a region of 40% in the middle of the distribution, separating 30% in each tail:\n\nggplot() +\n  stat_function(fun = dnorm, geom = \"density\", \n                xlim = c(qnorm(0.3), qnorm(0.3, lower.tail = FALSE)), \n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n\n\n\n\n\n\n\nYou could even turn this into a convenient function…\n\nplot_distribution &lt;- function(lower_z = -4, upper_z = 4) {\n  \n  ggplot() +\n  stat_function(fun = dnorm, \n                geom = \"density\", \n                xlim = c(lower_z, upper_z), # xlim goes up to 2\n                fill = \"red\", color = NA) +\n  stat_function(fun = dnorm, xlim = c(-4, 4))\n\n}\n\nplot_distribution(-2, 2)\n\n\n\n\n\n\n\n\nI’m not saying this will make your life easier: you still have to know your way around ggplot(), pnorm(), qnorm(), and the idea of chopping the distribution into different regions. But it can be satisfying to produce a visual reference corresponding to your numeric answer.",
    "crumbs": [
      "Syllabus",
      "Recitation",
      "Problem Set 4"
    ]
  }
]